<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>AI编译器后端&amp;运行时</title>
    <url>/2025/04/16/AI%E7%BC%96%E8%AF%91%E5%99%A8%E5%90%8E%E7%AB%AF-%E8%BF%90%E8%A1%8C%E6%97%B6/</url>
    <content><![CDATA[<p><img src="/images/image-20250420234716898.png" alt="image-20250420234716898"></p>
<span id="more"></span>

<blockquote>
<p>在尝试将pytorch接入aie平台的过程中，发现自己对于AI编译器在机器学习系统中的定位所知甚少。由于先前经验大多只停留在将高级语言转换成硬件相关的ir代码，而忽略了后端以及运行时系统协作方面的知识，遇到瓶颈。因此本博客通过参考机器学习系统书籍，以及目前主流AI编译器的后端和运行时系统的协作，查漏补缺，补全自己的知识盲点。</p>
</blockquote>
<h2 id="AI编译器后端和运行时架构"><a href="#AI编译器后端和运行时架构" class="headerlink" title="AI编译器后端和运行时架构"></a><font color = brown>AI编译器后端和运行时架构</font></h2><p>通过参考<a href="https://openmlsys.github.io/chapter_backend_and_runtime/index.html">mlsys open book</a>和<a href="https://zhuanlan.zhihu.com/p/651036523">tensorflow xla解读</a>，对机器学习编译有了新的理解。机器学习编译技术需要与运行时系统相互写作，才能将一个机器学习模型完整的跑在GPU&#x2F;NPU上。下图是AI系统的一个软件栈示意图：</p>
<img src="/images/image-20250418172339205.png" alt="image-20250418172339205" style="zoom: 80%;" />

<p>上述优化流程，是针对静态图的全图编译运行流程（tensorflow XLA这种动态圈图有一定区别）。按照软件栈层级关系，可以分为编译层和运行时系统层。</p>
<ul>
<li>编译层：编译层的输入是整个静态计算图或是部分子图，因此首先需要编译器前端用户代码进行解析翻译得到计算图IR，并对其进行设备信息无关的优化，此时的优化并不考虑程序执行的底层硬件信息。编译器后端的主要职责是对前端下发的IR做进一步的计算图优化，让其更加贴合硬件，并为IR中的计算节点选择在硬件上执行的算子，然后为每个算子的输入输出分配硬件内存，最终生成一个可以在硬件上执行的任务序列。</li>
<li>运行时系统层：运行时系统设计一般均有一个虚拟机VM用以支持调度和快速部署（IREE架构的VM和Relay VM）。其输入是编译层生成的可在硬件上执行的任务序列，<strong>如果考虑跨硬件部署，则需要考虑使用函数类型擦除技术统一接口</strong>。然后，VM将任务序列转换成硬件设备api和host端代码，最终分别生成可执行。</li>
</ul>
<p>后续会详细解读每一层的流程，以及可参考文献。</p>
<h3 id="编译系统"><a href="#编译系统" class="headerlink" title="编译系统"></a><font color = green>编译系统</font></h3><h3 id="运行时系统"><a href="#运行时系统" class="headerlink" title="运行时系统"></a><font color = green>运行时系统</font></h3><p>这一章节关于运行时系统的描述，主要参考<a href="https://www.lei.chat/posts/single-node-ml-runtime-foundation/">Lei chat blog</a>，本章的描述均基于单节点来讨论，服务器等多节点暂不涉及。</p>
<p>首先，我们需要明确，什么是机器学习系统的runtime system。runtime system的主要工作有两点：</p>
<ul>
<li>管理资源（内存资源等）</li>
<li>调度执行</li>
</ul>
<p>根据实际语义，runtime亦有不同。多卡分布式训练场景下，runtime负责将训练任务分配调度到数据中心的不同节点。单节点上，runtime负责派发张量计算到特定的加速器。<strong>本章节关注单节点runtime系统，其一般运行在CPU上来管理资源并调度机器学习任务负载到一个或多个加速器上。</strong></p>
<h4 id="运行时抽象"><a href="#运行时抽象" class="headerlink" title="运行时抽象"></a>运行时抽象</h4><p>运行时系统给用户提供了三个抽象：</p>
<ul>
<li><p>主机设备分离</p>
<blockquote>
<p>摘自博客的解释：</p>
<p><img src="/images/image-20250421103410716.png" alt="image-20250421103410716"></p>
<p><img src="/images/image-20250421103520060.png" alt="image-20250421103520060"></p>
</blockquote>
</li>
<li><p>虚拟机抽象</p>
<p>这一部分是我们理解整个runtime的重点，具体的实际例子可以参考<a href="https://github.com/iree-org/iree">IREE </a>和<a href="https://zhuanlan.zhihu.com/p/504066888">tvm runtime</a>设计细节。针对机器学习系统以及我们的cpu加速器协同架构，我们会有如下insights：</p>
<ul>
<li>首先我们观察主机设备架构，会发现相比设备端，主机端的系统架构是比较统一的（ARM64， x86_64和RISC-V）。</li>
<li>观察机器学习的任务，发现机器学习任务一般是<a href="https://llvm.org/doxygen/classllvm_1_1LoopNest.html">完美循环</a>（定义见llvm官方文档）。因此<strong>访存方面用逻辑偏移即可，而非物理指针</strong>。</li>
<li>机器学习模型部署在端侧，因此轻量化也是一大考量。</li>
</ul>
<p>基于上述insights，主流ai编译器均定义一套主机端虚拟指令集和虚拟机来实现调度。计算密集任务放在加速器上，存储资源追踪管理以及维度形状计算等逻辑（适合cpu端计算的逻辑）放在虚拟机上。如下图所示是IREE的虚拟机架构图：</p>
<p><img src="/images/image-20250421105323107.png" alt="image-20250421105323107"></p>
<p>上述虚拟机设计，还有一大好处，那就是可以很自然的完成从mlir系统到虚拟机指令ISA的编译转换。因此整个调度控制流程可以自然地通过编译器来控制，进一步提供可优化点。</p>
</li>
<li><p>HAL硬件抽象层</p>
<p>针对边缘&#x2F;客户端设备的多样化硬件生态，最佳实践是基于现代图形&#x2F;计算API（如Vulkan、DirectX 12、Metal）构建硬件抽象层（HAL）。这些API提供低开销、跨平台支持、显式控制等优势，能统一管理不同厂商的GPU&#x2F;加速器（如NVIDIA&#x2F;AMD&#x2F;Apple）。虽然这些API直接使用复杂（如代码冗长），但通过编译器自动生成调用逻辑，可绕过开发难度，同时保留底层控制权。这种设计既能满足高性能需求（通过低层级优化），又能覆盖多平台&#x2F;硬件的兼容性（如Windows&#x2F;iOS&#x2F;Android），为碎片化设备提供统一且高效的计算支持。如下是iree的HAL，VM架构图。</p>
<p><img src="/images/image-20250421110831028.png" alt="image-20250421110831028"></p>
</li>
</ul>
<p>上述是runtime system的三个抽象，而实际生产中，runtime系统也要充分考虑性能问题：</p>
<blockquote>
<p>在GPU资源管理与同步中，核心挑战在于平衡计算吞吐量与系统整体效率。由于GPU内存性能常落后于算力提升，单纯优化单内核代码（如矩阵乘法）难以发挥理论峰值，需避免运行时因频繁创建&#x2F;销毁资源（如缓冲区、同步对象）或跨组件数据拷贝&#x2F;强制等待导致的性能损耗。为此，可借鉴图形应用（如游戏引擎）的成熟实践：<strong>预分配资源池</strong>减少动态开销、<strong>预录异步命令链</strong>最大化GPU并行度、<strong>细粒度同步</strong>基于资源生命周期。这些策略可通过编译器自动化实现（如依赖分析与指令提升）。IREE运行时采用<strong>异步优先</strong>设计，支持跨组件无缝传递缓冲区与同步原语，利用操作系统底层机制（如Linux的dma_fence）避免硬性边界，既保障ML模型的高效执行，又能嵌入复杂应用（如图像处理管线）而不成为性能瓶颈，兼顾性能与系统协同。</p>
</blockquote>
<p>上述关于运行时系统的描述，是比较普遍的，下面结合Open MLsys一书，详细解读不同运行时系统的分类。</p>
<blockquote>
<p>经过算子选择与内存分配之后，计算任务可以通过运行时完成计算的调度与在硬件上的执行。根据是否将算子编译为计算图，计算的调度可以分为单算子调度与计算图调度两种方式，例如在MindSpore中分别提供了PyNative模式和Graph模式。而根据硬件提供的能力差异，计算图的执行方式又可以分为逐算子下发执行的交互式执行以及将整个计算图或者部分子图一次性下发到硬件的下沉式执行两种模式。                                                                                                                                                                      —- 《Open Mlysy》</p>
</blockquote>
<h4 id="单算子调度"><a href="#单算子调度" class="headerlink" title="单算子调度"></a>单算子调度</h4><p>一行行算子line by line的交由python运行时执行，好处是方便debug和看到即时效果，坏处是只能顺序执行，丧失很多并行性，同时也无法支持编译优化。</p>
<h4 id="计算图调度"><a href="#计算图调度" class="headerlink" title="计算图调度"></a>计算图调度</h4><p>在一个典型的异构计算环境中，主要存在CPU、GPU以及NPU等多种计算设备，因此一张计算图可以由运行在不同设备上的算子组成为异构计算图。</p>
<p><img src="/images/image-20250418182641748.png" alt="image-20250418182641748"></p>
<p>所述计算图由如下几类异构硬件对应的算子组成：</p>
<ul>
<li><strong>CPU算子</strong>：由C++语言编写实现并在主机上通过CPU执行的算子，CPU计算的性能取决于是否能够充分利用CPU多核心的计算能力。</li>
<li><strong>GPU算子</strong>：以英伟达GPU芯片为例，通过在主机侧将GPU Kernel逐个下发到GPU设备上，由GPU芯片执行算子的计算逻辑，由于芯片上具备大量的并行执行单元，可以为高度并行的算法提供强大的加速能力。</li>
<li><strong>NPU算子</strong>：以华为Ascend芯片为例， Ascend是一个高度集成的SoC芯片，NPU的优势是支持将部分或整个计算图下沉到芯片中完成计算，计算过程中不与Host发生交互，因此具备较高的计算性能。</li>
<li><strong>Python算子</strong>：在执行模式上与CPU算子类似，都是由主机上的CPU执行计算，区别在于计算逻辑是由Python语言的运行时通过Python解释器解释执行。</li>
</ul>
<p>一般主流架构均提供了指定算子所在运行设备的能力，此处可以参考mindspore的具体实现。</p>
<p>完成计算图中算子对应设备的标记以后，计算图已经准备好被调度与执行，根据硬件能力的差异，可以将异构计算图的执行分为三种模式，分别是逐算子交互式执行，整图下沉执行与子图下沉执行。</p>
<ul>
<li>交互式执行主要针对CPU和GPU的场景，计算图中的算子按照输入和输出的依赖关系被逐个调度与执行；</li>
<li>而整图下沉执行模式主要是针对NPU芯片而言，这类芯片主要的优势是能够将整个神经网络的计算图一次性下发到设备上，无需借助主机的CPU能力而独立完成计算图中所有算子的调度与执行，减少了主机和芯片的交互次数，借助NPU的张量加速能力，提高了计算效率和性能；</li>
<li>子图下沉执行模式是前面两种执行模式的结合，由于计算图自身表达的灵活性，对于复杂场景的计算图在NPU芯片上进行整图下沉执行的效率不一定能达到最优，因此可以将对于NPU芯片执行效率低下的部分分离出来，交给CPU或者GPU等执行效率更高的设备处理，而将部分更适合NPU计算的子图下沉到NPU进行计算，这样可以兼顾性能和灵活性两方面。</li>
</ul>
<blockquote>
<p>虽然在计算图上可以充分表达算子间的并发关系，在实际代码中会产生由于并发而引起的一些不预期的副作用场景</p>
<img src="/images/image-20250418183157655.png" alt="image-20250418183157655" style="zoom: 50%;" />

<p>如上图所示，虚线是operator之间的隐含依赖关系，但是在计算图中难以表示。因此在实际执行计算图的过程中，会分为交互式下发和下沉式（执行方式区分）。</p>
</blockquote>
<h4 id="算子交互执行"><a href="#算子交互执行" class="headerlink" title="算子交互执行"></a>算子交互执行</h4><p>框架的运行时根据计算图中算子的依赖关系，按照某种执行序（例如广度优先序）逐个将算子下发到硬件上执行。我们处理的计算图一般为异构计算图（计算图中的硬件有多种）。解读异构计算图交互执行之前，先解读同构计算图，异构计算图需要先通过子图切分为同构计算图。</p>
<p>同构计算图有两种执行方式，如下表所示：</p>
<table>
<thead>
<tr>
<th align="right">执行方式</th>
<th align="right">串行执行</th>
<th align="right">并行执行</th>
</tr>
</thead>
<tbody><tr>
<td align="right">算子执行顺序</td>
<td align="right">固定</td>
<td align="right">不固定</td>
</tr>
<tr>
<td align="right">算子执行线程</td>
<td align="right">单线程</td>
<td align="right">多线程</td>
</tr>
<tr>
<td align="right">所需执行资源</td>
<td align="right">较低</td>
<td align="right">较高</td>
</tr>
</tbody></table>
<p>而对于异构计算图，<font color = red>一般来说计算图的优化都是基于非异构计算图来实现的，要求计算图中的算子为同一设备上的，方便算子间的融合替换等优化操作，因此需要将一张异构计算图切分为多个非异构计算图，这里切分就比较灵活了，可以定义各种切分规则，一般按照产生尽量少的子图的切分规则来切分，尽量将多的同一设备上的算子放在一张子图中。</font>如下图是一种可能的子图切分方式：</p>
<img src="/images/image-20250418185356799.png" alt="image-20250418185356799" style="zoom:50%;" />

<p>切分出子图后，下一步需要考虑的是子图执行。共可以分为两种策略：</p>
<ul>
<li><p><strong>子图拆分执行</strong>：将切分后的多个子图分开执行，即一个子图执行完再执行另一个子图，上一个子图的输出数据会传输给下一个子图的输入数据，并且下一个子图需要将输入数据拷贝为本图的device数据，如Graph_2_GPU需要将Graph_1_CPU的输出数据从CPU拷贝到GPU，反过来Graph_3_CPU需要将Graph2GPU的输出数据从GPU拷贝到CPU，子图之间互相切换执行有一定的开销。</p>
<img src="/images/image-20250418185513957.png" alt="image-20250418185513957" style="zoom: 50%;" />
</li>
<li><p><strong>子图合并执行</strong>：将切分后的多个子图进行合并，合并为一个整体的DAG执行，如 <a href="https://openmlsys.github.io/chapter_backend_and_runtime/compute_schedule_and_execute.html#graph-exec-7">图7.5.12</a>所示，通过算子的设备属性来插入拷贝算子以实现不同设备上的算子数据传输，并且拷贝算子也是进入整图中的，从而形成一个大的整图执行，减少子图之间的切换执行开销。</p>
<img src="/images/image-20250418185549176.png" alt="image-20250418185549176" style="zoom:50%;" /></li>
</ul>
<p>下表中对于这两种执行方式进行了对比：</p>
<table>
<thead>
<tr>
<th align="right">执行方式</th>
<th align="right">子图拆分</th>
<th align="right">子图合并</th>
</tr>
</thead>
<tbody><tr>
<td align="right">异构数据传输</td>
<td align="right">子图之间拷贝</td>
<td align="right">算子之间拷贝</td>
</tr>
<tr>
<td align="right">执行额外开销</td>
<td align="right">子图切换执行开销</td>
<td align="right">无</td>
</tr>
<tr>
<td align="right">执行并发粒度</td>
<td align="right">子图并发</td>
<td align="right">算子原生并发</td>
</tr>
</tbody></table>
<p>总结一下，主要是并发粒度的区别。<strong>子图切换执行开销是比较大的，因此子图合并是比较普适的做法。</strong></p>
<blockquote>
<p>mindspore的异构执行是子图合并，而非异构执行是并行执行，因此为子图合并并行执行（任意计算架构都有两个象限）。</p>
</blockquote>
<h4 id="下沉式执行"><a href="#下沉式执行" class="headerlink" title="下沉式执行"></a>下沉式执行</h4><p>下沉式执行是通过专用芯片的SoC架构，将整个或部分计算图一次性调度到芯片上以完成全量数据的计算。</p>
<h2 id="计算图编译器"><a href="#计算图编译器" class="headerlink" title="计算图编译器"></a><font color = brown>计算图编译器</font></h2><blockquote>
<p>先前一章节阐述的是目前主流成熟的AI编译器后端和运行时架构，而面向FPGA或是AIE这样的可重构硬件，往往无法直接复用ASIC NPU的编译流程（缺乏成熟的runtime系统）。本章节结合Efficient processing for DNN的chapter6，讲解如何将一个dataflow graph映射到这些硬件上去。</p>
</blockquote>
<p><img src="/images/image-20250419141303976.png" alt="image-20250419141303976"></p>
<p>上图摘自cornell的机器学习硬件课程slides。在众多参考资料中，将dataflow映射到FPGA或是特定NPU（如AIE），被称为Mapping，而不是Compiler。如下引用Efficient processing for DNN一书中的一段话，讲明两者的类比关系：</p>
<blockquote>
<p>In conventional computer systems, the compiler translates the program into machine-readable binary codes for execution; in the processing of DNNs, the mapper translates the desired DNN layer computation (i.e., problem specification) along with its shape and size4 into a hardwarecompatible mapping for execution. While the compiler usually optimizes just for performance, the mapper will typically optimize for performance and&#x2F;or energy efficiency.</p>
<img src="/images/image-20250419154151487.png" alt="image-20250419154151487" style="zoom:50%;" />

<img src="/images/image-20250419154218830.png" alt="image-20250419154218830" style="zoom:50%;" />

<p>总结一下，有如下异同点：</p>
<ul>
<li>输入方面，传统编译是程序，而DNN加速器映射是问题特性以及输入的shape。</li>
<li>Compiler的结果是可执行程序，而mapper是硬件的配置文件。</li>
<li>需要的硬件信息方面，编译器需要知道处理器的架构和微架构，而mapper需要知道DNN加速器的数据流和约束。</li>
</ul>
</blockquote>
<p><img src="/images/image-20250419141926408.png" alt="image-20250419141926408"></p>
<p>上图是计算图编译器的一个典型架构。</p>
<p><img src="/images/image-20250419143546984.png" alt="image-20250419143546984"></p>
<h2 id="典型编译器解读"><a href="#典型编译器解读" class="headerlink" title="典型编译器解读"></a><font color = brown>典型编译器解读</font></h2><h3 id="TVM-后端架构-Runtime"><a href="#TVM-后端架构-Runtime" class="headerlink" title="TVM 后端架构&amp;Runtime"></a><font color = green>TVM 后端架构&amp;Runtime</font></h3><h4 id="运行时框架图"><a href="#运行时框架图" class="headerlink" title="运行时框架图"></a>运行时框架图</h4><p>TVM编译器的一大优势是支持多硬件平台&#x2F;设备部署，因此其运行时系统设计是重中之重。其运行时系统可以大体拆解为两个部分：</p>
<ul>
<li>每个硬件继承自ModuleNode类，利用<a href="https://zhuanlan.zhihu.com/p/363991566">PackedFunc技术</a>将特定device的设备API封装成统一格式的指令（类似bladeDISC等编译器的类型擦除设计）。</li>
<li>一个Executor调度每一个指令并运行。该Executor分为GraphExecutor（针对静态图）和Relay VM（针对动态图）。</li>
</ul>
<p>如下图所示分别是TVM运行时框架和Relay VM示意图：</p>
<p><img src="/images/image-20250416230058925.png" alt="image-20250416230058925"></p>
<img src="/images/image-20250416231046705.png" alt="image-20250416231046705" style="zoom: 67%;" />

<h3 id="XLA编译器后端-Runtime"><a href="#XLA编译器后端-Runtime" class="headerlink" title="XLA编译器后端&amp;Runtime"></a><font color = green>XLA编译器后端&amp;Runtime</font></h3><p>重点参考<a href="https://zhuanlan.zhihu.com/p/651036523">XLA编译器机制博客</a>。</p>
<h3 id="BladeDISC后端架构-Runtime"><a href="#BladeDISC后端架构-Runtime" class="headerlink" title="BladeDISC后端架构&amp;Runtime"></a><font color = green>BladeDISC后端架构&amp;Runtime</font></h3><h2 id="TPU-MLIR系统代码解读"><a href="#TPU-MLIR系统代码解读" class="headerlink" title="TPU-MLIR系统代码解读"></a><font color = brown>TPU-MLIR系统代码解读</font></h2><h2 id="References"><a href="#References" class="headerlink" title="References"></a><font color = brown>References</font></h2><ol>
<li><a href="https://www.iteye.com/blog/rednaxelafx-492667">虚拟机技术解读</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/504066888">TVM runtime系统讲解</a></li>
<li><a href="https://abdelfattah-class.github.io/ece5545/">Cornell: Machine learning hardware and system课程</a></li>
<li><a href="https://drive.google.com/file/d/1ikgOdZxnMz1ExqwrAiuTY9exbe3yMWbB/view">IREE运行时系统设计</a></li>
</ol>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>机器学习编译</category>
        <category>后端</category>
        <category>运行时</category>
      </categories>
      <tags>
        <tag>机器学习编译器</tag>
        <tag>后端</tag>
        <tag>运行时</tag>
      </tags>
  </entry>
  <entry>
    <title>Allo项目初探</title>
    <url>/2025/03/29/Allo%E9%A1%B9%E7%9B%AE%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<p><img src="/images/image-20250330205827771.png" alt="image-20250330205827771"></p>
<span id="more"></span>

<h2 id="项目构建"><a href="#项目构建" class="headerlink" title="项目构建"></a><font color = brown>项目构建</font></h2><p>主要参考<a href="https://cornell-zhang.github.io/allo/setup/index.html">Allo doc</a>的set up章节，这里我选择源码构建的方式。项目构建流程如下：</p>
<ol>
<li><p>clone项目</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone --recursive git@github.com:micropuma/allo.git</span><br></pre></td></tr></table></figure>
</li>
<li><p>构建llvm子项目</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd allo/externals/llvm-project</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Apply our patch</span></span><br><span class="line">git apply ../llvm_patch</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Python 3.12 is required</span></span><br><span class="line">mkdir -p build</span><br></pre></td></tr></table></figure>

<p>由于需要python bind11，以及后续需要pip install很多python库，因此这里推荐用conda创建虚拟环境：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n allo python=3.12         # python版本一定是3.12</span><br><span class="line">conda activate allo</span><br><span class="line">pip install pybind11</span><br></pre></td></tr></table></figure>

<p>编写shell脚本，一键构建llvm：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd build</span><br><span class="line">cmake -G Ninja ../llvm \</span><br><span class="line">    -DLLVM_ENABLE_PROJECTS=&quot;clang;mlir;openmp&quot; \</span><br><span class="line">    -DLLVM_BUILD_EXAMPLES=ON \</span><br><span class="line">    -DLLVM_TARGETS_TO_BUILD=&quot;host&quot; \</span><br><span class="line">    -DCMAKE_BUILD_TYPE=Release \</span><br><span class="line">    -DLLVM_ENABLE_ASSERTIONS=ON \</span><br><span class="line">    -DLLVM_INSTALL_UTILS=ON \</span><br><span class="line">    -DMLIR_ENABLE_BINDINGS_PYTHON=ON \</span><br><span class="line">    -DPython3_EXECUTABLE=`which python3`</span><br><span class="line">ninja -j $(nproc)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">export</span> environment variable</span></span><br><span class="line">export LLVM_BUILD_DIR=$(pwd)</span><br></pre></td></tr></table></figure>
</li>
<li><p>构建Allo项目，首先回退到项目根目录，运行如下命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">python3 -m pip install -v -e . -i https://pypi.tuna.tsinghua.edu.cn/simple  # 建议用清华镜像，否则容易拉取失败</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="Allo项目使用"><a href="#Allo项目使用" class="headerlink" title="Allo项目使用"></a><font color = brown>Allo项目使用</font></h2><h3 id="Pytorch集成例子"><a href="#Pytorch集成例子" class="headerlink" title="Pytorch集成例子"></a><font color = green>Pytorch集成例子</font></h3><p>Allo项目除了使用ADL（加速定义语言）来描述计算任务外，也可以集成在pytorch中使用。具体参考<a href="https://cornell-zhang.github.io/allo/dive/pytorch.html">allo pytorch集成例子</a>。</p>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        x = x + y</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = Model()</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> allo</span><br><span class="line">example_inputs = [torch.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">10</span>), torch.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">10</span>)]</span><br><span class="line">llvm_mod = allo.frontend.from_pytorch(model, example_inputs=example_inputs)</span><br><span class="line"></span><br><span class="line">golden = model(*example_inputs)</span><br><span class="line">np_inputs = [x.detach().numpy() <span class="keyword">for</span> x <span class="keyword">in</span> example_inputs]</span><br><span class="line">res = llvm_mod(*np_inputs)</span><br><span class="line">torch.testing.assert_close(res, golden.detach().numpy())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Passed!&quot;</span>)</span><br><span class="line"></span><br><span class="line">mod = allo.frontend.from_pytorch(model, example_inputs=example_inputs, target=<span class="string">&quot;vhls&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(mod.hls_code)</span><br></pre></td></tr></table></figure>

<p>执行如下scirpt：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libstdc++.so.6 python test.py</span><br></pre></td></tr></table></figure>

<blockquote>
<p>:key:注意，由于conda环境中的libstdc++版本低，可能会在import allo的时候报错。强制指定系统libstdc++版本即可。</p>
</blockquote>
<h3 id="Allo-AIE集成"><a href="#Allo-AIE集成" class="headerlink" title="Allo AIE集成"></a><font color = green>Allo AIE集成</font></h3><p>Allo项目目前可以集成AIE后端了，目前只局限于aie2（NPU）。该流程是端到端的编译流程，支持从pytorch或是huggingface上的模型lower到aie后端，并执行。具体流程如下：</p>
<p><img src="/images/image-20250407111644037.png" alt="image-20250407111644037"></p>
<p>目前存在的流程问题：</p>
<ul>
<li><p>Pytorch只能target到cpu和fpga上，目前没有支持pytorch到aie的端到端流程。</p>
<p><img src="/images/image-20250407124451930.png" alt="image-20250407124451930"></p>
</li>
<li><p>对于aie，目前allo支持用python编写算子，并且只能支持aie2（NPU），没有支持VCK190。</p>
<p><img src="/images/image-20250407124501753.png" alt="image-20250407124501753"></p>
</li>
<li><p>Pytorch算子支持受局限。</p>
<p><img src="/images/image-20250407124509106.png" alt="image-20250407124509106"></p>
</li>
</ul>
<h2 id="Allo项目架构"><a href="#Allo项目架构" class="headerlink" title="Allo项目架构"></a><font color = brown>Allo项目架构</font></h2><p><img src="/images/image-20250331184126715.png" alt="image-20250331184126715"></p>
<h3 id="前端"><a href="#前端" class="headerlink" title="前端"></a><font color = green>前端</font></h3><p><code>Allo</code>项目在前端使用python语言编写，主要完成<strong>python系统</strong>和<strong>pytorch系统</strong>到mlir系统的接入。针对pytorch系统，用于可以使用<code>torch.compile(model, &quot;allo&quot;)</code>将pytorch代码编译成allo的中间表达形式，借助于TorchDynamo系统。具体的，基于<code>torch.fx</code>作为高层ir，将其中的每一个pytorch计算单元翻译成allo的function call，后续转换和优化交由allo来完成。硬件无关优化比如算子融合在torch系统中完成，而硬件相关优化则由allo完成，两者是正交关系。</p>
<h2 id="代码生成"><a href="#代码生成" class="headerlink" title="代码生成"></a><font color = green>代码生成</font></h2><p>用户通过<code>s.build(&lt;target&gt;)</code>来针对指定硬件生成代码。目前支持的后端硬件有：CPU，AMD的fpga以及AMD AIE（Ryzen AI）。</p>
<h2 id="中间优化"><a href="#中间优化" class="headerlink" title="中间优化"></a><font color = green>中间优化</font></h2><p>这一部分完全在mlir系统中完成所有的优化。</p>
<blockquote>
<p>Allo项目的框架是比较简单的，后续会结合源代码详细解读。</p>
</blockquote>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color = brown>参考资料</font></h2><ol>
<li><a href="https://cornell-zhang.github.io/allo/">Allo doc</a></li>
<li><a href="http://arxiv.org/abs/2404.04815">Allo paper</a></li>
<li><a href="https://github.com/cornell-zhang/allo">Allo project</a></li>
</ol>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>机器学习编译</category>
        <category>异构设计</category>
      </categories>
      <tags>
        <tag>机器学习编译器</tag>
        <tag>mlir</tag>
        <tag>异构计算系统</tag>
      </tags>
  </entry>
  <entry>
    <title>BladeDISC初探</title>
    <url>/2025/04/01/BladeDISC%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<p><img src="/images/image-20250401195537142.png" alt="image-20250401195537142"></p>
<span id="more"></span>

<blockquote>
<p>在大模型训练推理场景中，一个十分大的瓶颈是动态shape问题。比如nlp领域，处理的句子长短不一，tensor的shape是动态变化的，到runtime才能确定。这给机器学习编译器带来很大的困扰，以XLA为首的sota编译器均是静态shape的，在性能上会有一定损失。BladeDISC是阿里提出的针对动态shape的机器学习编译器，并且经过大量实验和实际生产检验。本文重点关注BladeDISC的构建，pytorch使用方式以及基础架构解读。后续文章会讲解优化流程和论文解读。</p>
</blockquote>
<h2 id="源码构建"><a href="#源码构建" class="headerlink" title="源码构建"></a><font color = brown>源码构建</font></h2><h4 id="Build-from-source"><a href="#Build-from-source" class="headerlink" title="Build from source"></a>Build from source</h4><ul>
<li><p>下载<a href="https://hub.docker.com/r/bladedisc/bladedisc/tags?page=1&name=devel">BladeDisc</a>镜像</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker pull bladedisc/bladedisc:latest-devel-cu118</span><br></pre></td></tr></table></figure>

<ul>
<li>使用cu118版本</li>
</ul>
</li>
<li><p>运行该镜像</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker run --gpus all -it -v $PWD:/disc bladedisc/bladedisc:latest-devel-cu118 bash</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改一下<code>pytorch_blade/scripts/build_pytorch_blade.sh</code>里面的<code>TORCH_BLADE_CI_BUILD_TORCH_VERSION</code>。修改为存在的<code>requirements.txt</code>即可。</p>
<blockquote>
<p>构建过程中，onnx由于带宽等问题，可能会报error，添加-i <a href="https://pypi.tuna.tsinghua.edu.cn/simple%E6%8C%87%E5%AE%9Apypi%E9%95%9C%E5%83%8F%E5%8D%B3%E5%8F%AF%E3%80%82">https://pypi.tuna.tsinghua.edu.cn/simple指定pypi镜像即可。</a></p>
</blockquote>
</li>
<li><p>pytorch版本构建</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd pytorch_blade &amp;&amp; bash ./scripts/build_pytorch_blade.sh</span><br><span class="line">python setup.py bdist_wheel</span><br><span class="line">pip install ./pytorch_blade/dist/torch_blade-0.2.0+2.0.1.cu118-cp38-cp38-linux_x86_64.whl</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a><font color = green>错误处理</font></h3><p>如果报错没有安全git，在docker中用：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git config --global --add safe.directory /disc</span><br></pre></td></tr></table></figure>

<h4 id="quick-install"><a href="#quick-install" class="headerlink" title="quick install"></a>quick install</h4><p>参考<a href="https://github.com/alibaba/BladeDISC/blob/main/docs/install_with_docker.md">docker install</a></p>
<h2 id="Pytorch部署BERT模型"><a href="#Pytorch部署BERT模型" class="headerlink" title="Pytorch部署BERT模型"></a><font color = brown>Pytorch部署BERT模型</font></h2><h3 id="Hugging-Face模型下载"><a href="#Hugging-Face模型下载" class="headerlink" title="Hugging Face模型下载"></a><font color = green>Hugging Face模型下载</font></h3><ul>
<li><p>手动下载模型（适合服务器联网不稳定的情况使用）</p>
<p>找到<a href="https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment/tree/main">Bert sentiment inference 模型</a>，主要手动下载如下几个文件：</p>
<p><img src="/images/image-20250306203839463.png" alt="image-20250306203839463"></p>
<p>在python代码中使用离线下载的模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_path = <span class="string">&quot;./model&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_path)</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(model_path).cuda().<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
</li>
<li><p>直接通过transformers 包下载，该下载方式通过huggingface对应模型网页的<code>use this model</code>获取</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Load model directly</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>)</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(<span class="string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>确保环境有transformers包即可</p>
</li>
<li><p>通过<code>huggingface-cli</code>下载</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">huggingface-cli download nlptown/bert-base-multilingual-uncased-sentiment</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="做BERT-Inference的testbench"><a href="#做BERT-Inference的testbench" class="headerlink" title="做BERT Inference的testbench"></a><font color = green>做BERT Inference的testbench</font></h3><p>我的测试codes如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch_blade</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> (</span><br><span class="line">    pipeline,</span><br><span class="line">    AutoTokenizer,</span><br><span class="line">    AutoModelForSequenceClassification,</span><br><span class="line">    TextClassificationPipeline,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">############################################# download model from huggingface #############################################</span></span><br><span class="line">model_path = <span class="string">&quot;./model&quot;</span></span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_path)</span><br><span class="line"></span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(model_path).cuda().<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plain_tokenizer</span>(<span class="params">inputs_str, return_tensors</span>):</span><br><span class="line">    inputs = tokenizer(inputs_str, return_tensors=return_tensors, padding=<span class="literal">True</span>)</span><br><span class="line">    inputs = &#123;key: value.cuda() <span class="keyword">for</span> key, value <span class="keyword">in</span> inputs.items()&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># torch_blade.optimize 不支持 None 作为输入</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;token_type_ids&quot;</span> <span class="keyword">in</span> inputs <span class="keyword">and</span> inputs[<span class="string">&quot;token_type_ids&quot;</span>] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">del</span> inputs[<span class="string">&quot;token_type_ids&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (inputs[<span class="string">&#x27;input_ids&#x27;</span>], inputs[<span class="string">&#x27;attention_mask&#x27;</span>], inputs.get(<span class="string">&#x27;token_type_ids&#x27;</span>, <span class="literal">None</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PlainTextClassificationPipeline</span>(<span class="title class_ inherited__">TextClassificationPipeline</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, model_inputs</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.model(*model_inputs)</span><br><span class="line"></span><br><span class="line">classifier = pipeline(</span><br><span class="line">    <span class="string">&#x27;sentiment-analysis&#x27;</span>,</span><br><span class="line">    model=model,</span><br><span class="line">    tokenizer=plain_tokenizer,</span><br><span class="line">    pipeline_class=PlainTextClassificationPipeline,</span><br><span class="line">    device=<span class="number">0</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">input_strs = [</span><br><span class="line">    <span class="string">&quot;We are very happy to show you the story.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;We hope you don&#x27;t hate it.&quot;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">results = classifier(input_strs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> inp_str, result <span class="keyword">in</span> <span class="built_in">zip</span>(input_strs, results):</span><br><span class="line">    <span class="built_in">print</span>(inp_str)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot; label: <span class="subst">&#123;result[<span class="string">&#x27;label&#x27;</span>]&#125;</span>, with a score: <span class="subst">&#123;<span class="built_in">round</span>(result[<span class="string">&#x27;score&#x27;</span>], <span class="number">4</span>)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">############################################# Use BladeDISC for optimization #############################################</span></span><br><span class="line">inputs_str = <span class="string">&quot;Hey, the cat is cute.&quot;</span></span><br><span class="line">inputs = plain_tokenizer(inputs_str, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"></span><br><span class="line">torch_config = torch_blade.config.Config()</span><br><span class="line">torch_config.enable_mlir_amp = <span class="literal">False</span>  <span class="comment"># disable mix-precision</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Ensure inputs are properly formatted for optimization</span></span><br><span class="line">model_inputs = <span class="built_in">tuple</span>(i <span class="keyword">for</span> i <span class="keyword">in</span> inputs <span class="keyword">if</span> i <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad(), torch_config:</span><br><span class="line">    optimized_ts = torch_blade.optimize(model, allow_tracing=<span class="literal">True</span>, model_inputs=model_inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Move optimized model to CUDA</span></span><br><span class="line">optimized_ts = optimized_ts.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save the optimized TorchScript model</span></span><br><span class="line">torch.jit.save(optimized_ts, <span class="string">&quot;opt.disc.pt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">############################################# testbench #############################################</span></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">benchmark</span>(<span class="params">model, inputs, num_iters=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        model(*inputs)</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line"></span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        model(*inputs)</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    end = time.time()</span><br><span class="line">    <span class="keyword">return</span> (end - start) / num_iters * <span class="number">1000.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bench_and_report</span>(<span class="params">input_strs</span>):</span><br><span class="line">    inputs = plain_tokenizer(input_strs, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">    model_inputs = <span class="built_in">tuple</span>(i <span class="keyword">for</span> i <span class="keyword">in</span> inputs <span class="keyword">if</span> i <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">    avg_latency_baseline = benchmark(model, model_inputs)</span><br><span class="line">    avg_latency_bladedisc = benchmark(optimized_ts, model_inputs)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Seqlen: <span class="subst">&#123;[<span class="built_in">len</span>(s) <span class="keyword">for</span> s <span class="keyword">in</span> input_strs]&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Baseline: <span class="subst">&#123;avg_latency_baseline:<span class="number">.4</span>f&#125;</span> ms&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;BladeDISC: <span class="subst">&#123;avg_latency_bladedisc:<span class="number">.4</span>f&#125;</span> ms&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;BladeDISC speedup: <span class="subst">&#123;avg_latency_baseline / avg_latency_bladedisc:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">input_strs = [</span><br><span class="line">    <span class="string">&quot;We are very happy to show you the story.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;We hope you don&#x27;t hate it.&quot;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">bench_and_report(input_strs)</span><br></pre></td></tr></table></figure>

<p>上述codes中，BladeDISC的核心如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad(), torch_config:</span><br><span class="line">    optimized_ts = torch_blade.optimize(model, allow_tracing=<span class="literal">True</span>, model_inputs=model_inputs)</span><br></pre></td></tr></table></figure>

<p>通过编译手段，生成优化后的pytorch script，注意：<strong>目前pytorch仅仅支持inference，尚不支持train</strong>。对于<code>HuggingFace</code>模型的pipeline有更深层兴趣的，参考<a href="https://huggingface.co/docs/transformers/quicktour">HuggingFace quick tour</a>。</p>
<h3 id="Pytorch-WorkFlow"><a href="#Pytorch-WorkFlow" class="headerlink" title="Pytorch WorkFlow"></a><font color = green>Pytorch WorkFlow</font></h3><p><img src="/images/image-20250311211223452.png" alt="image-20250311211223452"></p>
<p><img src="/images/image-20250311220823276.png" alt="image-20250311220823276"></p>
<p>参考<a href="https://github.com/alibaba/BladeDISC/blob/main/docs/developers/bladedisc_torch_overview.md">Torch-Blade教程</a>即可。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color = brown>参考资料</font></h2><ol>
<li><a href="https://github.com/alibaba/BladeDISC">BladeDisc github</a></li>
</ol>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>机器学习编译</category>
        <category>动态shape</category>
      </categories>
      <tags>
        <tag>机器学习编译器</tag>
        <tag>mlir</tag>
        <tag>动态shape</tag>
      </tags>
  </entry>
  <entry>
    <title>MLIR Basic: DDR framework</title>
    <url>/2025/04/12/MLIR-Basic-DDR-framework/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>MLIR Basic: ODS framework</title>
    <url>/2025/04/12/MLIR-Basic-ODS-framework/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Pytorch新特性解读</title>
    <url>/2025/04/05/Pytorch%E6%96%B0%E7%89%B9%E6%80%A7%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p><img src="/images/image-20250405105059068.png" alt="image-20250405105059068"></p>
<span id="more"></span>

<blockquote>
<p>在pytorch2中，pytorch增加了许多新的特性，其中最重要的是<code>torchdynamo</code>和<code>torchinductor</code>。本篇博客结合pytorch2在asplos的paper，解读新特性背后的编译技术。</p>
</blockquote>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>机器学习编译</category>
        <category>机器学习系统</category>
      </categories>
      <tags>
        <tag>机器学习编译器</tag>
        <tag>pytorch技术</tag>
        <tag>静态图捕捉和编译</tag>
      </tags>
  </entry>
  <entry>
    <title>MLIR debug tips</title>
    <url>/2025/04/20/MLIR-debug-tips/</url>
    <content><![CDATA[<h2 id="VS-Code-Debugger"><a href="#VS-Code-Debugger" class="headerlink" title="VS Code Debugger"></a><font color = brown>VS Code Debugger</font></h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-exec set scheduler-locking step    # 冻结其他线程</span><br></pre></td></tr></table></figure>



<h2 id="MLIR-Debug-Practice"><a href="#MLIR-Debug-Practice" class="headerlink" title="MLIR Debug Practice"></a><font color = brown>MLIR Debug Practice</font></h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Legalizing operation : &#x27;vector.transfer_read&#x27;(0x62c0434198a0) &#123;</span><br><span class="line"><span class="meta prompt_">  %</span><span class="language-bash">6 = <span class="string">&quot;vector.transfer_read&quot;</span>(%0, %arg3, %5) &#123;operand_segment_sizes = dense&lt;[1, 1, 1, 0]&gt; : vector&lt;4xi32&gt;, permutation_map = affine_map&lt;(d0) -&gt; (d0)&gt;&#125; : (memref&lt;32xf32&gt;, index, f32) -&gt; vector&lt;8xf32&gt;</span></span><br><span class="line"></span><br><span class="line">  * Fold &#123;</span><br><span class="line">  &#125; -&gt; FAILURE : unable to fold</span><br><span class="line"></span><br><span class="line">  * Pattern : &#x27;vector.transfer_read -&gt; ()&#x27; &#123;</span><br><span class="line">    ** Insert  : &#x27;arith.constant&#x27;(0x62c0433289e0)</span><br><span class="line">    ** Insert  : &#x27;arith.constant&#x27;(0x62c04332a260)</span><br><span class="line">    ** Insert  : &#x27;arith.subi&#x27;(0x62c0432f4b50)</span><br><span class="line">    ** Insert  : &#x27;vector.create_mask&#x27;(0x62c043329e30)</span><br><span class="line"></span><br><span class="line">    //===-------------------------------------------===//</span><br><span class="line">    Legalizing operation : &#x27;vector.transfer_read&#x27;(0x62c0434198a0) &#123;</span><br><span class="line">      // -----// IR Dump After AffineScalarReplacement //----- //</span><br><span class="line"><span class="meta prompt_">%</span><span class="language-bash">10 = <span class="string">&quot;vector.transfer_read&quot;</span>(%0, %arg3, %5, %9) &#123;in_bounds = [<span class="literal">true</span>], operand_segment_sizes = dense&lt;1&gt; : vector&lt;4xi32&gt;, permutation_map = affine_map&lt;(d0) -&gt; (d0)&gt;&#125; : (memref&lt;32xf32&gt;, index, f32, vector&lt;8xi1&gt;) -&gt; vector&lt;8xf32&gt;</span></span><br><span class="line"></span><br><span class="line">      * Fold &#123;</span><br><span class="line">      &#125; -&gt; FAILURE : unable to fold</span><br><span class="line"></span><br><span class="line">      * Pattern : &#x27;vector.transfer_read -&gt; ()&#x27; &#123;</span><br><span class="line">      &#125; -&gt; FAILURE : pattern was already applied</span><br><span class="line"></span><br><span class="line">      * Pattern : &#x27;vector.transfer_read -&gt; ()&#x27; &#123;</span><br><span class="line">        ** Insert  : &#x27;vector.splat&#x27;(0x62c04332a5b0)</span><br><span class="line">        ** Insert  : &#x27;vector.maskedload&#x27;(0x62c04329aad0)</span><br><span class="line">        ** Replace : &#x27;vector.transfer_read&#x27;(0x62c0434198a0)</span><br><span class="line"></span><br><span class="line">        //===-------------------------------------------===//</span><br><span class="line">        Legalizing operation : &#x27;vector.splat&#x27;(0x62c04332a5b0) &#123;</span><br><span class="line">          %10 = &quot;vector.splat&quot;(%5) : (f32) -&gt; vector&lt;8xf32&gt;</span><br><span class="line"></span><br><span class="line">          * Fold &#123;</span><br><span class="line">            ** Insert  : &#x27;arith.constant&#x27;(0x62c043420680)</span><br><span class="line">            ** Replace : &#x27;vector.splat&#x27;(0x62c04332a5b0)</span><br><span class="line"></span><br><span class="line">            //===-------------------------------------------===//</span><br><span class="line">            Legalizing operation : &#x27;arith.constant&#x27;(0x62c043420680) &#123;</span><br><span class="line">              %10 = &quot;arith.constant&quot;() &#123;value = dense&lt;0.000000e+00&gt; : vector&lt;8xf32&gt;&#125; : () -&gt; vector&lt;8xf32&gt;</span><br><span class="line"></span><br><span class="line">              * Fold &#123;</span><br><span class="line">              &#125; -&gt; FAILURE : unable to fold</span><br><span class="line">            &#125; -&gt; FAILURE : no matched legalization pattern</span><br><span class="line">            //===-------------------------------------------===//</span><br><span class="line">          &#125; -&gt; FAILURE : failed to legalize generated constant &#x27;arith.constant&#x27;</span><br><span class="line">        &#125; -&gt; FAILURE : no matched legalization pattern</span><br><span class="line">        //===-------------------------------------------===//</span><br><span class="line">      &#125; -&gt; FAILURE : failed to legalize generated operation &#x27;vector.splat&#x27;(0x000062C04332A5B0)</span><br><span class="line">    &#125; -&gt; FAILURE : pattern failed to match</span><br><span class="line">  &#125; -&gt; FAILURE : no matched legalization pattern</span><br><span class="line">  //===-------------------------------------------===//</span><br><span class="line">&#125; -&gt; FAILURE : failed to legalize operation updated in-place &#x27;vector.transfer_read&#x27;</span><br><span class="line">&#125; -&gt; FAILURE : pattern failed to match</span><br><span class="line"></span><br><span class="line">* Pattern : &#x27;vector.transfer_read -&gt; ()&#x27; &#123;</span><br><span class="line">&#125; -&gt; FAILURE : pattern failed to match</span><br><span class="line">&#125; -&gt; FAILURE : no matched legalization pattern</span><br><span class="line">//===-------------------------------------------===//</span><br></pre></td></tr></table></figure>





<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color = brown>参考资料</font></h2><ol>
<li><a href="https://vscode.js.cn/docs/debugtest/debugging">vscode debug文档</a></li>
</ol>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>mlir</category>
        <category>debug</category>
      </categories>
      <tags>
        <tag>mlir</tag>
        <tag>vscode debug</tag>
      </tags>
  </entry>
  <entry>
    <title>Transform Dialect tutorial1</title>
    <url>/2025/04/08/Transform-Dialect-tutorial1/</url>
    <content><![CDATA[<p><img src="/images/image-20250408184457571.png" alt="image-20250408184457571"></p>
<span id="more"></span>

<blockquote>
<p>Transform dialect是mlir基础设施实现的调度dsl。通过在同一个mlir文件中，使用标准ir定义计算任务（payload ir），利用transform ir定义调度方式（schedule ir），最后借助于transform-interpreter注册和使用整个pass。</p>
</blockquote>
<h2 id="Motivation-of-Transform-Dialect"><a href="#Motivation-of-Transform-Dialect" class="headerlink" title="Motivation of Transform Dialect"></a><font color = brown>Motivation of Transform Dialect</font></h2><p>总结许多成熟编译器，比如Halide，TVM，TC等，可以发现如下规律：</p>
<ol>
<li><strong>调度表示</strong>（Schedule Representation）：以结构化数据描述优化流程的元信息集合</li>
<li><strong>声明式规范</strong>（Declarative Specification）：通过定义预期目标状态而非具体操作步骤进行配置</li>
<li><strong>多版本化</strong>（Multi-Versioning）：针对不同硬件&#x2F;场景生成多个优化方案分支</li>
<li><strong>运行时调度</strong>（Runtime Dispatch）：通过动态决策机制选择最优版本执行</li>
<li><strong>垂直时序控制</strong>（Vertical Sequencing）：在单一功能域内进行深度优化组合</li>
</ol>
<p>MLIR基础设施中，也希望提供用户类似TVM的计算调度分离的能力，从而方便用户对于一个计算任务自定义调度优化策略。在MLIR中，一切都是<code>op</code>操作，调度操作也是<code>op</code>，封装在transform这个dialect中，这便是transform dialect的由来。</p>
<blockquote>
<p>后续教程主要来源于<a href="https://mlir.llvm.org/docs/Tutorials/transform/">transform tutorial</a></p>
</blockquote>
<h2 id="Chapter1：利用transform-op组建pipeline"><a href="#Chapter1：利用transform-op组建pipeline" class="headerlink" title="Chapter1：利用transform op组建pipeline"></a><font color = brown>Chapter1：利用transform op组建pipeline</font></h2><p>transform dialect内置丰富的schedule op，计算调度分离机制使得用户需要定义payload ir以及schedule ir。在transform的官方tutorial中给的例子，是将一个矩阵乘-逐项加-relu操作，利用transform op做调度优化。</p>
<ul>
<li><p>payload ir（详细定义计算任务）：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Original function to optimize.</span></span><br><span class="line">func.func @<span class="built_in">fc_relu</span>(%lhs: tensor&lt;<span class="number">512</span>x512xf32&gt;, %rhs: tensor&lt;<span class="number">512</span>x512xf32&gt;,</span><br><span class="line">                   %bias: tensor&lt;<span class="number">512</span>x512xf32&gt;, %output: tensor&lt;<span class="number">512</span>x512xf32&gt;)</span><br><span class="line">                   -&gt; tensor&lt;<span class="number">512</span>x512xf32&gt; &#123;</span><br><span class="line">  <span class="comment">// Matrix-matrix multiplication.</span></span><br><span class="line">  %matmul = linalg.matmul <span class="built_in">ins</span>(%lhs, %rhs: tensor&lt;<span class="number">512</span>x512xf32&gt;, tensor&lt;<span class="number">512</span>x512xf32&gt;)</span><br><span class="line">                          <span class="built_in">outs</span>(%output: tensor&lt;<span class="number">512</span>x512xf32&gt;) -&gt; tensor&lt;<span class="number">512</span>x512xf32&gt;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Elementwise addition.</span></span><br><span class="line">  %biased = linalg.elemwise_binary &#123; fun = <span class="meta">#linalg.binary_fn<span class="string">&lt;add&gt;</span> &#125;</span></span><br><span class="line">    <span class="built_in">ins</span>(%matmul, %bias : tensor&lt;<span class="number">512</span>x512xf32&gt;, tensor&lt;<span class="number">512</span>x512xf32&gt;)</span><br><span class="line">    <span class="built_in">outs</span>(%output : tensor&lt;<span class="number">512</span>x512xf32&gt;) -&gt; tensor&lt;<span class="number">512</span>x512xf32&gt;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Elementwise max with 0 (ReLU).</span></span><br><span class="line">  %c0f = arith.constant <span class="number">0.0</span> : f32</span><br><span class="line">  %relued = linalg.elemwise_binary &#123; fun = <span class="meta">#linalg.binary_fn<span class="string">&lt;max_signed&gt;</span> &#125;</span></span><br><span class="line">    <span class="built_in">ins</span>(%biased, %c0f : tensor&lt;<span class="number">512</span>x512xf32&gt;, f32)</span><br><span class="line">    <span class="built_in">outs</span>(%output : tensor&lt;<span class="number">512</span>x512xf32&gt;) -&gt; tensor&lt;<span class="number">512</span>x512xf32&gt;</span><br><span class="line">  func.<span class="keyword">return</span> %relued : tensor&lt;<span class="number">512</span>x512xf32&gt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>payload ir的定义就使用linalg等标准mlir ir定义即可。</p>
</li>
<li><p>schedule ir（调度原语）</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">module</span> attributes &#123;transform.with_named_sequence&#125; &#123;  </span><br><span class="line">  transform.named_sequence @__transform_main(</span><br><span class="line">      %arg0: !transform.any_op,</span><br><span class="line">      %arg1: !transform.op&lt;<span class="string">&quot;linalg.matmul&quot;</span>&gt;,</span><br><span class="line">      %arg2: !transform.op&lt;<span class="string">&quot;linalg.elemwise_binary&quot;</span>&gt;) &#123;</span><br><span class="line">    <span class="comment">// Since the %arg2 handle is associated with both elementwise operations,</span></span><br><span class="line">    <span class="comment">// we need to split it into two handles so we can target only the second</span></span><br><span class="line">    <span class="comment">// elementwise operation.</span></span><br><span class="line">    %add, %max = transform.split_handle %arg2 : (!transform.op&lt;<span class="string">&quot;linalg.elemwise_binary&quot;</span>&gt;)</span><br><span class="line">        -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// The actual tiling transformation takes tile sizes as attributes. It produces a</span></span><br><span class="line">    <span class="comment">// handle to the loop generated during tiling.</span></span><br><span class="line">    %tiled, %loop = transform.structured.tile_using_forall %max tile_sizes [<span class="number">8</span>, <span class="number">32</span>]</span><br><span class="line">        : (!transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// We can now fuse the other operations into the loop. Here, we fuse</span></span><br><span class="line">    <span class="comment">// operations one-by-one. This requires the operation that is being fused</span></span><br><span class="line">    <span class="comment">// to define the value used within the loop, so the order of such fusions</span></span><br><span class="line">    <span class="comment">// is important. We could also use &quot;transform.merge_handles&quot; to obtain</span></span><br><span class="line">    <span class="comment">// a single handle to all operations and give it to `fuse_into_containing_op`</span></span><br><span class="line">    <span class="comment">// that would take care of the ordering in this case</span></span><br><span class="line">    %add_fused, %loop2 = transform.structured.fuse_into_containing_op %add into %loop</span><br><span class="line">        : (!transform.any_op, !transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">    %matmul_fused, %loop3 = transform.structured.fuse_into_containing_op %arg1 into %loop2</span><br><span class="line">        : (!transform.op&lt;<span class="string">&quot;linalg.matmul&quot;</span>&gt;, !transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Tile again to get the desired size. Note that this time this tiles the</span></span><br><span class="line">    <span class="comment">// &quot;add&quot; operation and fuses matmul into the loop, but doesn&#x27;t affect the</span></span><br><span class="line">    <span class="comment">// &quot;max&quot; operation. This illustrates the precise targeting with the transform</span></span><br><span class="line">    <span class="comment">// dialect. Otherwise, it is difficult to differentiate &quot;add&quot; and &quot;max&quot;, both</span></span><br><span class="line">    <span class="comment">// of which having the same kind.</span></span><br><span class="line">    %tiled_second, %loop_second = transform.structured.tile_using_forall %add_fused tile_sizes [<span class="number">4</span>, <span class="number">4</span>]</span><br><span class="line">        : (!transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">    %matmul_fused_2, %loop_second_2 =</span><br><span class="line">        transform.structured.fuse_into_containing_op %matmul_fused into %loop_second</span><br><span class="line">        : (!transform.any_op, !transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// // Since outlining is currently only implemented for region-holding operations</span></span><br><span class="line">    <span class="comment">// // such as loops, use tiling to size 1 to materialize the outer loop that is</span></span><br><span class="line">    <span class="comment">// // going to be outlined.</span></span><br><span class="line">    <span class="comment">// %_0, %loop_third = transform.structured.tile_using_forall %tiled_second tile_sizes [1]</span></span><br><span class="line">    <span class="comment">//     : (!transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span></span><br><span class="line">    <span class="comment">// %_1, %outline_target = transform.structured.fuse_into_containing_op %matmul_fused_2 into %loop_third</span></span><br><span class="line">    <span class="comment">//     : (!transform.any_op, !transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span></span><br><span class="line">    <span class="comment">// %func, %call = transform.loop.outline %outline_target &#123;func_name = &quot;outlined&quot;&#125;</span></span><br><span class="line">    <span class="comment">//     : (!transform.any_op) -&gt; (!transform.any_op, !transform.op&lt;&quot;func.call&quot;&gt;)</span></span><br><span class="line">  </span><br><span class="line">    transform.yield</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>完整代码片段如下。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// RUN: mlir-opt %s \</span></span><br><span class="line"><span class="comment">// RUN:   --pass-pipeline=&quot;builtin.module(transform-interpreter&#123; \</span></span><br><span class="line"><span class="comment">// RUN:        debug-bind-trailing-args=linalg.matmul,linalg.elemwise_binary&#125;,\</span></span><br><span class="line"><span class="comment">// RUN:        canonicalize,cse,symbol-dce)&quot; |\</span></span><br><span class="line"><span class="comment">// RUN: FileCheck %s</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ****************************** IMPORTANT NOTE ******************************</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// If you are changing this file, you may also need to change</span></span><br><span class="line"><span class="comment">// mlir/docs/Tutorials/Transform accordingly.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// ****************************************************************************</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Original function to optimize.</span></span><br><span class="line">func.func @<span class="built_in">fc_relu</span>(%lhs: tensor&lt;<span class="number">512</span>x512xf32&gt;, %rhs: tensor&lt;<span class="number">512</span>x512xf32&gt;,</span><br><span class="line">                   %bias: tensor&lt;<span class="number">512</span>x512xf32&gt;, %output: tensor&lt;<span class="number">512</span>x512xf32&gt;)</span><br><span class="line">                   -&gt; tensor&lt;<span class="number">512</span>x512xf32&gt; &#123;</span><br><span class="line">  <span class="comment">// Matrix-matrix multiplication.</span></span><br><span class="line">  %matmul = linalg.matmul <span class="built_in">ins</span>(%lhs, %rhs: tensor&lt;<span class="number">512</span>x512xf32&gt;, tensor&lt;<span class="number">512</span>x512xf32&gt;)</span><br><span class="line">                          <span class="built_in">outs</span>(%output: tensor&lt;<span class="number">512</span>x512xf32&gt;) -&gt; tensor&lt;<span class="number">512</span>x512xf32&gt;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Elementwise addition.</span></span><br><span class="line">  %biased = linalg.elemwise_binary &#123; fun = <span class="meta">#linalg.binary_fn<span class="string">&lt;add&gt;</span> &#125;</span></span><br><span class="line">    <span class="built_in">ins</span>(%matmul, %bias : tensor&lt;<span class="number">512</span>x512xf32&gt;, tensor&lt;<span class="number">512</span>x512xf32&gt;)</span><br><span class="line">    <span class="built_in">outs</span>(%output : tensor&lt;<span class="number">512</span>x512xf32&gt;) -&gt; tensor&lt;<span class="number">512</span>x512xf32&gt;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Elementwise max with 0 (ReLU).</span></span><br><span class="line">  %c0f = arith.constant <span class="number">0.0</span> : f32</span><br><span class="line">  %relued = linalg.elemwise_binary &#123; fun = <span class="meta">#linalg.binary_fn<span class="string">&lt;max_signed&gt;</span> &#125;</span></span><br><span class="line">    <span class="built_in">ins</span>(%biased, %c0f : tensor&lt;<span class="number">512</span>x512xf32&gt;, f32)</span><br><span class="line">    <span class="built_in">outs</span>(%output : tensor&lt;<span class="number">512</span>x512xf32&gt;) -&gt; tensor&lt;<span class="number">512</span>x512xf32&gt;</span><br><span class="line">  func.<span class="keyword">return</span> %relued : tensor&lt;<span class="number">512</span>x512xf32&gt;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// CHECK: func @outlined</span></span><br><span class="line"><span class="comment">// CHECK:   linalg.matmul</span></span><br><span class="line"><span class="comment">// CHECK:   linalg.elemwise_binary &#123;fun = #linalg.binary_fn&lt;add&gt;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// CHECK-LABEL: func @fc_relu</span></span><br><span class="line"><span class="comment">// CHECK: scf.forall</span></span><br><span class="line"><span class="comment">// CHECK:   scf.forall</span></span><br><span class="line"><span class="comment">// CHECK:     %[[SLICE4:.+]] = tensor.extract_slice</span></span><br><span class="line"><span class="comment">// CHECK:     %[[SLICE5:.+]] = tensor.extract_slice</span></span><br><span class="line"><span class="comment">// CHECK:     %[[SLICE6:.+]] = tensor.extract_slice</span></span><br><span class="line"><span class="comment">// CHECK:     %[[SLICE7:.+]] = tensor.extract_slice</span></span><br><span class="line"><span class="comment">// CHECK:     %[[SLICE8:.+]] = tensor.extract_slice</span></span><br><span class="line"><span class="comment">// CHECK:     func.call @outlined(%[[SLICE4]], %[[SLICE5]], %[[SLICE6]], %[[SLICE7]], %[[SLICE8]])</span></span><br><span class="line"><span class="comment">// CHECK-NOT: linalg.matmul</span></span><br><span class="line"><span class="comment">// CHECK-NOT: linalg.elemwise_binary</span></span><br><span class="line"><span class="comment">// CHECK:     scf.forall.in_parallel</span></span><br><span class="line"><span class="comment">// CHECK:   linalg.elemwise_binary &#123;fun = #linalg.binary_fn&lt;max_signed&gt;&#125;</span></span><br><span class="line"><span class="comment">// CHECK:   scf.forall.in_parallel</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Declaration of the &quot;microkernel&quot; function that we will be targeting.</span></span><br><span class="line">func.func <span class="keyword">private</span> @<span class="built_in">microkernel</span>(</span><br><span class="line">    %lhs: tensor&lt;<span class="number">4</span>x512xf32&gt;,</span><br><span class="line">    %rhs: tensor&lt;<span class="number">512</span>x4xf32&gt;,</span><br><span class="line">    %bias: tensor&lt;<span class="number">4</span>x4xf32&gt;,</span><br><span class="line">    %init: tensor&lt;<span class="number">4</span>x4xf32&gt;,</span><br><span class="line">    %output: tensor&lt;<span class="number">4</span>x4xf32&gt;) -&gt; tensor&lt;<span class="number">4</span>x4xf32&gt;</span><br><span class="line"></span><br><span class="line"><span class="keyword">module</span> attributes &#123;transform.with_named_sequence&#125; &#123;  </span><br><span class="line">  transform.named_sequence @__transform_main(</span><br><span class="line">      %arg0: !transform.any_op,</span><br><span class="line">      %arg1: !transform.op&lt;<span class="string">&quot;linalg.matmul&quot;</span>&gt;,</span><br><span class="line">      %arg2: !transform.op&lt;<span class="string">&quot;linalg.elemwise_binary&quot;</span>&gt;) &#123;</span><br><span class="line">    <span class="comment">// Since the %arg2 handle is associated with both elementwise operations,</span></span><br><span class="line">    <span class="comment">// we need to split it into two handles so we can target only the second</span></span><br><span class="line">    <span class="comment">// elementwise operation.</span></span><br><span class="line">    %add, %max = transform.split_handle %arg2 : (!transform.op&lt;<span class="string">&quot;linalg.elemwise_binary&quot;</span>&gt;)</span><br><span class="line">        -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// The actual tiling transformation takes tile sizes as attributes. It produces a</span></span><br><span class="line">    <span class="comment">// handle to the loop generated during tiling.</span></span><br><span class="line">    %tiled, %loop = transform.structured.tile_using_forall %max tile_sizes [<span class="number">8</span>, <span class="number">32</span>]</span><br><span class="line">        : (!transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// We can now fuse the other operations into the loop. Here, we fuse</span></span><br><span class="line">    <span class="comment">// operations one-by-one. This requires the operation that is being fused</span></span><br><span class="line">    <span class="comment">// to define the value used within the loop, so the order of such fusions</span></span><br><span class="line">    <span class="comment">// is important. We could also use &quot;transform.merge_handles&quot; to obtain</span></span><br><span class="line">    <span class="comment">// a single handle to all operations and give it to `fuse_into_containing_op`</span></span><br><span class="line">    <span class="comment">// that would take care of the ordering in this case</span></span><br><span class="line">    %add_fused, %loop2 = transform.structured.fuse_into_containing_op %add into %loop</span><br><span class="line">        : (!transform.any_op, !transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">    %matmul_fused, %loop3 = transform.structured.fuse_into_containing_op %arg1 into %loop2</span><br><span class="line">        : (!transform.op&lt;<span class="string">&quot;linalg.matmul&quot;</span>&gt;, !transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Tile again to get the desired size. Note that this time this tiles the</span></span><br><span class="line">    <span class="comment">// &quot;add&quot; operation and fuses matmul into the loop, but doesn&#x27;t affect the</span></span><br><span class="line">    <span class="comment">// &quot;max&quot; operation. This illustrates the precise targeting with the transform</span></span><br><span class="line">    <span class="comment">// dialect. Otherwise, it is difficult to differentiate &quot;add&quot; and &quot;max&quot;, both</span></span><br><span class="line">    <span class="comment">// of which having the same kind.</span></span><br><span class="line">    %tiled_second, %loop_second = transform.structured.tile_using_forall %add_fused tile_sizes [<span class="number">4</span>, <span class="number">4</span>]</span><br><span class="line">        : (!transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">    %matmul_fused_2, %loop_second_2 =</span><br><span class="line">        transform.structured.fuse_into_containing_op %matmul_fused into %loop_second</span><br><span class="line">        : (!transform.any_op, !transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// // Since outlining is currently only implemented for region-holding operations</span></span><br><span class="line">    <span class="comment">// // such as loops, use tiling to size 1 to materialize the outer loop that is</span></span><br><span class="line">    <span class="comment">// // going to be outlined.</span></span><br><span class="line">    <span class="comment">// %_0, %loop_third = transform.structured.tile_using_forall %tiled_second tile_sizes [1]</span></span><br><span class="line">    <span class="comment">//     : (!transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span></span><br><span class="line">    <span class="comment">// %_1, %outline_target = transform.structured.fuse_into_containing_op %matmul_fused_2 into %loop_third</span></span><br><span class="line">    <span class="comment">//     : (!transform.any_op, !transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span></span><br><span class="line">    <span class="comment">// %func, %call = transform.loop.outline %outline_target &#123;func_name = &quot;outlined&quot;&#125;</span></span><br><span class="line">    <span class="comment">//     : (!transform.any_op) -&gt; (!transform.any_op, !transform.op&lt;&quot;func.call&quot;&gt;)</span></span><br><span class="line">  </span><br><span class="line">    transform.yield</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Chapter2：自定义一个简单的transform-operation"><a href="#Chapter2：自定义一个简单的transform-operation" class="headerlink" title="Chapter2：自定义一个简单的transform operation"></a><font color = brown>Chapter2：自定义一个简单的transform operation</font></h2><h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a><font color = green>目的</font></h3><p>很多时候，transform dialect中的operation无法满足我们调度优化需求。比如在chapter1中，我们最后tiling的小块可以替换成手写的算子microkernel，这个替换操作在transform中没有直接的op。可以实现自定义operation（<code>transform.my.change_call_target</code>），使用方法如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Rewrite the call target.</span></span><br><span class="line">transform.my.change_call_target %call, <span class="string">&quot;microkernel&quot;</span> : !transform.any_op</span><br></pre></td></tr></table></figure>

<p>即将%call这个handle（一个operation）转变成microkernel调用。</p>
<blockquote>
<p>在实际编译开发中，microkernel相当于微内核算子，一般为手写，编译器可以自动发现可以替换的op做替换，以进一步提高性能。</p>
</blockquote>
<h3 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a><font color = green>项目结构</font></h3><p>transform extension的写法，和mlir的一个standalone项目的结构是类似的。项目结构可以参考<a href="https://github.com/llvm/llvm-project/tree/main/mlir/examples/standalone">mlir standalone项目框架</a>。</p>
<p>如下是chapter2的项目结构：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">├── build.sh</span><br><span class="line">├── CMakeLists.txt</span><br><span class="line">├── include</span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   ├── MyExtension.h</span><br><span class="line">│   └── MyExtension.td</span><br><span class="line">├── lib</span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   └── MyExtension.cpp</span><br><span class="line">├── test</span><br><span class="line">│   ├── invalid.mlir</span><br><span class="line">│   ├── sequence.mlir</span><br><span class="line">│   └── test.sh</span><br><span class="line">└── transform-opt</span><br><span class="line">    ├── CMakeLists.txt</span><br><span class="line">    └── transform-opt.cpp</span><br></pre></td></tr></table></figure>

<p>为transform dialect定义一个operation，主要需要思考如下几个点：</p>
<ul>
<li>利用ods系统定义一个operation，自动生成.h和.cpp文件。</li>
<li>该operation需要重载几个interface。<ul>
<li>TransformOpInterface是transform dialect的op必须实现的interface，主要实现<code>apply</code>方法。</li>
<li>MemoryEffectsOpInterface是内存side effect定义interface，需要实现<code>getEffects</code>方法。</li>
</ul>
</li>
<li>注册operation。</li>
</ul>
<h3 id="代码框架讲解"><a href="#代码框架讲解" class="headerlink" title="代码框架讲解"></a><font color = green>代码框架讲解</font></h3><p>后续按照这个顺序来罗列代码，代码注释十分全面了。</p>
<h4 id="定义operation"><a href="#定义operation" class="headerlink" title="定义operation"></a>定义operation</h4><p><code>/include/MyExtension.td</code></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//===-- MyExtension.td - Transform dialect tutorial --------*- tablegen -*-===//</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.</span></span><br><span class="line"><span class="comment">// See https://llvm.org/LICENSE.txt for license information.</span></span><br><span class="line"><span class="comment">// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//===----------------------------------------------------------------------===//</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// This file defines Transform dialect extension operations used in the</span></span><br><span class="line"><span class="comment">// Chapter 2 of the Transform dialect tutorial.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//===----------------------------------------------------------------------===//</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifndef</span> MY_EXTENSION</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MY_EXTENSION</span></span><br><span class="line"></span><br><span class="line">include <span class="string">&quot;mlir/Dialect/Transform/IR/TransformDialect.td&quot;</span></span><br><span class="line">include <span class="string">&quot;mlir/Dialect/Transform/Interfaces/TransformInterfaces.td&quot;</span></span><br><span class="line">include <span class="string">&quot;mlir/IR/OpBase.td&quot;</span></span><br><span class="line">include <span class="string">&quot;mlir/Interfaces/SideEffectInterfaces.td&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Define the new operation. By convention, prefix its name with the name of the dialect </span></span><br><span class="line"><span class="comment">// extension, &quot;my.&quot;. The full operation name will be further prefixed with &quot;transform.&quot;.</span></span><br><span class="line">def ChangeCallTargetOp : Op&lt;Transform_Dialect, <span class="string">&quot;my.change_call_target&quot;</span>,</span><br><span class="line">    <span class="comment">// Indicate that the operation implements the required TransformOpInterface and</span></span><br><span class="line">    <span class="comment">// MemoryEffectsOpInterface.</span></span><br><span class="line">    [DeclareOpInterfaceMethods&lt;TransformOpInterface&gt;,</span><br><span class="line">     DeclareOpInterfaceMethods&lt;MemoryEffectsOpInterface&gt;]&gt; &#123;</span><br><span class="line">  <span class="comment">// Provide a brief and a full description. It is recommended that the latter describes </span></span><br><span class="line">  <span class="comment">// the effects on the operands and how the operation processes various failure modes.</span></span><br><span class="line">  let summary = <span class="string">&quot;Changes the callee of a call operation to the specified one&quot;</span>;</span><br><span class="line">  let description = [&#123;</span><br><span class="line">    For each `func.call` payload operation associated with the handle, changes its </span><br><span class="line">    callee to be the symbol whose name is provided as an attribute to <span class="keyword">this</span> operation.</span><br><span class="line"></span><br><span class="line">    Generates a silenceable failure <span class="keyword">if</span> the operand is associated with payload operations </span><br><span class="line">    that are <span class="keyword">not</span> `func.call`.</span><br><span class="line">    Only reads the operand.</span><br><span class="line">  &#125;];</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The arguments include the handle to the payload operations and the attribute that </span></span><br><span class="line">  <span class="comment">// specifies the new callee. The handle must implement TransformHandleTypeInterface.   </span></span><br><span class="line">  <span class="comment">// We use a string attribute as the symbol may not exist in the transform IR so the </span></span><br><span class="line">  <span class="comment">// verification may fail. </span></span><br><span class="line">  let arguments = (ins</span><br><span class="line">    TransformHandleTypeInterface:$call,</span><br><span class="line">    StrAttr:$new_target);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The results are empty as the transformation does not produce any new payload.</span></span><br><span class="line">  let results = (outs);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Provide nice syntax.</span></span><br><span class="line">  let assemblyFormat = <span class="string">&quot;$call `,` $new_target attr-dict `:` type($call)&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span> <span class="comment">// MY_EXTENSION</span></span></span><br></pre></td></tr></table></figure>

<p>上述td定义和mlir op定义是一样的，需要注意如下几点：</p>
<ul>
<li><p>定义interface：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">[DeclareOpInterfaceMethods&lt;TransformOpInterface&gt;,</span><br><span class="line"> DeclareOpInterfaceMethods&lt;MemoryEffectsOpInterface&gt;]</span><br></pre></td></tr></table></figure>
</li>
<li><p>operation的定义：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// The arguments include the handle to the payload operations and the attribute that </span></span><br><span class="line"><span class="comment">// specifies the new callee. The handle must implement TransformHandleTypeInterface.   </span></span><br><span class="line"><span class="comment">// We use a string attribute as the symbol may not exist in the transform IR so the </span></span><br><span class="line"><span class="comment">// verification may fail. </span></span><br><span class="line">let arguments = (ins</span><br><span class="line">  TransformHandleTypeInterface:$call,</span><br><span class="line">  StrAttr:$new_target);</span><br></pre></td></tr></table></figure>

<p>重点是argument中需要传入一个handle，该handle必须实现TransformHandleTypeInterface，即这个handle可以通过transform op来操纵。</p>
</li>
</ul>
<p><code>MyExtension.h</code></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//===-- MyExtension.h - Transform dialect tutorial --------------*- c++ -*-===//</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.</span></span><br><span class="line"><span class="comment">// See https://llvm.org/LICENSE.txt for license information.</span></span><br><span class="line"><span class="comment">// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//===----------------------------------------------------------------------===//</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// This file defines Transform dialect extension operations used in the</span></span><br><span class="line"><span class="comment">// Chapter 2 of the Transform dialect tutorial.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//===----------------------------------------------------------------------===//</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/Bytecode/BytecodeOpInterface.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/Dialect/Transform/IR/TransformDialect.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/Dialect/Transform/Interfaces/TransformInterfaces.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> GET_OP_CLASSES</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;MyExtension.h.inc&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Registers our Transform dialect extension.</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">registerMyExtension</span><span class="params">(::mlir::DialectRegistry &amp;registry)</span></span>;</span><br></pre></td></tr></table></figure>

<p>这个头文件和mlir基础操作一样，利用宏定义获取.h.inc中的op定义，并定义一个extension的注册函数。</p>
<p>CmakeLists.txt如下：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Tell Tablegen to use MyExtension.td as input.</span></span><br><span class="line"><span class="keyword">set</span>(LLVM_TARGET_DEFINITIONS MyExtension.td)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ask Tablegen to generate op declarations and definitions from ODS.</span></span><br><span class="line">mlir_tablegen(MyExtension.h.inc -gen-op-decls)</span><br><span class="line">mlir_tablegen(MyExtension.cpp.inc -gen-op-defs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add a CMakeTarget we can depend on to ensure the generation happens before the compilation.</span></span><br><span class="line">add_public_tablegen_target(MyExtensionCh2IncGen)</span><br></pre></td></tr></table></figure>

<h4 id="实现operation需要重载的interface操作"><a href="#实现operation需要重载的interface操作" class="headerlink" title="实现operation需要重载的interface操作"></a>实现operation需要重载的interface操作</h4><p><code>/lib/MyExtension.cpp</code>中的代码可以做拆分</p>
<ul>
<li><p>定义transform dialect的extension：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Define a new transform dialect extension. This uses the CRTP idiom to</span></span><br><span class="line"><span class="comment">// identify extensions.</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyExtension</span></span><br><span class="line">    : <span class="keyword">public</span> ::mlir::transform::TransformDialectExtension&lt;MyExtension&gt; &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="comment">// The TypeID of this extension.</span></span><br><span class="line">  <span class="built_in">MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID</span>(MyExtension)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The extension must derive the base constructor.</span></span><br><span class="line">  <span class="keyword">using</span> Base::Base;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// This function initializes the extension, similarly to `initialize` in</span></span><br><span class="line">  <span class="comment">// dialect definitions. List individual operations and dependent dialects</span></span><br><span class="line">  <span class="comment">// here.</span></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">init</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
<li><p>extension的初始化：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">MyExtension::init</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Similarly to dialects, an extension can declare a dependent dialect. This</span></span><br><span class="line">  <span class="comment">// dialect will be loaded along with the extension and, therefore, along with</span></span><br><span class="line">  <span class="comment">// the Transform dialect. Only declare as dependent the dialects that contain</span></span><br><span class="line">  <span class="comment">// the attributes or types used by transform operations. Do NOT declare as</span></span><br><span class="line">  <span class="comment">// dependent the dialects produced during the transformation.</span></span><br><span class="line">  <span class="comment">// declareDependentDialect&lt;MyDialect&gt;();</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// When transformations are applied, they may produce new operations from</span></span><br><span class="line">  <span class="comment">// previously unloaded dialects. Typically, a pass would need to declare</span></span><br><span class="line">  <span class="comment">// itself dependent on the dialects containing such new operations. To avoid</span></span><br><span class="line">  <span class="comment">// confusion with the dialects the extension itself depends on, the Transform</span></span><br><span class="line">  <span class="comment">// dialects differentiates between:</span></span><br><span class="line">  <span class="comment">//   - dependent dialects, which are used by the transform operations, and</span></span><br><span class="line">  <span class="comment">//   - generated dialects, which contain the entities (attributes, operations,</span></span><br><span class="line">  <span class="comment">//     types) that may be produced by applying the transformation even when</span></span><br><span class="line">  <span class="comment">//     not present in the original payload IR.</span></span><br><span class="line">  <span class="comment">// In the following chapter, we will be add operations that generate function</span></span><br><span class="line">  <span class="comment">// calls and structured control flow operations, so let&#x27;s declare the</span></span><br><span class="line">  <span class="comment">// corresponding dialects as generated.</span></span><br><span class="line">  <span class="built_in">declareGeneratedDialect</span>&lt;::mlir::scf::SCFDialect&gt;();</span><br><span class="line">  <span class="built_in">declareGeneratedDialect</span>&lt;::mlir::func::FuncDialect&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Finally, we register the additional transform operations with the dialect.</span></span><br><span class="line">  <span class="comment">// List all operations generated from ODS. This call will perform additional</span></span><br><span class="line">  <span class="comment">// checks that the operations implement the transform and memory effect</span></span><br><span class="line">  <span class="comment">// interfaces required by the dialect interpreter and assert if they do not.</span></span><br><span class="line">  <span class="built_in">registerTransformOps</span>&lt;</span><br><span class="line"><span class="meta">#<span class="keyword">define</span> GET_OP_LIST</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;MyExtension.cpp.inc&quot;</span></span></span><br><span class="line">      &gt;();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>上述的一个重点是<code>declareDependentDialect</code>和<code>declareGeneratedDialect</code>的区别。<code>dependent dialects</code> 是 transform op 需要的，<code>generated dialects</code> 是 transform 执行后生成的。这里scf和func均是可能生成的dialect。</p>
</li>
<li><p>另一个重点是，<code>registerTransformOps</code>会注册ods定义的operation，同时会有检测机制check是否完成transform和sideeffect的interface。</p>
</li>
</ul>
</li>
<li><p>对特定op做interface重写：</p>
<ul>
<li><p>transform interface的apply方法重写：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">::mlir::DiagnosedSilenceableFailure mlir::transform::ChangeCallTargetOp::<span class="built_in">apply</span>(</span><br><span class="line">    <span class="comment">// The rewriter that should be used when modifying IR.</span></span><br><span class="line">    ::mlir::transform::TransformRewriter &amp;rewriter,</span><br><span class="line">    <span class="comment">// The list of payload IR entities that will be associated with the</span></span><br><span class="line">    <span class="comment">// transform IR values defined by this transform operation. In this case, it</span></span><br><span class="line">    <span class="comment">// can remain empty as there are no results.</span></span><br><span class="line">    ::mlir::transform::TransformResults &amp;results,</span><br><span class="line">    <span class="comment">// The transform application state. This object can be used to query the</span></span><br><span class="line">    <span class="comment">// current associations between transform IR values and payload IR entities.</span></span><br><span class="line">    <span class="comment">// It can also carry additional user-defined state.</span></span><br><span class="line">    ::mlir::transform::TransformState &amp;state) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// First, we need to obtain the list of payload operations that are associated</span></span><br><span class="line">  <span class="comment">// with the operand handle.</span></span><br><span class="line">  <span class="keyword">auto</span> payload = state.<span class="built_in">getPayloadOps</span>(<span class="built_in">getCall</span>());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Then, we iterate over the list of operands and call the actual IR-mutating</span></span><br><span class="line">  <span class="comment">// function. We also check the preconditions here.</span></span><br><span class="line">  <span class="keyword">for</span> (Operation *payloadOp : payload) &#123;</span><br><span class="line">    <span class="keyword">auto</span> call = <span class="built_in">dyn_cast</span>&lt;::mlir::func::CallOp&gt;(payloadOp);</span><br><span class="line">    <span class="keyword">if</span> (!call) &#123;</span><br><span class="line">      DiagnosedSilenceableFailure diag =</span><br><span class="line">          <span class="built_in">emitSilenceableError</span>() &lt;&lt; <span class="string">&quot;only applies to func.call payloads&quot;</span>;</span><br><span class="line">      diag.<span class="built_in">attachNote</span>(payloadOp-&gt;<span class="built_in">getLoc</span>()) &lt;&lt; <span class="string">&quot;offending payload&quot;</span>;</span><br><span class="line">      <span class="keyword">return</span> diag;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">updateCallee</span>(call, <span class="built_in">getNewTarget</span>());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If everything went well, return success.</span></span><br><span class="line">  <span class="keyword">return</span> DiagnosedSilenceableFailure::<span class="built_in">success</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>side effect的interface重写：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">void</span> mlir::transform::ChangeCallTargetOp::<span class="built_in">getEffects</span>(</span><br><span class="line">    ::llvm::SmallVectorImpl&lt;::mlir::MemoryEffects::EffectInstance&gt; &amp;effects) &#123;</span><br><span class="line">  <span class="comment">// Indicate that the `call` handle is only read by this operation because the</span></span><br><span class="line">  <span class="comment">// associated operation is not erased but rather modified in-place, so the</span></span><br><span class="line">  <span class="comment">// reference to it remains valid.</span></span><br><span class="line">  <span class="built_in">onlyReadsHandle</span>(<span class="built_in">getCallMutable</span>(), effects);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Indicate that the payload is modified by this operation.</span></span><br><span class="line">  <span class="built_in">modifiesPayload</span>(effects);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>注册整个myextension：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">registerMyExtension</span><span class="params">(::mlir::DialectRegistry &amp;registry)</span> </span>&#123;</span><br><span class="line">  registry.<span class="built_in">addExtensions</span>&lt;MyExtension&gt;();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>CmakeLists.txt如下：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Outside examples, this should be `add_mlir_library`.</span></span><br><span class="line">add_mlir_dialect_library(</span><br><span class="line">  <span class="comment"># Library called MyExtension.</span></span><br><span class="line">  MyExtensionCh2</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Built from the following source files.</span></span><br><span class="line">  MyExtension.cpp</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Make includes visible without top-level path.</span></span><br><span class="line">  ADDITIONAL_HEADER_DIRS</span><br><span class="line">  <span class="variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>/<span class="keyword">include</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Make sure ODS declaration and definitions are generated before compiling this.</span></span><br><span class="line">  DEPENDS</span><br><span class="line">  MyExtensionCh2IncGen</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Link in the transform dialect, an all generated dialects.</span></span><br><span class="line">  LINK_LIBS PRIVATE</span><br><span class="line">  MLIRTransformDialect</span><br><span class="line">  MLIRFuncDialect</span><br><span class="line">  MLIRSCFDialect</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4 id="transform-opt-驱动编写"><a href="#transform-opt-驱动编写" class="headerlink" title="transform-opt 驱动编写"></a>transform-opt 驱动编写</h4><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//===-- transform-opt.cpp - Transform dialect tutorial entry point --------===//</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.</span></span><br><span class="line"><span class="comment">// See https://llvm.org/LICENSE.txt for license information.</span></span><br><span class="line"><span class="comment">// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//===----------------------------------------------------------------------===//</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// This is the top-level file for the Transform dialect tutorial chapter 2.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//===----------------------------------------------------------------------===//</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;MyExtension.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/Dialect/Transform/Transforms/Passes.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/IR/DialectRegistry.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/IR/MLIRContext.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/InitAllDialects.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/InitAllExtensions.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/Tools/mlir-opt/MlirOptMain.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/Transforms/Passes.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdlib&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> test &#123;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">registerTestTransformDialectExtension</span><span class="params">(mlir::DialectRegistry &amp;)</span></span>;</span><br><span class="line">&#125; <span class="comment">// namespace test</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Register all &quot;core&quot; dialects and our transform dialect extension.</span></span><br><span class="line">  mlir::DialectRegistry registry;</span><br><span class="line">  mlir::<span class="built_in">registerAllDialects</span>(registry);</span><br><span class="line">  mlir::<span class="built_in">registerAllExtensions</span>(registry);</span><br><span class="line">  <span class="built_in">registerMyExtension</span>(registry);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Register transform interpreter pass.</span></span><br><span class="line">  mlir::transform::<span class="built_in">registerInterpreterPass</span>();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Register a handful of cleanup passes that we can run to make the output IR</span></span><br><span class="line">  <span class="comment">// look nicer.</span></span><br><span class="line">  mlir::<span class="built_in">registerCanonicalizerPass</span>();</span><br><span class="line">  mlir::<span class="built_in">registerCSEPass</span>();</span><br><span class="line">  mlir::<span class="built_in">registerSymbolDCEPass</span>();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Delegate to the MLIR utility for parsing and pass management.</span></span><br><span class="line">  <span class="keyword">return</span> mlir::<span class="built_in">MlirOptMain</span>(argc, argv, <span class="string">&quot;transform-opt-ch2&quot;</span>, registry)</span><br><span class="line">                 .<span class="built_in">succeeded</span>()</span><br><span class="line">             ? EXIT_SUCCESS</span><br><span class="line">             : EXIT_FAILURE;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>CmakeLists.txt如下：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">get_property</span>(dialect_libs GLOBAL PROPERTY MLIR_DIALECT_LIBS)</span><br><span class="line"><span class="keyword">get_property</span>(conversion_libs GLOBAL PROPERTY MLIR_CONVERSION_LIBS)</span><br><span class="line"><span class="keyword">set</span>(LIBS</span><br><span class="line">        <span class="variable">$&#123;dialect_libs&#125;</span></span><br><span class="line">        <span class="variable">$&#123;conversion_libs&#125;</span></span><br><span class="line">        MLIRIR</span><br><span class="line">        MLIRMlirOptMain</span><br><span class="line">        MLIRSideEffectInterfaces</span><br><span class="line">        MyExtensionCh2</span><br><span class="line">        )</span><br><span class="line">add_llvm_executable(transform-opt transform-opt.cpp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">target_link_libraries</span>(transform-opt PRIVATE <span class="variable">$&#123;LIBS&#125;</span>)</span><br></pre></td></tr></table></figure>

<p>这部分代码和mlir代码没有区别，注册我们的myextension，然后注册各种pass，<strong>重点是注册<code>registerInterpreterPass</code>这个pass</strong>。</p>
<p>顶层CmakeLists.txt如下：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.20</span>.<span class="number">0</span>)</span><br><span class="line"><span class="keyword">project</span>(standalone-dialect LANGUAGES CXX C)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(CMAKE_BUILD_WITH_INSTALL_NAME_DIR <span class="keyword">ON</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_STANDARD <span class="number">17</span> CACHE <span class="keyword">STRING</span> <span class="string">&quot;C++ standard to conform to&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(CMAKE_SOURCE_DIR <span class="keyword">STREQUAL</span> CMAKE_CURRENT_SOURCE_DIR)</span><br><span class="line">  <span class="keyword">find_package</span>(MLIR REQUIRED CONFIG)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">message</span>(STATUS <span class="string">&quot;Using MLIRConfig.cmake in: $&#123;MLIR_DIR&#125;&quot;</span>)</span><br><span class="line">  <span class="keyword">message</span>(STATUS <span class="string">&quot;Using LLVMConfig.cmake in: $&#123;LLVM_DIR&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">set</span>(LLVM_RUNTIME_OUTPUT_INTDIR <span class="variable">$&#123;CMAKE_BINARY_DIR&#125;</span>/bin)</span><br><span class="line">  <span class="keyword">set</span>(LLVM_LIBRARY_OUTPUT_INTDIR <span class="variable">$&#123;CMAKE_BINARY_DIR&#125;</span>/lib)</span><br><span class="line">  <span class="keyword">set</span>(MLIR_BINARY_DIR <span class="variable">$&#123;CMAKE_BINARY_DIR&#125;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">list</span>(APPEND CMAKE_MODULE_PATH <span class="string">&quot;$&#123;MLIR_CMAKE_DIR&#125;&quot;</span>)</span><br><span class="line">  <span class="keyword">list</span>(APPEND CMAKE_MODULE_PATH <span class="string">&quot;$&#123;LLVM_CMAKE_DIR&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">include</span>(TableGen)</span><br><span class="line">  <span class="keyword">include</span>(AddLLVM)</span><br><span class="line">  <span class="keyword">include</span>(AddMLIR)</span><br><span class="line">  <span class="keyword">include</span>(HandleLLVMOptions)</span><br><span class="line"><span class="keyword">else</span>()</span><br><span class="line">  <span class="comment"># Build via external projects mechanism</span></span><br><span class="line">  <span class="keyword">set</span>(MLIR_MAIN_SRC_DIR <span class="variable">$&#123;LLVM_MAIN_SRC_DIR&#125;</span>/../mlir)</span><br><span class="line">  <span class="keyword">set</span>(MLIR_INCLUDE_DIR <span class="variable">$&#123;MLIR_MAIN_SRC_DIR&#125;</span>/<span class="keyword">include</span>)</span><br><span class="line">  <span class="keyword">set</span>(MLIR_GENERATED_INCLUDE_DIR <span class="variable">$&#123;LLVM_BINARY_DIR&#125;</span>/tools/mlir/<span class="keyword">include</span>)</span><br><span class="line">  <span class="keyword">set</span>(MLIR_INCLUDE_DIRS <span class="string">&quot;$&#123;MLIR_INCLUDE_DIR&#125;;$&#123;MLIR_GENERATED_INCLUDE_DIR&#125;&quot;</span>)</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(MLIR_ENABLE_BINDINGS_PYTHON)</span><br><span class="line">  <span class="keyword">include</span>(MLIRDetectPythonEnv)</span><br><span class="line">  mlir_configure_python_dev_packages()</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(STANDALONE_SOURCE_DIR <span class="variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>)</span><br><span class="line"><span class="keyword">set</span>(STANDALONE_BINARY_DIR <span class="variable">$&#123;PROJECT_BINARY_DIR&#125;</span>)</span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;LLVM_INCLUDE_DIRS&#125;</span>)</span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;MLIR_INCLUDE_DIRS&#125;</span>)</span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;STANDALONE_SOURCE_DIR&#125;</span>/<span class="keyword">include</span>)</span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;STANDALONE_BINARY_DIR&#125;</span>/<span class="keyword">include</span>)</span><br><span class="line"><span class="keyword">link_directories</span>(<span class="variable">$&#123;LLVM_BUILD_LIBRARY_DIR&#125;</span>)</span><br><span class="line"><span class="keyword">add_definitions</span>(<span class="variable">$&#123;LLVM_DEFINITIONS&#125;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_subdirectory</span>(<span class="keyword">include</span>)</span><br><span class="line"><span class="keyword">add_subdirectory</span>(lib)</span><br><span class="line"><span class="keyword">add_subdirectory</span>(transform-opt)</span><br></pre></td></tr></table></figure>

<h3 id="Transform-extension的核心实现"><a href="#Transform-extension的核心实现" class="headerlink" title="Transform extension的核心实现"></a><font color = green>Transform extension的核心实现</font></h3><p>transform extension的核心操作，就是实现<code>transform.my.change_call_target</code> op的apply方法和getEffect方法，这两个方法决定了tranform之后生成的代码等。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">void</span> mlir::transform::ChangeCallTargetOp::<span class="built_in">getEffects</span>(</span><br><span class="line">    ::llvm::SmallVectorImpl&lt;::mlir::MemoryEffects::EffectInstance&gt; &amp;effects) &#123;</span><br><span class="line">  <span class="comment">// Indicate that the `call` handle is only read by this operation because the</span></span><br><span class="line">  <span class="comment">// associated operation is not erased but rather modified in-place, so the</span></span><br><span class="line">  <span class="comment">// reference to it remains valid.</span></span><br><span class="line">  <span class="built_in">onlyReadsHandle</span>(<span class="built_in">getCallMutable</span>(), effects);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Indicate that the payload is modified by this operation.</span></span><br><span class="line">  <span class="built_in">modifiesPayload</span>(effects);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Implementation of our transform dialect operation.</span></span><br><span class="line"><span class="comment">// This operation returns a tri-state result that can be one of:</span></span><br><span class="line"><span class="comment">// - success when the transformation succeeded;</span></span><br><span class="line"><span class="comment">// - definite failure when the transformation failed in such a way that</span></span><br><span class="line"><span class="comment">//   following transformations are impossible or undesirable, typically it could</span></span><br><span class="line"><span class="comment">//   have left payload IR in an invalid state; it is expected that a diagnostic</span></span><br><span class="line"><span class="comment">//   is emitted immediately before returning the definite error;</span></span><br><span class="line"><span class="comment">// - silenceable failure when the transformation failed but following</span></span><br><span class="line"><span class="comment">//   transformations are still applicable, typically this means a precondition</span></span><br><span class="line"><span class="comment">//   for the transformation is not satisfied and the payload IR has not been</span></span><br><span class="line"><span class="comment">//   modified. The silenceable failure additionally carries a Diagnostic that</span></span><br><span class="line"><span class="comment">//   can be emitted to the user.</span></span><br><span class="line">::mlir::DiagnosedSilenceableFailure mlir::transform::ChangeCallTargetOp::<span class="built_in">apply</span>(</span><br><span class="line">    <span class="comment">// The rewriter that should be used when modifying IR.</span></span><br><span class="line">    ::mlir::transform::TransformRewriter &amp;rewriter,</span><br><span class="line">    <span class="comment">// The list of payload IR entities that will be associated with the</span></span><br><span class="line">    <span class="comment">// transform IR values defined by this transform operation. In this case, it</span></span><br><span class="line">    <span class="comment">// can remain empty as there are no results.</span></span><br><span class="line">    ::mlir::transform::TransformResults &amp;results,</span><br><span class="line">    <span class="comment">// The transform application state. This object can be used to query the</span></span><br><span class="line">    <span class="comment">// current associations between transform IR values and payload IR entities.</span></span><br><span class="line">    <span class="comment">// It can also carry additional user-defined state.</span></span><br><span class="line">    ::mlir::transform::TransformState &amp;state) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// First, we need to obtain the list of payload operations that are associated</span></span><br><span class="line">  <span class="comment">// with the operand handle.</span></span><br><span class="line">  <span class="keyword">auto</span> payload = state.<span class="built_in">getPayloadOps</span>(<span class="built_in">getCall</span>());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Then, we iterate over the list of operands and call the actual IR-mutating</span></span><br><span class="line">  <span class="comment">// function. We also check the preconditions here.</span></span><br><span class="line">  <span class="keyword">for</span> (Operation *payloadOp : payload) &#123;</span><br><span class="line">    <span class="keyword">auto</span> call = <span class="built_in">dyn_cast</span>&lt;::mlir::func::CallOp&gt;(payloadOp);</span><br><span class="line">    <span class="keyword">if</span> (!call) &#123;</span><br><span class="line">      DiagnosedSilenceableFailure diag =</span><br><span class="line">          <span class="built_in">emitSilenceableError</span>() &lt;&lt; <span class="string">&quot;only applies to func.call payloads&quot;</span>;</span><br><span class="line">      diag.<span class="built_in">attachNote</span>(payloadOp-&gt;<span class="built_in">getLoc</span>()) &lt;&lt; <span class="string">&quot;offending payload&quot;</span>;</span><br><span class="line">      <span class="keyword">return</span> diag;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">updateCallee</span>(call, <span class="built_in">getNewTarget</span>());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If everything went well, return success.</span></span><br><span class="line">  <span class="keyword">return</span> DiagnosedSilenceableFailure::<span class="built_in">success</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">static</span> <span class="type">void</span> <span class="title">updateCallee</span><span class="params">(mlir::func::CallOp call, llvm::StringRef newTarget)</span> </span>&#123;</span><br><span class="line">  call.<span class="built_in">setCallee</span>(newTarget);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述两段代码注释已经非常详尽了，再次不多赘述。</p>
<h2 id="Chapter3：实现更加复杂的transform-operation"><a href="#Chapter3：实现更加复杂的transform-operation" class="headerlink" title="Chapter3：实现更加复杂的transform operation"></a><font color = brown>Chapter3：实现更加复杂的transform operation</font></h2><p>这一部分的tutorial完成两件事：</p>
<ul>
<li>给chapter2实现的transform operation针对的payload handle添加constraint trait，通过使用trait的方式简化遍历匹配<code>func::call</code>这一流程。</li>
<li>添加一个新的op，复习整个transform的流程。实现一个callOpInterface，使得handle不局限于func.call，而是只要实现了callOpInterface的op均可。</li>
</ul>
<h2 id="Chapter4：利用transform-op来做payload的匹配"><a href="#Chapter4：利用transform-op来做payload的匹配" class="headerlink" title="Chapter4：利用transform op来做payload的匹配"></a><font color = brown>Chapter4：利用transform op来做payload的匹配</font></h2><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color = brown>参考资料</font></h2><ol>
<li><a href="https://llvm.org/devmtg/2023-05/slides/Tutorial-May11/02-Zinenko-TransformDialectTutorial.pdf">transform dialect tutorial talk - EuroLLVM</a></li>
</ol>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>mlir</category>
        <category>dialect学习</category>
      </categories>
      <tags>
        <tag>mlir</tag>
        <tag>dialect学习</tag>
      </tags>
  </entry>
  <entry>
    <title>tpu-mlir初探</title>
    <url>/2025/04/08/tpu-mlir%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<p><img src="/images/image-20250411150531193.png" alt="image-20250411150531193"></p>
<span id="more"></span>

<blockquote>
<p>TPU-MLIR是算能开发的一套端到端编译框架，支持将onnx模型编译到多款tpu上运行。该项目基于mlir，写法标准，是极佳的mlir学习项目。从中可以学到mlir的各种语法，多层级ir的设计，以及如何完成runtime的接入。</p>
</blockquote>
<h2 id="项目构建"><a href="#项目构建" class="headerlink" title="项目构建"></a><font color = brown>项目构建</font></h2><p>参考<a href="https://github.com/sophgo/tpu-mlir">tpu-mlir github</a>，整个项目的构建比较简单：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker pull sophgo/tpuc_dev:latest     # 获取tpu-mlir镜像</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">myname1234 is just an example, you can <span class="built_in">set</span> your own name</span></span><br><span class="line">docker run --privileged --name myname1234 -v $PWD:/workspace -it sophgo/tpuc_dev:latest    # 构建容器</span><br><span class="line">cd tpu-mlir</span><br><span class="line">source ./envsetup.sh</span><br><span class="line">./build.sh</span><br></pre></td></tr></table></figure>



<h2 id="项目架构以及技术点解读"><a href="#项目架构以及技术点解读" class="headerlink" title="项目架构以及技术点解读"></a><font color = brown>项目架构以及技术点解读</font></h2><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a><font color  = green>整体架构</font></h3><p>这一部分主要参考<a href="http://arxiv.org/abs/2210.15016">TPU-MLIR论文</a>。</p>
<p><img src="/images/image-20250411151435735.png" alt="image-20250411151435735"></p>
<p>上图中的框架，对应代码是<code>tpu-mlir/regression/run_model.py</code>。该顶层模块中调用<code>model_transfer.py</code>和<code>model_deploy.py</code>， 分别完成模型 –&gt; top ir –&gt; tpu ir 和tpu ir到<a href="https://doc.sophgo.com/docs/3.0.0/docs_latest_release/nntc/html/usage/bmodel.html">bmodel部署</a>。如果想要得到int8的模型，则需要调用<code>run_calibration.py</code>。具体流程如下图所示：</p>
<p><img src="/images/image-20250412125832691.png" alt="image-20250412125832691"></p>
<p>参考<a href="https://tpumlir.org/docs/developer_manual/04_over_design.html">TPU-MLIR 官方文档</a>理解更细节的mlir转换图。</p>
<h3 id="架构亮点"><a href="#架构亮点" class="headerlink" title="架构亮点"></a><font color = green>架构亮点</font></h3><blockquote>
<p>TPU-MLIR项目的价值一是提供mlir学习资料，整个项目的架构和实现比较简洁，二是提供tpu设计思路，此部分值得我们重点关注</p>
</blockquote>
<ul>
<li><strong>ONNX converter</strong>：用python编写的前端工具，可以用来参考如何将高级语言快速转换为mlir ir。</li>
<li><strong>多层级ir</strong>：TPU-MLIR设计了Top dialect和Tpu dialect，是比较经典的ir设计（硬件无关ir，统一不同模型框架语义，以及硬件相关ir，负责提供硬件抽象），诸如GLOW编译器也是这种ir设计。可以用来参考如何使用mlir系统设计ir抽象。</li>
<li><strong>优化pass</strong>：这一部分是TPU-MLIR的一个重点，主要有如下方面。<ul>
<li>动态shape，TPU-MLIR是支持动态shape的inference的，可以做参考</li>
<li>高层算子融合pass</li>
<li>TPU相关pass，这一部分是后续重点关注pass。包括：<ul>
<li>TPU inference，主要借助于onnx 的runtime，后续会专门介绍</li>
<li>weight-reorder，op-reorder等tpu专属优化</li>
<li>layer group优化，类似局部缓存融合优化（<a href="https://dl.acm.org/doi/10.1145/3503222.3507723">Astitch论文</a>），后续专门介绍</li>
</ul>
</li>
<li>量化，TPU-MLIR对于量化操作做了诸多考量，包含带<strong>calibration的量化</strong>，对称量化和非对称量化等。</li>
</ul>
</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color = brown>参考资料</font></h2><ol>
<li><a href="https://github.com/micropuma/tpu-mlir">TPU-MLIR github</a></li>
<li><a href="http://arxiv.org/abs/2210.15016">TPU-MLIR论文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/613328745">TPU-MLIR博客</a></li>
</ol>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>机器学习编译</category>
        <category>TPU技术</category>
        <category>量化技术</category>
      </categories>
      <tags>
        <tag>机器学习编译器</tag>
        <tag>mlir</tag>
        <tag>异构计算系统</tag>
      </tags>
  </entry>
  <entry>
    <title>计算密集算子融合</title>
    <url>/2025/03/31/%E8%AE%A1%E7%AE%97%E5%AF%86%E9%9B%86%E7%AE%97%E5%AD%90%E8%9E%8D%E5%90%88/</url>
    <content><![CDATA[<p><img src="/images/image-20250331211738664.png" alt="image-20250331211738664"></p>
<span id="more"></span>

<blockquote>
<p>机器学习中算子可以分为<strong>计算密集算子</strong>和<strong>访存密集算子</strong>，之前的博客已经讲解了<strong>访存密集算子融合技术</strong>，本文重点解读计算密集算子融合，即针对gemm等算子的计算优化。本文主要结合BladeDISC和Rammer等文章加以解读</p>
</blockquote>
<h2 id="BladeDISC的计算密集算子融合"><a href="#BladeDISC的计算密集算子融合" class="headerlink" title="BladeDISC的计算密集算子融合"></a><font color = brown>BladeDISC的计算密集算子融合</font></h2><h3 id="计算密集算子融合pipeline-overview"><a href="#计算密集算子融合pipeline-overview" class="headerlink" title="计算密集算子融合pipeline overview"></a><font color = green>计算密集算子融合pipeline overview</font></h3><p>在<code>BladeDISC</code>项目的pipeline中，涉及计算密集算子融合有三处：</p>
<h4 id="针对简单的mhlo-dot算子做融合"><a href="#针对简单的mhlo-dot算子做融合" class="headerlink" title="针对简单的mhlo_dot算子做融合"></a>针对简单的mhlo_dot算子做融合</h4><p><img src="/images/image-20250403152318767.png" alt="image-20250403152318767"></p>
<p>具体流程图如下：</p>
<pre class="mermaid">flowchart TD
    A[开始: DiscDotMergePass] --> B[执行共享操作数合并]
    B --> C[执行批量合并]
    C --> D{合并成功?}
    D -->|是| E[结束]
    D -->|否| F[标记失败]

    subgraph 共享操作数合并流程
        B1[遍历所有基本块] --> B2[初始化ShareOperandMap]
        B2 --> B3[遍历block中的DotGeneralOp]
        B3 --> B4[提取共享操作数和维度信息]
        B4 --> B5[构建聚类映射]
        B5 --> B6[检测循环依赖]
        B6 --> B7[尝试合并聚类]
        B7 --> B8[应用合并操作]
        B8 --> B9[清理原始操作]
    end

    subgraph 批量合并流程
        C1[形状分析] --> C2[构建MergingShapeMap]
        C2 --> C3[检测可合并聚类]
        C3 --> C4[维度扩展操作]
        C4 --> C5[创建批量concat]
        C5 --> C6[生成批量dot]
        C6 --> C7[切片替换原始操作]
        C7 --> C8[清理原始操作]
    end

    B -->|核心操作| B1
    C -->|核心操作| C1
    B9 -->|清理后| C
    C8 --> D</pre>

<h4 id="混合精度优化"><a href="#混合精度优化" class="headerlink" title="混合精度优化"></a>混合精度优化</h4><p><img src="/images/image-20250402205232495.png" alt="image-20250402205232495"></p>
<h4 id="针对GEMM做layout优化"><a href="#针对GEMM做layout优化" class="headerlink" title="针对GEMM做layout优化"></a>针对GEMM做layout优化</h4><p><img src="/images/image-20250402205308725.png" alt="image-20250402205308725"></p>
<p>具体流程图如下：</p>
<pre class="mermaid">graph TD
    A[原始卷积操作 mhlo::ConvolutionOp] --> B[转换为 DynamicConvOp]
    B --> C[提取卷积参数]
    C --> D[推断预期布局]
    D --> E{当前布局符合预期?}
    E -->|是| F[保持原布局]
    E -->|否| G[插入转置操作调整布局]
    G --> H[输入布局调整]
    G --> I[滤波器布局调整]
    G --> J[输出布局调整]
    H --> K[更新输入布局]
    I --> L[更新滤波器布局]
    J --> M[更新输出布局]
    K --> N[更新卷积属性]
    L --> N
    M --> N
    N --> O[优化后的卷积操作]
    
    P[量化卷积操作 QuantizedDynamicConvOp] --> C
    Q[GPU/CUDA环境] --> D
    R[CPU环境] --> D
    
    style A fill:#f9d,stroke:#333
    style P fill:#f9d,stroke:#333
    style B fill:#bbf,stroke:#333
    style C fill:#cfc,stroke:#333
    style D fill:#cfc,stroke:#333
    style E fill:#ffd,stroke:#333
    style G fill:#fcc,stroke:#333
    style N fill:#cfc,stroke:#333
    style O fill:#9f9,stroke:#333</pre>

]]></content>
      <categories>
        <category>编译技术</category>
        <category>机器学习编译</category>
        <category>计算优化</category>
      </categories>
      <tags>
        <tag>机器学习编译器</tag>
        <tag>mlir</tag>
        <tag>算子融合技术</tag>
      </tags>
  </entry>
  <entry>
    <title>tpu-mlir源码解读</title>
    <url>/2025/04/11/tpu-mlir%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p><img src="/images/image-20250411150531193.png" alt="image-20250411150531193"></p>
<span id="more"></span>

<blockquote>
<p>本博客主要学习TPU-MLIR的代码结构，mlir编程技术等。</p>
</blockquote>
<h2 id="TPU-MLIR测试流程"><a href="#TPU-MLIR测试流程" class="headerlink" title="TPU-MLIR测试流程"></a><font color = brown>TPU-MLIR测试流程</font></h2><p>开始对TPU-MLIR项目的详细解读之前，先编写好测试case。我们选择对于Yolov5s做测试，具体的模型和训练测试数据可以参考<code>regression/</code>文件夹。需要如下文件：</p>
<ul>
<li>yolov5s.onnx</li>
<li>COCO2017数据集</li>
<li>image图片集</li>
</ul>
<p>具体脚本如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir model_yolov5s &amp;&amp; cd model_yolov5s</span><br><span class="line">cp $&#123;REGRESSION_PATH&#125;/model/yolov5s.onnx .</span><br><span class="line">cp -rf $&#123;REGRESSION_PATH&#125;/dataset/COCO2017 .</span><br><span class="line">cp -rf $&#123;REGRESSION_PATH&#125;/image .</span><br><span class="line">mkdir workspace &amp;&amp; cd workspace</span><br></pre></td></tr></table></figure>

<p>然后调用TPU-MLIR的python driver程序，完成整个流程。由于目前先不关注量化部分，所以先只用<code>model_transform.py</code>和<code>model_deploy.py</code>两个python文件。具体脚本如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">model_transform.py \</span><br><span class="line">    --model_name yolov5s \</span><br><span class="line">    --model_def ./yolov5s.onnx \</span><br><span class="line">    --input_shapes [[1,3,640,640]] \</span><br><span class="line">    --mean 0.0,0.0,0.0 \</span><br><span class="line">    --scale 0.0039216,0.0039216,0.0039216 \</span><br><span class="line">    --keep_aspect_ratio \</span><br><span class="line">    --pixel_format rgb \</span><br><span class="line">    --output_names 350,498,646 \</span><br><span class="line">    --test_input ./image/dog.jpg \</span><br><span class="line">    --test_result yolov5s_top_outputs.npz \</span><br><span class="line">    --mlir yolov5s.mlir</span><br><span class="line"></span><br><span class="line">model_deploy.py \</span><br><span class="line">  --mlir yolov5s.mlir \</span><br><span class="line">  --quantize F16 \</span><br><span class="line">  --processor bm1684x \</span><br><span class="line">  --test_input yolov5s_in_f32.npz \</span><br><span class="line">  --test_reference yolov5s_top_outputs.npz \</span><br><span class="line">  --model yolov5s_1684x_f16.bmodel</span><br></pre></td></tr></table></figure>

<p>最终生成的npu可执行是bmodle模型，是Sophon AI平台专用的高效推理模型文件，包含特定npu的硬件指令集。上述流程得到的中间表示，权重文件，最终的bmodel文件如下：</p>
<img src="/images/image-20250417183143802.png" alt="image-20250417183143802" style="zoom:50%;" />

<blockquote>
<ul>
<li><p>onnx到mlir系统：纯python，无第三方依赖的一套前端parser:rocket:</p>
</li>
<li><p>图优化：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tpuc-opt yolov5s_origin.mlir --shape-infer --canonicalize --extra-optimize -o yolov5s.mlir</span><br></pre></td></tr></table></figure>

<ul>
<li>shape infer pass</li>
<li>canonicalize pass：对于top dialect的每个op都有对应的标准化，详细代码见：<code>lib/Dialect/Top/Canonicalize</code>。</li>
<li>extra  optimize：目前还不清楚这个pass做了什么</li>
</ul>
</li>
<li><p>对于输入数据，做处理：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">model_runner.py --input yolov5s_in_f32.npz --model ./yolov5s.onnx --output yolov5s_ref_outputs.npz</span><br></pre></td></tr></table></figure>
</li>
<li><p>deploy阶段（后续会详细介绍）</p>
</li>
</ul>
</blockquote>
<h2 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a><font color = brown>代码结构</font></h2><p>TPU-MLIR项目的一大特点，就是代码结构非常规整。</p>
<ul>
<li><p>tools中是各种驱动工具</p>
</li>
<li><p>python中是python binding的驱动工具</p>
</li>
<li><p>include文件结构如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">├── CMakeLists.txt</span><br><span class="line">├── tpu_mlir</span><br><span class="line">│   ├── Backend</span><br><span class="line">│   ├── Builder</span><br><span class="line">│   │   ├── BM168x</span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   ├── Conversion</span><br><span class="line">│   │   ├── TopToLinalg</span><br><span class="line">│   │   ├── TopToTosa</span><br><span class="line">│   │   └── TopToTpu</span><br><span class="line">│   ├── Dialect</span><br><span class="line">│   │   ├── CMakeLists.txt</span><br><span class="line">│   │   ├── Top</span><br><span class="line">│   │   │   ├── CMakeLists.txt</span><br><span class="line">│   │   │   ├── IR</span><br><span class="line">│   │   │   │   ├── CMakeLists.txt</span><br><span class="line">│   │   │   │   ├── TopOps.h</span><br><span class="line">│   │   │   │   └── TopOps.td</span><br><span class="line">│   │   │   └── Transforms</span><br><span class="line">│   │   │       ├── CMakeLists.txt</span><br><span class="line">│   │   │       ├── Passes.h</span><br><span class="line">│   │   │       └── Passes.td</span><br><span class="line">│   │   └── Tpu</span><br><span class="line">│   │       ├── CMakeLists.txt</span><br><span class="line">│   │       ├── IR</span><br><span class="line">│   │       │   ├── CMakeLists.txt</span><br><span class="line">│   │       │   ├── TpuOps.h</span><br><span class="line">│   │       │   └── TpuOps.td</span><br><span class="line">│   │       └── Transforms</span><br><span class="line">│   │           ├── CoreParallel</span><br><span class="line">│   │           │   └── CoreParallel.hpp</span><br><span class="line">│   │           ├── DevParallel</span><br><span class="line">│   │           │   ├── Distribute.h</span><br><span class="line">│   │           │   └── DistributeUtils.h</span><br><span class="line">│   │           ├── LayerGroup</span><br><span class="line">│   │           ├── Passes.h</span><br><span class="line">│   │           ├── Passes.td</span><br><span class="line">│   │           └── TruncOp</span><br><span class="line">│   │               └── TruncOp.h</span><br><span class="line">│   ├── InitAll.h</span><br><span class="line">│   ├── Interfaces</span><br><span class="line">│   ├── Support</span><br><span class="line">│   └── Traits</span><br><span class="line">│       ├── Traits.h</span><br><span class="line">│       └── Traits.td</span><br><span class="line">└── tpu_mlir-c</span><br><span class="line">    ├── Dialects.h</span><br><span class="line">    └── RegisterEverything.h</span><br></pre></td></tr></table></figure>

<p>这个文件结构十分值得解读。</p>
<ul>
<li>Conversion是dailect之间的转换函数</li>
<li>Backend和builder是辅助tpu 方言代码生成npu bmodel</li>
<li>Dialect中包含top顶层dialect和tpu dialect<ul>
<li>ir是方言的定义</li>
<li>transforms是ir内部的优化pass，tpu dialect有比较多的优化pass</li>
</ul>
</li>
<li>Interface中包含operation使用的接口定义，同理traits的作用</li>
<li>Support的作用是代码生成过程中的api支持，后续会详细解读</li>
</ul>
</li>
<li><p>lib文件结构类似，特殊的在dialect文件夹下多了Canonicalize文件夹，<strong>该文件作用是针对每个operation完成一些图层面优化</strong>。</p>
</li>
</ul>
<blockquote>
<p>:key:一个十分好的总结TPU-MLIR工程目录结构和编译流程图，红色部分为前端，绿色部分为TOP层以及与之有关功能，蓝色部分为TPU层以及与之有关的功能</p>
</blockquote>
<p><img src="/images/image-20250418175103004.png" alt="image-20250418175103004"></p>
<h2 id="计算图IR设计"><a href="#计算图IR设计" class="headerlink" title="计算图IR设计"></a><font color = brown>计算图IR设计</font></h2><h2 id="Top-dialect设计"><a href="#Top-dialect设计" class="headerlink" title="Top dialect设计"></a><font color = green>Top dialect设计</font></h2><p>这一部分是很好的机器学习静态图表示的资料，因为<code>Top dialect</code>完整的涵盖了机器学习的大部分算子操作，做了相当完善的抽象。这里的解读以<code>Top_conv</code>算子来解读。</p>
<h4 id="Interface设计"><a href="#Interface设计" class="headerlink" title="Interface设计"></a>Interface设计</h4><h4 id="Canoicalization设计"><a href="#Canoicalization设计" class="headerlink" title="Canoicalization设计"></a>Canoicalization设计</h4><h2 id="Pass优化设计"><a href="#Pass优化设计" class="headerlink" title="Pass优化设计"></a><font color = brown>Pass优化设计</font></h2><h2 id="后端设计"><a href="#后端设计" class="headerlink" title="后端设计"></a><font color  = brown>后端设计</font></h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">|---include/tpu_mlir</span><br><span class="line">	|---Dialect</span><br><span class="line">		|---TPU/IR/TpuOps.td：各算子定义，codegen函数定义在traits中</span><br><span class="line">	|---Interfaces</span><br><span class="line">		|---*GenInterface.td：各种codegen模式的声明与实现</span><br><span class="line">|---lib</span><br><span class="line">	|---Backend</span><br><span class="line">		|---Arch.cpp：架构抽象基类，包含后端函数（供codegen调用）与硬件参数</span><br><span class="line">		|---BM168x：168x系列的架构类实现</span><br><span class="line">		|---CV18xx：cv18xx系列的架构类实现</span><br><span class="line">	|---Builder：bmodel定义</span><br><span class="line">	|---Dialect</span><br><span class="line">		|---Tpu</span><br><span class="line">			|---Interfaces：各op codegen函数的重载实现</span><br><span class="line">			|---Transforms</span><br><span class="line">				|---BM168x/Codegen.cpp：bm168系列codegen pass实现</span><br><span class="line">	|---Interfaces：</span><br><span class="line">		|---*GenInterface.cpp：各种codegen模式的补充实现（大部分逻辑实现在了td文件中）</span><br></pre></td></tr></table></figure>

<p>上图是TPU-MLIR的后端代码结构，主要有如下几个重点：</p>
<ul>
<li>TPU dialect用来抽象tpu硬件指令集（这里和AIE dialect等抽象层级还不一样，这里的抽象是对于所有TPU的一个粗粒度抽象，后续还要通过backend映射到对应tpu。<strong>因此这个TPU dialect的设计十分有参考意义</strong>）。</li>
<li>Backend针对每个TPU架构都有对应的指令集api。</li>
<li>Interface里面主要是各个TPU operation的codegen逻辑。<ul>
<li>codegen分为localgen和global gen，对应的是argument是在global mem中还是显示移动到local mem中。</li>
<li><strong>所有算子均要实现global mem</strong>，部分算子考虑layer grouping策略，所以要实现local  mem。</li>
</ul>
</li>
</ul>
<blockquote>
<p>TPU-MLIR后端的另一个重点是，编译器后端如何和runtime系统协同做算子映射和调度的。</p>
</blockquote>
<h3 id="TPU-MLIR-backend框架"><a href="#TPU-MLIR-backend框架" class="headerlink" title="TPU-MLIR backend框架"></a><font color = green>TPU-MLIR backend框架</font></h3><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color = brown>参考资料</font></h2><ol>
<li><a href="https://zhuanlan.zhihu.com/p/615180103">TPU-MLIR博客</a></li>
<li><a href="https://mlir.llvm.org/docs/Interfaces/">MLIR interface官方doc</a></li>
<li><a href="https://tpumlir.org/en/">TPU-MLIR 官方文档</a></li>
<li><a href="https://tpumlir.org/en/2023/04/04/how-to-add-a-new-operator-for-tpu-mlir.html">Add an op for TPU-MLIR – blog</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/718042043">TPU-MLIR blog 1</a></li>
</ol>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>机器学习编译</category>
        <category>TPU技术</category>
        <category>量化技术</category>
      </categories>
      <tags>
        <tag>机器学习编译器</tag>
        <tag>mlir</tag>
        <tag>异构计算系统</tag>
      </tags>
  </entry>
  <entry>
    <title>访存密集算子融合</title>
    <url>/2025/04/01/%E8%AE%BF%E5%AD%98%E5%AF%86%E9%9B%86%E7%AE%97%E5%AD%90%E8%9E%8D%E5%90%88/</url>
    <content><![CDATA[<p><img src="/images/image-20250331214150737.png" alt="image-20250331214150737"></p>
<span id="more"></span>

<blockquote>
<p>这是算子优化系列的第一篇，主要聚焦于存储密集型算子的融合优化。探讨这一主题的动机源于 BladeDISC 编译器，这个由阿里云开发的编译器专注于解决动态 shape 问题，并在大模型业务中实现了显著的优化。BladeDISC 的一大亮点是复用了 astitch 论文中提出的多元访存算子融合方法，因此，本文将对这一问题展开深入讨论，从两方面来分析：XLA传统融合和BladeDISC使用的stich融合。</p>
</blockquote>
<h2 id="XLA代码解析"><a href="#XLA代码解析" class="headerlink" title="XLA代码解析"></a><font color = brown>XLA代码解析</font></h2><p>在阿里的bladedisc的ppt中，可以看到如下总结：</p>
<p><img src="/images/image-20250325195503234.png" alt="image-20250325195503234"></p>
<p>先考虑从<code>mlir-hlo</code>项目入手，理解xla alike的fusion是如何做的。具体代码在<a href="https://github.com/tensorflow/mlir-hlo/blob/1857b1eac21ef5b30b088ccc79ef2fa0e3161621/lib/Dialect/mhlo/transforms/mhlo_fusion.cc#L549">mhlo_fusion.cc</a>中。上述图片其实已经总结了XLA可以做的两种kernel fusion方式：<code>KLoop</code>和<code>KInput</code>。可以看出，<code>XLA</code>还是重点关注的算子pattern是比较简单的。</p>
<h3 id="Source-Code源码分析"><a href="#Source-Code源码分析" class="headerlink" title="Source Code源码分析"></a><font color = green>Source Code源码分析</font></h3><blockquote>
<p>核心原理是通过<strong>形状约束分析</strong>和图论中的<strong>边收缩算法</strong>，动态识别可融合的操作组，并生成高效的融合计划。</p>
<p><font color = red>形状约束是阿里团队提出的动态shape约束。</font></p>
</blockquote>
<p>该代码是典型的两阶段code，第一阶段分析出fusion plan，第二阶段应用fusion plan做相应的fusion：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">runOnOperation</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">    FuncOp func = <span class="built_in">getOperation</span>();</span><br><span class="line">    <span class="keyword">if</span> (!<span class="built_in">IsTargetFunc</span>(func)) &#123;</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// process each block and do fusion within a block.</span></span><br><span class="line">    <span class="keyword">for</span> (Block&amp; block : func) &#123;</span><br><span class="line">      <span class="comment">// 收集一个block内部的所有operation list</span></span><br><span class="line">      SmallVector&lt;Operation*, <span class="number">4</span>&gt; op_list;</span><br><span class="line">      <span class="keyword">for</span> (Operation&amp; op : block) &#123;</span><br><span class="line">        op_list.<span class="built_in">push_back</span>(&amp;op);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="function">FusionPlanner <span class="title">planner</span><span class="params">(op_list)</span></span>;</span><br><span class="line">      llvm::Optional&lt;FusionPlan&gt; plan = planner.<span class="built_in">Run</span>();</span><br><span class="line">      <span class="keyword">if</span> (!plan) &#123;</span><br><span class="line">        <span class="built_in">emitError</span>(func.<span class="built_in">getLoc</span>(), <span class="string">&quot;can&#x27;t find a fusion plan&quot;</span>);</span><br><span class="line">        <span class="built_in">signalPassFailure</span>();</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (!<span class="built_in">ApplyFusionPlan</span>(*plan)) &#123;</span><br><span class="line">        <span class="built_in">emitError</span>(func.<span class="built_in">getLoc</span>(), <span class="string">&quot;apply fusion plan failed&quot;</span>);</span><br><span class="line">        <span class="built_in">signalPassFailure</span>();</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>上述代码段时整个pass的入口，也是整体流程控制。</p>
<ul>
<li><code>FusionPlanner planner(op_list)</code>针对一个block的oplist，初始化一个fusion plan。</li>
<li><code>planner.Run()</code>根据planner构建的图，做子图划分，生成潜在的fusion pattern集和。</li>
<li><code>ApplyFusionPlan</code>为第二阶段，将fusion pattern list用于代码改写，完成最终的fusion操作。</li>
</ul>
<p>接下来分别展开这三个函数。</p>
<h4 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h4><p>下面先来介绍一些辅助函数：</p>
<ol>
<li><p>首先，先明确<code>XLA Fusion</code>的几个<strong>基本数据结构概念</strong>：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> FusionPattern = std::vector&lt;Operation*&gt;;</span><br><span class="line"><span class="keyword">using</span> FusionPlan = std::vector&lt;FusionPattern&gt;;</span><br></pre></td></tr></table></figure>

<p>FusionPattern是可融合的operation集和。FusionPlan是FusionPattern集和。</p>
</li>
<li><p><code>XLA</code>针对memory-intensive的算子，主要考虑如下两个算子：<code>ReduceOp</code>和<code>element-wiseOp</code>。分别有如下可能的融合方案：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// reduceOp主要判断，其operand的define point的op是否shape相同</span></span><br><span class="line"><span class="comment">// 如果相同，reduceOp考虑的模式是operand融合</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">IsFusibleWithOperand</span><span class="params">(Operation* op)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 相比IsFusibleWithConsumer，支持reduce op</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">IsMhlo</span>(op) &amp;&amp;</span><br><span class="line">         (op-&gt;<span class="built_in">hasTrait</span>&lt;::mlir::OpTrait::Elementwise&gt;() || <span class="built_in">isa</span>&lt;ReduceOp&gt;(op));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// element-wise操作或是常量操作，可以考虑是否可以和consumer融合</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">IsFusibleWithConsumer</span><span class="params">(Operation* op)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 必须是MHLO操作，并且是elementwise操作，或是常量操作</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">IsMhlo</span>(op) &amp;&amp; (op-&gt;<span class="built_in">hasTrait</span>&lt;::mlir::OpTrait::Elementwise&gt;() ||</span><br><span class="line">                        <span class="built_in">matchPattern</span>(op, <span class="built_in">m_Constant</span>()));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 全局的isFusible判断</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">IsFusible</span><span class="params">(Operation* op)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 只有常量操作或是能和consumer融合的操作才是可融合的，或是能和operand融合的操作</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">matchPattern</span>(op, <span class="built_in">m_Constant</span>()) || <span class="built_in">IsFusibleWithConsumer</span>(op) ||</span><br><span class="line">         <span class="built_in">IsFusibleWithOperand</span>(op);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>针对<code>FusionPattern</code>和其他block的交互：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">SmallVector&lt;Value, 4&gt; <span class="title">GetInputsOfFusionPattern</span><span class="params">(<span class="type">const</span> FusionPattern&amp; pattern)</span> </span>&#123;</span><br><span class="line">  SmallVector&lt;Value, <span class="number">4</span>&gt; inputs;</span><br><span class="line">  DenseSet&lt;Value&gt; input_set;</span><br><span class="line">  DenseSet&lt;Operation*&gt; op_set;</span><br><span class="line">  <span class="comment">// 收集一个fusion pattern里的所有operation</span></span><br><span class="line">  <span class="keyword">for</span> (Operation* op : pattern) &#123;</span><br><span class="line">    <span class="type">bool</span> inserted = op_set.<span class="built_in">insert</span>(op).second;</span><br><span class="line">    (<span class="type">void</span>)inserted;</span><br><span class="line">    <span class="built_in">assert</span>(inserted &amp;&amp; <span class="string">&quot;FusionPattern contains duplicate operations&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (Operation* op : pattern) &#123;</span><br><span class="line">    <span class="keyword">for</span> (Value operand : op-&gt;<span class="built_in">getOperands</span>()) &#123;</span><br><span class="line">      Operation* operand_op = operand.<span class="built_in">getDefiningOp</span>();</span><br><span class="line">      <span class="comment">// 如果defining op在pattern里，则跳过</span></span><br><span class="line">      <span class="comment">// 否则加入到inputs中，表示是该潜在的fusion pattern的输入</span></span><br><span class="line">      <span class="keyword">if</span> (op_set.<span class="built_in">find</span>(operand_op) != op_set.<span class="built_in">end</span>()) &#123;</span><br><span class="line">        <span class="comment">// skip if defining op is in the pattern</span></span><br><span class="line">        <span class="keyword">continue</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (input_set.<span class="built_in">insert</span>(operand).second) &#123;</span><br><span class="line">        inputs.<span class="built_in">push_back</span>(operand);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> inputs;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 收集融合操作中所有被外部使用的输出，作为融合后的输出</span></span><br><span class="line"><span class="function">SmallVector&lt;Value, 4&gt; <span class="title">GetOutputsOfFusionPattern</span><span class="params">(<span class="type">const</span> FusionPattern&amp; pattern)</span> </span>&#123;</span><br><span class="line">  SmallVector&lt;Value, <span class="number">4</span>&gt; outputs;</span><br><span class="line">  DenseSet&lt;Operation*&gt; op_set;</span><br><span class="line">  <span class="keyword">for</span> (Operation* op : pattern) &#123;</span><br><span class="line">    <span class="comment">// 检查是否有重复的operation</span></span><br><span class="line">    <span class="type">bool</span> inserted = op_set.<span class="built_in">insert</span>(op).second;</span><br><span class="line">    (<span class="type">void</span>)inserted;</span><br><span class="line">    <span class="built_in">assert</span>(inserted &amp;&amp; <span class="string">&quot;FusionPattern contains duplicate operations&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 尝试做融合的operation</span></span><br><span class="line">  <span class="keyword">for</span> (Operation* op : pattern) &#123;</span><br><span class="line">    <span class="keyword">for</span> (Value result : op-&gt;<span class="built_in">getResults</span>()) &#123;</span><br><span class="line">      <span class="comment">// 判断该operation是否result有被外部使用</span></span><br><span class="line">      <span class="type">bool</span> has_external_user = llvm::<span class="built_in">any_of</span>(</span><br><span class="line">          result.<span class="built_in">getUses</span>(),</span><br><span class="line">          [&amp;](OpOperand&amp; use) &#123; <span class="keyword">return</span> !op_set.<span class="built_in">count</span>(use.<span class="built_in">getOwner</span>()); &#125;);</span><br><span class="line">      <span class="comment">// 显示收集被外部使用的output</span></span><br><span class="line">      <span class="keyword">if</span> (has_external_user) &#123;</span><br><span class="line">        outputs.<span class="built_in">push_back</span>(result);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> outputs;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述两个函数支持获取FusionPattern的外部输入和输出。</p>
</li>
<li><p>合并两个<code>FusionPattern</code>，这个函数比较重要，其实就是发现可以合并融合的operation list，扩大单个fusion region：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">FusionPattern <span class="title">MergeFusionPattern</span><span class="params">(<span class="type">const</span> FusionPattern&amp; lhs,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 <span class="type">const</span> FusionPattern&amp; rhs)</span> </span>&#123;</span><br><span class="line">  <span class="function">FusionPattern <span class="title">pattern</span><span class="params">(lhs)</span></span>;</span><br><span class="line">  pattern.<span class="built_in">insert</span>(pattern.<span class="built_in">end</span>(), rhs.<span class="built_in">begin</span>(), rhs.<span class="built_in">end</span>());</span><br><span class="line">  <span class="keyword">return</span> pattern;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>并查集用于做shape推导，<font color = red>注意，XLA的fusion中，shape 推导是比较简单的</font>:</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ShapeConstraintAnalysis</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">ShapeConstraintAnalysis</span><span class="params">(<span class="type">const</span> SmallVectorImpl&lt;Operation*&gt;&amp; op_list)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">PropagateEquality</span>(op_list);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Returns true is `lhs` and `rhs` are supposed to have same shape.</span></span><br><span class="line">  <span class="function"><span class="type">bool</span> <span class="title">HasSameShape</span><span class="params">(Value lhs, Value rhs)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 单纯判断两者在unionfind中的位置是否相同</span></span><br><span class="line">    <span class="keyword">return</span> impl_.<span class="built_in">isEquivalent</span>(<span class="built_in">ValueWrapper</span>(lhs), <span class="built_in">ValueWrapper</span>(rhs));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="comment">// shape equality propagation based on the shape constrains of</span></span><br><span class="line">  <span class="comment">// elementwise ops.</span></span><br><span class="line">  <span class="comment">// 针对elementwise操作，做shape相等传播</span></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">PropagateEquality</span><span class="params">(<span class="type">const</span> SmallVectorImpl&lt;Operation*&gt;&amp; op_list)</span> </span>&#123;</span><br><span class="line">    <span class="type">bool</span> converged = <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">      converged = <span class="literal">true</span>;</span><br><span class="line">      <span class="comment">// 显示对两个value做unionfind</span></span><br><span class="line">      <span class="keyword">auto</span> update = [&amp;](Value lhs, Value rhs) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!impl_.<span class="built_in">isEquivalent</span>(<span class="built_in">ValueWrapper</span>(lhs), <span class="built_in">ValueWrapper</span>(rhs))) &#123;</span><br><span class="line">          <span class="comment">// 有更改，说明还没有完全收敛</span></span><br><span class="line">          converged = <span class="literal">false</span>;</span><br><span class="line">          impl_.<span class="built_in">unionSets</span>(<span class="built_in">ValueWrapper</span>(lhs), <span class="built_in">ValueWrapper</span>(rhs));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;;</span><br><span class="line">      <span class="keyword">for</span> (Operation* op : op_list) &#123;</span><br><span class="line">        <span class="comment">// 只对有InferShapeEqualityOpInterface trait的operation做shape相等传播</span></span><br><span class="line">        <span class="keyword">auto</span> op_fusibility = <span class="built_in">dyn_cast</span>&lt;InferShapeEqualityOpInterface&gt;(op);</span><br><span class="line">        <span class="keyword">if</span> (!op_fusibility) <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> numInput = op-&gt;<span class="built_in">getNumOperands</span>();</span><br><span class="line">        <span class="type">int</span> numOutput = op-&gt;<span class="built_in">getNumResults</span>();</span><br><span class="line">        <span class="comment">// shape equality propagation between inputs.</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> input1 = <span class="number">0</span>; input1 &lt; numInput; ++input1)</span><br><span class="line">          <span class="keyword">for</span> (<span class="type">int</span> input2 = input1 + <span class="number">1</span>; input2 &lt; numInput; ++input2)</span><br><span class="line">            <span class="comment">// 通过op_fusibility.inferInputsShapeEquality函数判断两个input是否shape相等</span></span><br><span class="line">            <span class="keyword">if</span> (op_fusibility.<span class="built_in">inferInputsShapeEquality</span>(input1, input2))</span><br><span class="line">              <span class="built_in">update</span>(op-&gt;<span class="built_in">getOperand</span>(input1), op-&gt;<span class="built_in">getOperand</span>(input2));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// shape equality propagation between outputs.</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> output1 = <span class="number">0</span>; output1 &lt; numOutput; ++output1)</span><br><span class="line">          <span class="keyword">for</span> (<span class="type">int</span> output2 = output1 + <span class="number">1</span>; output2 &lt; numOutput; ++output2)</span><br><span class="line">            <span class="comment">// 同理，判断两个output是否shape相等</span></span><br><span class="line">            <span class="keyword">if</span> (op_fusibility.<span class="built_in">inferOutputsShapeEquality</span>(output1, output2))</span><br><span class="line">              <span class="built_in">update</span>(op-&gt;<span class="built_in">getResult</span>(output1), op-&gt;<span class="built_in">getResult</span>(output2));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// shape equality propagation between input and output.</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> input = <span class="number">0</span>; input &lt; numInput; ++input)</span><br><span class="line">          <span class="keyword">for</span> (<span class="type">int</span> output = <span class="number">0</span>; output &lt; numOutput; ++output)</span><br><span class="line">            <span class="comment">// 最关键的步骤，判断input和output是否shape相等</span></span><br><span class="line">            <span class="keyword">if</span> (op_fusibility.<span class="built_in">inferInputOutputShapeEquality</span>(input, output))</span><br><span class="line">              <span class="comment">// 如果相等，则调用lambda函数，将两者做unionfind</span></span><br><span class="line">              <span class="built_in">update</span>(op-&gt;<span class="built_in">getOperand</span>(input), op-&gt;<span class="built_in">getResult</span>(output));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">while</span> (!converged);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// a UnionFind set</span></span><br><span class="line">  <span class="comment">// 使用LLVM提供的内置UF集和</span></span><br><span class="line">  EquivalenceClasses&lt;ValueWrapper&gt; impl_;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="planner的初始化"><a href="#planner的初始化" class="headerlink" title="planner的初始化"></a>planner的初始化</h4><p>FusionPlanner的初始化逻辑是比较简单的：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FusionPlanner</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">FusionPlanner</span><span class="params">(<span class="type">const</span> SmallVectorImpl&lt;Operation*&gt;&amp; op_list)</span></span></span><br><span class="line"><span class="function">      : op_list_(op_list),</span></span><br><span class="line"><span class="function">        shape_analysis_(op_list),</span></span><br><span class="line"><span class="function">        cycle_detector_(op_list.size()) &#123;</span></span><br><span class="line">    <span class="comment">// 构建初始的cluster图</span></span><br><span class="line">    <span class="built_in">BuildNodeMap</span>();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>主要干了如下事情：</p>
<ol>
<li>初始化op_list_，用于后续的fusion</li>
<li>初始化shape_analysis_工具，用于知道fusion的shape compatible analysis</li>
<li>初始化cycle detector，<code>XLA</code>的fusion中，一个条件是不能引入loop</li>
</ol>
<p>最后构建一个nodemap，构建一个cluster图，用于后续子图生成，op遍历。</p>
<p>成员变量如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> SmallVectorImpl&lt;Operation*&gt;&amp; op_list_;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Shape equality checker</span></span><br><span class="line">ShapeConstraintAnalysis shape_analysis_;</span><br><span class="line"></span><br><span class="line"><span class="comment">// op -&gt; node_id</span></span><br><span class="line">std::unordered_map&lt;Operation*, <span class="type">int</span>&gt; op_to_node_id_;</span><br><span class="line"></span><br><span class="line"><span class="comment">// make sure not introduce cycle after fusion</span></span><br><span class="line">GraphCycles cycle_detector_;</span><br><span class="line">std::vector&lt;std::unique_ptr&lt;Cluster&gt;&gt; cluster_storage_;</span><br><span class="line"></span><br><span class="line"><span class="comment">// a UnionFind set. Each set represents a (partial) fused pattern</span></span><br><span class="line"><span class="comment">// and has a leader as representation.</span></span><br><span class="line">EquivalenceClasses&lt;<span class="type">int32_t</span>&gt; leader_for_node_;</span><br></pre></td></tr></table></figure>

<p>构建图的逻辑如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">BuildNodeMap</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 当前初始构建图，每个operation为一个node，为一个cluster的head，并leader_for_node_的并查集存储</span></span><br><span class="line">    <span class="type">int</span> num_nodes = op_list_.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> node_id = <span class="number">0</span>; node_id &lt; num_nodes; ++node_id) &#123;</span><br><span class="line">      <span class="comment">// 针对当前operation，构建一个cluster</span></span><br><span class="line">      <span class="comment">// 并设定当前op为cluster的头leader</span></span><br><span class="line">      Operation* op = op_list_[node_id];</span><br><span class="line">      <span class="built_in">MakeCluster</span>(node_id);</span><br><span class="line">      op_to_node_id_[op] = node_id;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// learder_for_node_是一个UnionFind set</span></span><br><span class="line">      leader_for_node_.<span class="built_in">insert</span>(node_id);</span><br><span class="line">      <span class="keyword">for</span> (Value operand : op-&gt;<span class="built_in">getOperands</span>()) &#123;</span><br><span class="line">        Operation* operand_op = operand.<span class="built_in">getDefiningOp</span>();</span><br><span class="line">        <span class="keyword">if</span> (operand_op == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">          <span class="comment">// skip block argument</span></span><br><span class="line">          <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 检查operand的def point是否属于别的融合组</span></span><br><span class="line">        <span class="comment">// 显示构建依赖关系</span></span><br><span class="line">        <span class="keyword">auto</span> iter = op_to_node_id_.<span class="built_in">find</span>(operand_op);</span><br><span class="line">        <span class="built_in">assert</span>(iter != op_to_node_id_.<span class="built_in">end</span>());</span><br><span class="line">        cycle_detector_.<span class="built_in">InsertEdge</span>(iter-&gt;second, node_id);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>遍历每一个op，在unionfind中，每一个op为一个cluster，后续fusion操作才会融合cluster。</li>
<li>初始化op_to_node_id_等的映射方式，unionfind中存储的是该op的node_id，即在当前fusionplan中的次序号。</li>
<li>operand的define op和当前op，在cycle_detector中显示插入依赖链条。</li>
</ul>
<p>上述整体逻辑是十分清楚的。</p>
<p>在FusionPlanner中，有一个私有类是<code>Cluster</code>：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Represent a (partial) fused pattern</span></span><br><span class="line">  <span class="comment">// 根据注释，这个cluster类并不是完整的融合模式，而是一个融合模式的一部分</span></span><br><span class="line">  <span class="comment">// 这是unionfind的思想，每个cluster都是一个融合模式的一部分，最终通过unionfind合并</span></span><br><span class="line">  <span class="keyword">class</span> <span class="title class_">Cluster</span> &#123;</span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Cluster</span>(<span class="type">int</span> node_id, FusionPlanner* planner) : <span class="built_in">node_id_</span>(node_id) &#123;</span><br><span class="line">      <span class="type">const</span> SmallVectorImpl&lt;Operation*&gt;&amp; op_list = planner-&gt;<span class="built_in">op_list</span>();</span><br><span class="line">      pattern_.<span class="built_in">push_back</span>(op_list[node_id]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Merges `other` into this cluster, and clears `other`.</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">Merge</span><span class="params">(Cluster* other)</span> </span>&#123;</span><br><span class="line">      pattern_.<span class="built_in">insert</span>(pattern_.<span class="built_in">end</span>(), other-&gt;pattern_.<span class="built_in">begin</span>(),</span><br><span class="line">                      other-&gt;pattern_.<span class="built_in">end</span>());</span><br><span class="line">      other-&gt;pattern_.<span class="built_in">clear</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The number of nodes in this cluster.</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">cluster_size</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> pattern_.<span class="built_in">size</span>(); &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The ID of the cluster as represented in `cycle_detector_`.</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">cycles_graph_node_id</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> node_id_; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Sets the ID of the cluster as represented in `cycle_detector_`.</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">set_cycles_graph_node_id</span><span class="params">(<span class="type">int</span> cycles_graph_node_id)</span> </span>&#123;</span><br><span class="line">      node_id_ = cycles_graph_node_id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Currently the fused pattern this cluster holds.</span></span><br><span class="line">    <span class="function"><span class="type">const</span> FusionPattern&amp; <span class="title">fused_pattern</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> pattern_; &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// ID of the representative node of this cluster.</span></span><br><span class="line">    <span class="type">int</span> node_id_;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// the fused pattern this cluster holds.</span></span><br><span class="line">    FusionPattern pattern_;</span><br><span class="line">  &#125;;</span><br></pre></td></tr></table></figure>

<p>上述cluster最主要的功能，就是可以对于cluster中的operation，显示记录他们可以应用的<code>FusionPattern</code>。同时还支持merge操作，将cluster之间进行fusion。该原理是将另一个cluster的pattern拷贝入当前cluster，并清空另一个cluster的pattern。</p>
<p><img src="/images/2.png" alt="2"></p>
<h4 id="planner运行"><a href="#planner运行" class="headerlink" title="planner运行"></a>planner运行</h4><p>核心部分，主要作用是构建cluster融合，并给对应cluster赋予fusion pattern。</p>
<p>主函数如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Returns a fusion plan if success, otherwise none.</span></span><br><span class="line">  <span class="comment">// 返回一个fusion plan，如果没有找到则返回none</span></span><br><span class="line">  <span class="function">llvm::Optional&lt;FusionPlan&gt; <span class="title">Run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Greedily search connected fusible pattern, and ops belonging to</span></span><br><span class="line">    <span class="comment">// a same fusion pattern are grouped into a cluster.</span></span><br><span class="line">    <span class="comment">// 每一个op有一个可融合的pattern，找寻compatible的pattern并合并对应的operation</span></span><br><span class="line">    <span class="built_in">RunEdgeContractionLoop</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// After doing edge contraction, each unique cluster having size</span></span><br><span class="line">    <span class="comment">// more than one represents a potential fusion pattern.</span></span><br><span class="line">    <span class="comment">// We collect all these clusters and construct a fusion plan.</span></span><br><span class="line">    <span class="comment">// union find中，每一个有大于1的size的cluster都是一个融合模式</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// Note that the ops in a fusion pattern are in topological ordering.</span></span><br><span class="line">    <span class="comment">// fusion pattern中的operation是按照拓扑排序的</span></span><br><span class="line">    <span class="comment">// 得到一个fusion plan集合后，后续apply这些fusion plan即可</span></span><br><span class="line">    FusionPlan plan;</span><br><span class="line">    DenseMap&lt;<span class="type">int</span>, <span class="type">int</span>&gt; pattern_ids;</span><br><span class="line">    <span class="keyword">for</span> (Operation* op : op_list_) &#123;</span><br><span class="line">      <span class="comment">// 获取该op所属的cluster</span></span><br><span class="line">      Cluster* cluster = <span class="built_in">GetClusterForNode</span>(op);</span><br><span class="line">      <span class="type">int</span> node_id = cluster-&gt;<span class="built_in">cycles_graph_node_id</span>();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 不可融合或是融合模式的size小于等于1，跳过</span></span><br><span class="line">      <span class="keyword">if</span> (!<span class="built_in">IsFusible</span>(op_list_[node_id]) ||</span><br><span class="line">          <span class="built_in">EffectiveSize</span>(<span class="built_in">GetClusterForNode</span>(op)-&gt;<span class="built_in">fused_pattern</span>()) &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">continue</span>;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (!pattern_ids.<span class="built_in">count</span>(node_id)) &#123;</span><br><span class="line">        <span class="type">int</span> pattern_id = pattern_ids.<span class="built_in">size</span>();</span><br><span class="line">        pattern_ids[node_id] = pattern_id;</span><br><span class="line">        plan.<span class="built_in">emplace_back</span>();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 特定的plan后添加operation</span></span><br><span class="line">      plan[pattern_ids[node_id]].<span class="built_in">push_back</span>(op);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> plan;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>这里需要理解一个重要的点：每一个cluster都有一个list，该list存储所有的fusion Pattern，一个cluster是否可以做融合，其实就是看发现了多少个fusion Pattern。</p>
<ul>
<li>如果fusion pattern &gt; 1，则其<strong>内部可以融合</strong>。</li>
<li>如果cluster之间有兼容的fusion pattern，则<strong>intra cluster 融合</strong>是ok的。</li>
</ul>
<p><font color =red>识别出潜在的fusion后，构造fusion plan：一个fusion pattern的vector存储结构。</font></p>
</blockquote>
<p>上述code可以分为两大阶段：<code>RunEdgeContractionLoop()</code>和融合pattern收集。</p>
<p><img src="/images/1.png" alt="1"></p>
<p>上述是一个完整的流程图。可以看出，run()函数最重要的function是<code>RunEdgeContractionLoop()</code>。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Greedily fuse connected node.</span></span><br><span class="line"><span class="comment">// 贪心算法，融合可融合的node</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">RunEdgeContractionLoop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">using</span> std::placeholders::_1;</span><br><span class="line">    <span class="keyword">using</span> std::placeholders::_2;</span><br><span class="line">    <span class="comment">// 理解std::bind操作</span></span><br><span class="line">    <span class="comment">// 给TryToContractEdge函数绑定了两个参数，第一个参数是this指针，第二个参数是_1和_2，是占位符</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">ForEachEdgeInPostOrder</span>(</span><br><span class="line">        std::<span class="built_in">bind</span>(&amp;FusionPlanner::TryToContractEdge, <span class="keyword">this</span>, _1, _2));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对于graph做后序遍历，并<strong>贪心引用边收缩算法</strong>。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 按照后序遍历的顺序，对每一个edge执行fn函数</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> FnTy&gt;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">ForEachEdgeInPostOrder</span><span class="params">(FnTy fn)</span> </span>&#123;</span><br><span class="line"><span class="type">bool</span> changed = <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int32_t</span> node : cycle_detector_.<span class="built_in">AllNodesInPostOrder</span>()) &#123;</span><br><span class="line">  Cluster* cluster_from = <span class="built_in">GetClusterForCyclesGraphNode</span>(node);</span><br><span class="line">  <span class="comment">// Make a copy of the set of successors because we may modify the graph in</span></span><br><span class="line">  <span class="comment">// TryToContractEdge.</span></span><br><span class="line">  <span class="comment">// 可能在TryToContractEdge函数中修改后续的node，所以这里需要拷贝一份</span></span><br><span class="line">  std::vector&lt;<span class="type">int32_t</span>&gt; successors_copy =</span><br><span class="line">      cycle_detector_.<span class="built_in">SuccessorsCopy</span>(cluster_from-&gt;<span class="built_in">cycles_graph_node_id</span>());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 对该节点的后续分别尝试做融合</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> to : successors_copy) &#123;</span><br><span class="line">    Cluster* cluster_to = <span class="built_in">GetClusterForCyclesGraphNode</span>(to);</span><br><span class="line">    <span class="comment">// 这里传入的fn是TryToContractEdge函数，传入cluster_from和cluster_to两个参数，通过std::bind绑定。</span></span><br><span class="line">    <span class="type">bool</span> contracted_edge = <span class="built_in">fn</span>(cluster_from, cluster_to);</span><br><span class="line">    changed |= contracted_edge;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> changed;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该逻辑是，后续访问每个node，并尝试对后续的每个op尝试引用fn函数，即<code>TryToContractEdge</code>函数。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// This function check if fusing `from` with `to` is valid and if so perform</span></span><br><span class="line"><span class="comment">// the merge. The validity is based on the operations in the clusters and</span></span><br><span class="line"><span class="comment">// the compatibility of the shapes of the outputs of the would-be fused</span></span><br><span class="line"><span class="comment">// clusters.</span></span><br><span class="line"><span class="comment">// Returns true is the merge was performed.</span></span><br><span class="line"><span class="comment">// 尝试合并两个cluster，如果合并成功则返回true</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">TryToContractEdge</span><span class="params">(Cluster* from, Cluster* to)</span> </span>&#123;</span><br><span class="line"><span class="type">int</span> node_to = to-&gt;<span class="built_in">cycles_graph_node_id</span>();</span><br><span class="line"><span class="type">int</span> node_from = from-&gt;<span class="built_in">cycles_graph_node_id</span>();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Both node_to and node_from should be fusible</span></span><br><span class="line"><span class="keyword">if</span> (!<span class="built_in">IsFusible</span>(op_list_[node_to]) || !<span class="built_in">IsFusible</span>(op_list_[node_from])) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 判断node from是否可以和consumer融合</span></span><br><span class="line"><span class="comment">// 即该node是否是const或是elementwise</span></span><br><span class="line"><span class="keyword">if</span> (!<span class="built_in">IsFusibleWithConsumer</span>(op_list_[node_from])) &#123;</span><br><span class="line">  <span class="comment">// This op cannot be fused with its consumers.</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 判断node to是否可以和operand融合</span></span><br><span class="line"><span class="comment">// 即该node是否是element wise或是const或是reduce</span></span><br><span class="line"><span class="keyword">if</span> (!<span class="built_in">IsFusibleWithOperand</span>(op_list_[node_to])) &#123;</span><br><span class="line">  <span class="comment">// This op cannot be fused with its operands.</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Output shapes of a fusion pattern should be compatible as described in</span></span><br><span class="line"><span class="comment">// the document of this class.</span></span><br><span class="line"><span class="comment">// 判断两个cluster的输出是否shape相同</span></span><br><span class="line">SmallVector&lt;Value, <span class="number">4</span>&gt; results = <span class="built_in">GetResultsOfFusedPattern</span>(from, to);</span><br><span class="line"></span><br><span class="line">Value ref = <span class="built_in">InferEffectiveWorkloadShape</span>(results[<span class="number">0</span>]);</span><br><span class="line"><span class="keyword">if</span> (!llvm::<span class="built_in">all_of</span>(results, [&amp;](Value result) &#123;</span><br><span class="line">      Value val = <span class="built_in">InferEffectiveWorkloadShape</span>(result);</span><br><span class="line">      <span class="keyword">return</span> shape_analysis_.<span class="built_in">HasSameShape</span>(ref, val);</span><br><span class="line">    &#125;)) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 实际的fusion操作，将cluster做fusion</span></span><br><span class="line"><span class="keyword">return</span> <span class="built_in">MergeClusters</span>(from, to);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意，如果op是reduceOp，则其只能是to，不能是from，所以<code>reduceOp</code>一定在sub graph的end point。</p>
<ul>
<li><p>判断from和to是否可fuse：isfusable()</p>
</li>
<li><p><code>GetResultsOfFusedPattern</code>：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// returns the outputs if two cluster were merged</span></span><br><span class="line"><span class="function">SmallVector&lt;Value, 4&gt; <span class="title">GetResultsOfFusedPattern</span><span class="params">(Cluster* from, Cluster* to)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 将两个cluster的fused_pattern合并</span></span><br><span class="line">FusionPattern fused_pattern =</span><br><span class="line">    <span class="built_in">MergeFusionPattern</span>(from-&gt;<span class="built_in">fused_pattern</span>(), to-&gt;<span class="built_in">fused_pattern</span>());</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">GetOutputsOfFusionPattern</span>(fused_pattern);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>尝试强行融合两个cluster的pattern，并获取一个fusion厚的output。</p>
</li>
<li><p>显示做shape 判断，来判断前一步的fusion是否是合法的：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">Value ref = <span class="built_in">InferEffectiveWorkloadShape</span>(results[<span class="number">0</span>]);</span><br><span class="line"><span class="keyword">if</span> (!llvm::<span class="built_in">all_of</span>(results, [&amp;](Value result) &#123;</span><br><span class="line">      Value val = <span class="built_in">InferEffectiveWorkloadShape</span>(result);</span><br><span class="line">      <span class="keyword">return</span> shape_analysis_.<span class="built_in">HasSameShape</span>(ref, val);</span><br><span class="line">    &#125;)) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个inferworkload逻辑如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 根据value的类型，判断workload的shape的推导方式</span></span><br><span class="line"><span class="comment">// 主要针对reduce op做特殊化处理</span></span><br><span class="line"><span class="function">Value <span class="title">InferEffectiveWorkloadShape</span><span class="params">(Value v)</span> </span>&#123;</span><br><span class="line">  Operation* op = v.<span class="built_in">getDefiningOp</span>();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 如果是reduce op，则返回operand的shape</span></span><br><span class="line">  <span class="comment">// 否则，v本身用于推导shape</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">isa_and_nonnull</span>&lt;ReduceOp&gt;(op) ? op-&gt;<span class="built_in">getOperand</span>(<span class="number">0</span>) : v;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>取融合后第一个输出的“有效工作负载形状”作为参考。</p>
</li>
<li><p>对普通操作，有效形状即输出本身的形状；对 <code>ReduceOp</code>，有效形状是操作数的形状（因为融合需基于其输入数据的形状）。</p>
</li>
<li><p>对融合后的所有输出结果，逐一检查它们的有效形状是否与参考形状一致。</p>
</li>
<li><p>若存在任意一个输出形状不匹配，返回 <code>false</code>，拒绝合并。</p>
</li>
</ul>
<p>这背后的原理如下:</p>
<blockquote>
<p><strong>kLoop 融合</strong>：要求所有输出形状一致，以放入同一并行循环。</p>
<p><strong>kInput 融合</strong>：</p>
<p>允许包含 <code>ReduceOp</code>，但其有效形状需与其他输出的有效形状一致（即 <code>ReduceOp</code> 的输入形状需与其他输出形状一致）。例如，若融合模式包含一个reduceOp和elementwiseOp，做如下操作：</p>
<ol>
<li><p><code>ReduceOp</code> 的有效形状是其输入（操作数）的形状。</p>
</li>
<li><p><code>ElementWise</code> 的有效形状是其输出的形状。</p>
</li>
<li><p>两者必须相同才能融合。</p>
</li>
</ol>
</blockquote>
</li>
</ul>
<p><img src="/images/3.png" alt="3"></p>
<h4 id="ApplyFusionPlan"><a href="#ApplyFusionPlan" class="headerlink" title="ApplyFusionPlan"></a>ApplyFusionPlan</h4><p>源码如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">ApplyFusionPlan</span><span class="params">(<span class="type">const</span> FusionPlan&amp; plan)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 遍历每个融合模式</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">const</span> FusionPattern&amp; pattern : plan) &#123;</span><br><span class="line">        <span class="comment">// 在pattern最后一个操作位置创建Builder</span></span><br><span class="line">        <span class="function">OpBuilder <span class="title">b</span><span class="params">(pattern.back())</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 收集所有操作的位置信息</span></span><br><span class="line">        SmallVector&lt;Location, <span class="number">4</span>&gt; locations;</span><br><span class="line">        locations.<span class="built_in">reserve</span>(pattern.<span class="built_in">size</span>());</span><br><span class="line">        <span class="keyword">for</span> (Operation* op : pattern) &#123;</span><br><span class="line">            locations.<span class="built_in">push_back</span>(op-&gt;<span class="built_in">getLoc</span>());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 创建融合位置标识（用于调试）</span></span><br><span class="line">        Location fused_loc = FusedLoc::<span class="built_in">get</span>(pattern.<span class="built_in">back</span>()-&gt;<span class="built_in">getContext</span>(), locations);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取外部输入和输出</span></span><br><span class="line">        SmallVector&lt;Value, <span class="number">4</span>&gt; inputs = <span class="built_in">GetInputsOfFusionPattern</span>(pattern);</span><br><span class="line">        SmallVector&lt;Value, <span class="number">4</span>&gt; outputs = <span class="built_in">GetOutputsOfFusionPattern</span>(pattern);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 构建输出类型列表</span></span><br><span class="line">        SmallVector&lt;Type, <span class="number">4</span>&gt; output_types;</span><br><span class="line">        output_types.<span class="built_in">reserve</span>(outputs.<span class="built_in">size</span>());</span><br><span class="line">        <span class="keyword">for</span> (Value v : outputs) &#123;</span><br><span class="line">            output_types.<span class="built_in">push_back</span>(v.<span class="built_in">getType</span>());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* 消费者调整阶段 */</span></span><br><span class="line">        <span class="comment">// 记录融合操作集合</span></span><br><span class="line">        <span class="function">DenseSet&lt;Operation*&gt; <span class="title">fused_set</span><span class="params">(pattern.begin(), pattern.end())</span></span>;</span><br><span class="line">        DenseSet&lt;Operation*&gt; consumers_set;  <span class="comment">// 已处理的消费者</span></span><br><span class="line">        SmallVector&lt;Operation*, <span class="number">4</span>&gt; consumers_vec; <span class="comment">// 待移动的消费者</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 确定原始代码范围：第一个融合操作到最后一个的迭代器范围</span></span><br><span class="line">        <span class="keyword">auto</span> first_iter = pattern.<span class="built_in">front</span>()-&gt;<span class="built_in">getIterator</span>();</span><br><span class="line">        <span class="keyword">auto</span> last_iter = pattern.<span class="built_in">back</span>()-&gt;<span class="built_in">getIterator</span>();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 扫描区间内的所有操作</span></span><br><span class="line">        <span class="keyword">for</span> (Operation&amp; cur_op : llvm::<span class="built_in">make_range</span>(first_iter, last_iter)) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!fused_set.<span class="built_in">contains</span>(&amp;cur_op)) &#123; <span class="comment">// 非融合操作</span></span><br><span class="line">                <span class="comment">// 检查是否是消费者：操作数来自融合集或已标记的消费者</span></span><br><span class="line">                <span class="type">bool</span> is_consumer = llvm::<span class="built_in">any_of</span>(cur_op.<span class="built_in">getOperands</span>(), </span><br><span class="line">                    [&amp;](Value v) &#123;</span><br><span class="line">                        Operation* def_op = v.<span class="built_in">getDefiningOp</span>();</span><br><span class="line">                        <span class="keyword">return</span> fused_set.<span class="built_in">contains</span>(def_op) || consumers_set.<span class="built_in">contains</span>(def_op);</span><br><span class="line">                    &#125;);</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> (is_consumer) &#123;</span><br><span class="line">                    consumers_set.<span class="built_in">insert</span>(&amp;cur_op); <span class="comment">// 标记为已处理</span></span><br><span class="line">                    consumers_vec.<span class="built_in">push_back</span>(&amp;cur_op); <span class="comment">// 加入移动队列</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 逆序移动消费者到融合点之后（防止顺序移动导致迭代器失效）</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span>* op : llvm::<span class="built_in">reverse</span>(consumers_vec)) &#123;</span><br><span class="line">            op-&gt;<span class="built_in">moveAfter</span>(pattern.<span class="built_in">back</span>()); <span class="comment">// 重定位到融合操作末尾</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* 创建融合操作 */</span></span><br><span class="line">        FusionOp fusion = b.<span class="built_in">create</span>&lt;mhlo::FusionOp&gt;(fused_loc, output_types, inputs);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 构建融合计算区域</span></span><br><span class="line">        Region&amp; region = fusion.<span class="built_in">fused_computation</span>();</span><br><span class="line">        region.<span class="built_in">push_back</span>(<span class="keyword">new</span> Block); <span class="comment">// 创建基本块</span></span><br><span class="line">        Block&amp; block = region.<span class="built_in">front</span>();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 将原始操作移入融合区域</span></span><br><span class="line">        <span class="keyword">for</span> (Operation* op : pattern) &#123;</span><br><span class="line">            op-&gt;<span class="built_in">moveBefore</span>(&amp;block, block.<span class="built_in">end</span>()); <span class="comment">// 保持原有顺序</span></span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 在区域末尾插入ReturnOp</span></span><br><span class="line">        b.<span class="built_in">setInsertionPoint</span>(&amp;block, block.<span class="built_in">end</span>());</span><br><span class="line">        b.<span class="built_in">create</span>&lt;mhlo::ReturnOp&gt;(fused_loc, outputs);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* 结果替换阶段 */</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> [output, fusion_result] : llvm::<span class="built_in">zip</span>(outputs, fusion.<span class="built_in">getResults</span>())) &#123;</span><br><span class="line">            <span class="comment">// 替换所有外部使用点</span></span><br><span class="line">            <span class="keyword">for</span> (OpOperand&amp; use : llvm::<span class="built_in">make_early_inc_range</span>(output.<span class="built_in">getUses</span>())) &#123;</span><br><span class="line">                <span class="keyword">if</span> (use.<span class="built_in">getOwner</span>()-&gt;<span class="built_in">getBlock</span>() != &amp;block) &#123; <span class="comment">// 外部使用</span></span><br><span class="line">                    use.<span class="built_in">set</span>(fusion_result); <span class="comment">// 替换为融合结果</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如下是<code>ApplyFusionPlan</code>的流程图</p>
<p><img src="/images/4.png" alt="4"></p>
<p>其中比较核心的是识别消费者操作，该consumer要求是直接或间接依赖fusion里的operation，但本身并不是fusion operation，并且其location在fusionOp的范围内，<strong>因此需要显示地移动</strong>。</p>
<p><img src="/images/4-1743436524621-10.png" alt="4"></p>
<h2 id="BladeDISC-source-code分析"><a href="#BladeDISC-source-code分析" class="headerlink" title="BladeDISC source code分析"></a><font color = brown>BladeDISC source code分析</font></h2><p><code>BladeDISC</code>的kernel 融合主要参考<code>AStitch</code>和<a href="http://arxiv.org/abs/2009.10924">titch fusion</a>。</p>
<img src="/images/image-20250326111430445.png" alt="4" style="zoom:67%;" />

<p>上述很好地阐述了应用XLA和fusion stitich技术的差异。<font color = red>XLA编译器无法对middle reduce操作做fusion</font>，fusion stitch划分四类memory intensive op融合方式，起到有效扩展作用：</p>
<img src="/images/image-20250326114004653.png" style="zoom:67%;" />

<h3 id="Source-code解读"><a href="#Source-code解读" class="headerlink" title="Source code解读"></a><font color = green>Source code解读</font></h3><p>一个总的框架：</p>
<img src="/images/image-20250327221745561.png" style="zoom:67%;" />

<p>上述详情参考<a href="http://arxiv.org/abs/2009.10924">BladeDISC slide</a>。</p>
<h4 id="节点类型划分"><a href="#节点类型划分" class="headerlink" title="节点类型划分"></a>节点类型划分</h4><p>stich的fusion pattern中，相比xla，其重点是将node划分为不同的类：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Represents a list of lmhlo ops that are going to be fused.</span></span><br><span class="line"><span class="comment">// Concepts for a fusion pattern:</span></span><br><span class="line"><span class="comment">//   - Root op: the op whose output is the fusion-pattern&#x27;s output.</span></span><br><span class="line"><span class="comment">//     Sub-root op可以理解为stich fusion的缝合点，为用shared-memory缝合的op bound</span></span><br><span class="line"><span class="comment">//   - Sub-root op: the op whose output is to be maintained on shared-memory for</span></span><br><span class="line"><span class="comment">//     kStitch fusion. Currently, we only support row-reduction to be a sub-root</span></span><br><span class="line"><span class="comment">//     op.</span></span><br><span class="line"><span class="comment">//   - Regular xroot op: either a root op or a sub-root op, for whose operands</span></span><br><span class="line"><span class="comment">//     we successfully build tile information during kStitch fusion-pattern init</span></span><br><span class="line"><span class="comment">//     phase.</span></span><br><span class="line"><span class="comment">//   - Irregular xroot op: an root op for whose operands we fail to build tile</span></span><br><span class="line"><span class="comment">//     information durint kStitch fusion-pattern init phase.</span></span><br><span class="line"><span class="comment">//   - Skeleton op: the op who will be used to build the loop skeleton when</span></span><br><span class="line"><span class="comment">//     lowering a kStitch fusion to parallel loops. Currently, sub-root ops, and</span></span><br><span class="line"><span class="comment">//     regular xroot ops who generate external only results, are skeleton ops.</span></span><br><span class="line"><span class="comment">//     Other xroot ops are lowered with input-inline fusion phase.</span></span><br><span class="line"><span class="comment">//   Note: for an regular xroot op which is not an skeleton op, the output data</span></span><br><span class="line"><span class="comment">//     to be written should be coverred by its corresponding skeleton op.</span></span><br><span class="line"><span class="comment">//     Otherwise, this xroot are regared as irregular.</span></span><br></pre></td></tr></table></figure>

<p>上述注释中详细解读了node的分类：</p>
<ul>
<li>Root op：<code>fusion-pattern</code>的output node，即fusion的边界。</li>
<li>Sub-root op：通过shared-memory fuse的op。</li>
<li>xroot op：在astich论文中，每个op的thread感知分配信息，都是通过分析sub-root或是root，然后反向传播到整个fusion区域。xroot op是分析出thread信息的root或sub-root</li>
<li>Irregular xroot：没有分析出thread信息的sub root或是root op。</li>
<li>Skeleton op：负责构建<strong>动态shape的并行循环骨架</strong>，是GPU kernel代码生成的模板基础。通过选择具有典型计算特征的子根操作（如行归约）作为骨架，能够自动推导出循环维度、分块策略等关键参数。主要负责<strong>codegen</strong>部分。</li>
</ul>
<h4 id="Shape-analysis"><a href="#Shape-analysis" class="headerlink" title="Shape analysis"></a>Shape analysis</h4><h4 id="GPU-Stitch策略"><a href="#GPU-Stitch策略" class="headerlink" title="GPU Stitch策略"></a>GPU Stitch策略</h4><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 重点关注如何将stitch技术用在gpu上</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StitchGpuFusionStrategy</span> : <span class="keyword">public</span> FusionStrategy &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">StitchGpuFusionStrategy</span>(<span class="type">const</span> FusionOptions&amp; options)</span><br><span class="line">      : <span class="built_in">FusionStrategy</span>(options) &#123;&#125;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="type">bool</span> <span class="title">isFusible</span><span class="params">(Operation* op)</span> <span class="keyword">override</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="type">bool</span> <span class="title">tryFuse</span><span class="params">(ShapeAnalysis&amp; shapeAnalysis, FusionPattern&amp; lhs,</span></span></span><br><span class="line"><span class="params"><span class="function">                       FusionPattern&amp; rhs, FusionPattern&amp; target)</span> <span class="keyword">override</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="type">bool</span> <span class="title">initFusionPattern</span><span class="params">(ShapeAnalysis&amp; shapeAnalysis,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 FusionPattern&amp; fusion_pattern)</span> <span class="keyword">override</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> StringRef <span class="title">getName</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123; <span class="keyword">return</span> <span class="string">&quot;StitchGpuFusionStrategy&quot;</span>; &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> Value <span class="title">getEffectiveShape</span><span class="params">(FusionPattern&amp; target, Value value)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">bool</span> <span class="title">tileCoverInfoPropagateO2I</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      ShapeAnalysis&amp; shapeAnalysis, DenseMap&lt;Value, TileInfo&gt;&amp; tile_plan,</span></span></span><br><span class="line"><span class="params"><span class="function">      Operation* op, SmallVector&lt;std::pair&lt;Value, TileInfo&gt;, <span class="number">4</span>&gt;&amp; in_info,</span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="type">bool</span>&amp; cover)</span></span>;</span><br><span class="line">  <span class="function"><span class="type">bool</span> <span class="title">findFusionPatternTypeAndSubroot</span><span class="params">(ShapeAnalysis&amp; shapeAnalysis,</span></span></span><br><span class="line"><span class="params"><span class="function">                                       FusionPattern&amp; fusion_pattern)</span></span>;</span><br><span class="line">  <span class="function"><span class="type">bool</span> <span class="title">tileXroots</span><span class="params">(ShapeAnalysis&amp; shapeAnalysis, FusionPattern&amp; fusion_pattern)</span></span>;</span><br><span class="line">  <span class="function"><span class="type">bool</span> <span class="title">backtraceTileAndCover</span><span class="params">(ShapeAnalysis&amp; shapeAnalysis,</span></span></span><br><span class="line"><span class="params"><span class="function">                             FusionPattern&amp; fusion_pattern, Value value)</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>整体的流程框架如下：</p>
<p><img src="/images/6.png" alt="6"></p>
<p>上述流程图很好的表达了整个流程框架：</p>
<ul>
<li><p><code>initFusionPatter</code>负责初始化fusion的option信息</p>
</li>
<li><p><code>findFusionPatternTypeAndSubroot</code>负责引用fusion，找寻subroot等特殊节点，其整体逻辑如下：</p>
<p><img src="/images/7.png" alt="7"></p>
</li>
<li><p><code>tileXroots</code>针对sub root做线程分配算法</p>
<p><img src="/images/8.png" alt="8"></p>
</li>
<li><p><code>backtraceTileAndCover</code>在一个fusion中，从不同sub root出发，做反向thread分配推导</p>
<p><img src="/images/9.png" alt="9"></p>
</li>
</ul>
<p>对应论文中的图片：</p>
<p><img src="/images/image-20250327184033247.png" alt="image-20250327184033247"></p>
<p>上述source code和论文中的对应描述参考论文中的chapter4.3的steps。</p>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>机器学习编译</category>
        <category>访存优化</category>
      </categories>
      <tags>
        <tag>机器学习编译器</tag>
        <tag>mlir</tag>
        <tag>算子融合技术</tag>
      </tags>
  </entry>
</search>
