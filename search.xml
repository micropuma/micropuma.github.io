<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>BladeDISC: RAL overview</title>
    <url>/2025/04/29/BladeDISC-RAL-overview/</url>
    <content><![CDATA[<p><img src="/images/image-20250429153010114.png" alt="image-20250429153010114"></p>
<span id="more"></span>

<blockquote>
<p>在先前的关于编译器后端&amp;运行时博客中，简单介绍了各种成熟的机器学习编译器的runtime system。本文结合<a href="https://github.com/alibaba/BladeDISC/blob/main/docs/developers/runtime_abstraction_layer.md">BladeDISC RAL文档</a>详细解读一下BladeDISC的runtime设计。由于BladeDISC一般作为python&#x2F;tensorflow的pulgin使用，所以其一部分runtime依托现成机器学习框架的runtime，本文也会简单补充一下pytorch的runtime系统。</p>
</blockquote>
<h2 id="BladeDISC-RAL"><a href="#BladeDISC-RAL" class="headerlink" title="BladeDISC RAL"></a><font color = borwn>BladeDISC RAL</font></h2><h3 id="RAL原理"><a href="#RAL原理" class="headerlink" title="RAL原理"></a><font color = green>RAL原理</font></h3><p>在开始深入BladeDISC的具体源码实现之前，我们先要讲明Compiler，Runtime之间的中间界面：硬件抽象层（RAL）。</p>
<img src="/images/image-20250429154132773.png" alt="image-20250429154132773" style="zoom:50%;" />

<p>如上图所示是各个部分的层级关系。对于RAL，BladeDISC编译器是如下解释的：</p>
<blockquote>
<p>Runtime Abstraction Layer (RAL) 是 BladeDISC 编译器的核心组件，旨在连接编译器与多样化的运行时环境，解决跨平台兼容性和资源管理问题。RAL 的核心功能体现在两个方面：首先，它通过抽象统一的接口屏蔽不同运行时环境（如 TensorFlow、PyTorch 或独立二进制）的底层差异，使编译器只需针对 RAL 生成代码，而无需为每个平台单独适配。这种设计不仅简化了编译器的开发逻辑，还支持“一次编译，多处运行”，降低了用户在不同框架间迁移的成本。其次，RAL 通过上下文对象（Context）集中管理有状态资源（如 GPU 内核、内存等），采用懒初始化（Lazy Initialization）策略优化性能。例如，GPU 内核的加载仅在首次使用时触发，后续调用直接复用，避免了重复初始化的开销。通过将资源状态（如“内核是否已加载”）隐藏在上下文背后，RAL 对外提供初始化无关的接口（如 <code>launch_kernel</code>），使得编译器生成的代码无需关注资源初始化细节，只需调用简单接口即可完成任务。</p>
</blockquote>
<h3 id="BladeDISC-RAL总体设计"><a href="#BladeDISC-RAL总体设计" class="headerlink" title="BladeDISC RAL总体设计"></a><font color = green>BladeDISC RAL总体设计</font></h3><p>这一块主要参考<a href="https://github.com/alibaba/BladeDISC/blob/main/docs/developers/runtime_abstraction_layer.md">BladeDISC RAL设计文档</a>。在设计文档中，从编译器角度和运行时角度两个角度来讲解RAL的设计理念。我个人的浅薄理解，从编译角度主要是解读如何将编译出的binary code，封装成Runtime API，而运行时角度则是如何调度运行封装好的Runtime API，从这两个角度均能体现RAL的设计的价值。</p>
<h4 id="从compiler角度"><a href="#从compiler角度" class="headerlink" title="从compiler角度"></a>从compiler角度</h4><p>RAL 在编译器侧通过一系列转换流程（Transformation Passes）将代码适配到 RAL 运行时环境，其核心设计围绕上下文注入、输入输出绑定和统一类型擦除的ABI展开。</p>
<ul>
<li><p>上下文注入</p>
<p>将状态操作（如 GPU 内核加载、内存分配）与编译器逻辑解耦。具体实现方式上</p>
<ol>
<li>自定义MLIR方言：通过 MLIR 框架定义 <code>disc_ral</code> 方言，引入 <code>disc_ral.RalExecutionContextType</code> 类型表示运行时上下文（Context）。该类型在 LLVM IR 中转换为指针类型，隐藏底层资源状态。</li>
<li>入口函数重写：译器入口函数（如 <code>main</code>）的首个参数强制注入 Context 对象，所有 RAL API 调用均以 Context 为第一个参数。例如：</li>
</ol>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 转换前：普通函数参数</span></span><br><span class="line">func @<span class="built_in">main</span>(%arg0 : memref&lt;?x?xf32&gt;, %arg1 : memref&lt;?x?xf32&gt;) -&gt; memref&lt;?x?xf32&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 转换后：首参数注入 Context</span></span><br><span class="line">func @<span class="built_in">main</span>(!disc_ral.context %ctx) &#123;</span><br><span class="line">  %arg0 = disc_ral.<span class="built_in">recv_input</span>(%ctx, <span class="number">0</span>)  <span class="comment">// 通过 Context 获取输入</span></span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p> 这种设计使得编译器无需感知资源状态（如“内核是否已加载”），专注优化逻辑（如算子融合、内存分配优化）。</p>
</blockquote>
</li>
<li><p>输入输出绑定</p>
<p>标准化编译模块与宿主环境（如 TensorFlow&#x2F;PyTorch）的数据交互接口。具体实现方式上：</p>
<ol>
<li><p>动态输入输出接收：入口函数的输入&#x2F;输出不再直接传递内存对象，而是通过 <code>disc_ral.recv_input</code> 和 <code>disc_ral.send_output</code> API 从 Context 中按需获取或发送。例如：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 原始 IR：显式传递内存引用</span></span><br><span class="line">func @<span class="built_in">main</span>(%arg0 : memref&lt;?x?xf32&gt;, %arg1 : memref&lt;?x?xf32&gt;) -&gt; memref&lt;?x?xf32&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 转换后：通过 Context 动态绑定</span></span><br><span class="line">func @<span class="built_in">main</span>(!disc_ral.context %ctx) &#123;</span><br><span class="line">  %arg0 = disc_ral.<span class="built_in">recv_input</span>(%ctx, <span class="number">0</span>)  <span class="comment">// 接收第 0 号输入</span></span><br><span class="line">  %ret = <span class="built_in">alloc</span>(...)</span><br><span class="line">  disc_ral.<span class="built_in">send_output</span>(%ctx, <span class="number">0</span>, %ret)   <span class="comment">// 发送第 0 号输出</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>这种动态输入输出绑定的好处是：</p>
<ul>
<li>允许部分输入未就绪时提前执行部分计算（如流水线并行），或提前发送部分输出，减少端到端延迟（<strong>runtime调度层面可以做一定的优化过程</strong>）。</li>
</ul>
</blockquote>
</li>
</ol>
</li>
<li><p>类型擦除call function api</p>
<p>跨语言（C&#x2F;C++&#x2F;Python）兼容调用 RAL 函数，同时保持 ABI 稳定。具体实现方式如下：</p>
<ol>
<li><p>类型擦除接口，负责将所有 RAL 函数在编译后统一转换为 C 语言风格的泛型接口 <code>ral_api_call</code>，通过 <code>api_name</code> 动态分发。例如：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// C++ 原函数</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">gemm</span><span class="params">(RalContext*, gpu_stream_handle*, MemRef&lt;<span class="type">float</span>,<span class="number">2</span>&gt; lhs, ...)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 统一类型擦除接口</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">ral_api_call</span><span class="params">(<span class="type">void</span>* ctx, <span class="type">const</span> <span class="type">char</span>* api_name, <span class="type">void</span>** args)</span></span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>统一函数签名编码，<code>api_name</code> 按规则生成唯一标识，包含设备类型（如 CPU&#x2F;GPU）、输入输出类型编码等。例如：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">api_name = <span class="string">&quot;ral_gemm___cpu___float_2_float_2_float_2___bool_bool&quot;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>自动注册机制，通过宏 <code>TAO_RAL_API</code> 将模板化的 C++ 函数注册为类型擦除接口，例如：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 注册 float 类型的 GEMM 到 CPU</span></span><br><span class="line"><span class="built_in">TAO_RAL_API</span>(<span class="string">&quot;ral_gemm&quot;</span>, <span class="string">&quot;cpu&quot;</span>, gemm&lt;<span class="type">float</span>&gt;)</span><br><span class="line"><span class="comment">// 注册 half 类型的 GEMM 到 GPU</span></span><br><span class="line"><span class="built_in">TAO_RAL_API</span>(<span class="string">&quot;ral_gemm&quot;</span>, <span class="string">&quot;gpu&quot;</span>, gemm&lt;half&gt;)</span><br></pre></td></tr></table></figure></li>
</ol>
<blockquote>
<p>这套类型擦除机制，允许开发者可直接使用 C++ 接口，编译器自动处理跨语言调用细节，避免手动维护类型转换代码。</p>
</blockquote>
</li>
</ul>
<h4 id="从runtime角度"><a href="#从runtime角度" class="headerlink" title="从runtime角度"></a>从runtime角度</h4><p><img src="/images/image-20250429153010114.png" alt="image-20250429153010114"></p>
<p>上述流程图很好地概括了runtime系统的架构。具体地可以分为三个方面来解读：</p>
<p>Runtime Abstraction Layer（RAL）在运行时通过三层机制实现与宿主环境（如TensorFlow、PyTorch）的高效交互，平衡跨平台兼容性与性能优化。</p>
<ul>
<li><p><strong>面向不同宿主平台（TensorFlow&#x2F;PyTorch）的定制化适配层</strong></p>
<p>处理输入输出绑定和元数据同步。例如，TensorFlow通过其特有的<code>Tensor</code>对象与RAL内存结构交互，而PyTorch需适配<code>torch.Tensor</code>的传输逻辑。这种差异化实现确保了数据在宿主环境与RAL间的高效传递，同时复用宿主原生的设备资源访问方式（如直接操作CUDA流），避免冗余数据拷贝或格式转换，最小化跨环境调用的性能损耗。</p>
</li>
<li><p><strong>面向不同底层硬件的设备驱动API</strong></p>
<p>涵盖内存管理、任务启动和同步操作。由于不同宿主环境对设备的抽象差异显著（如TensorFlow使用<code>DeviceContext</code>管理GPU，而PyTorch依赖<code>c10::Stream</code>），RAL需为同一设备在不同环境中实现独立的驱动接口。例如，GPU内存分配在TensorFlow中通过<code>GPUDevice::Allocate</code>完成，而在PyTorch中则需调用<code>CUDACachingAllocator</code>，RAL通过封装统一的<code>ral_malloc</code>接口隐藏这些差异，使编译器生成的代码无需感知底层环境细节。</p>
</li>
<li><p><strong>跨平台共享的自定义内核</strong>（如第三方库加速的矩阵乘法或排序算法）。</p>
<p>这些内核直接调用RAL驱动API访问设备资源，天然兼容不同宿主环境。例如，基于cuBLAS实现的GPU矩阵乘法内核通过<code>ral_gpu_memcpy</code>和<code>ral_gpu_stream_sync</code>操作内存与计算流，无论部署到TensorFlow还是PyTorch均无需修改代码。</p>
</li>
</ul>
<p>通过上述三个层次的职责划分，可以看出整个BladeDISC的底层runtime设计，其实是大量依赖于成熟的pytorch和tensorflow的运行时系统的，BladeDISC做的事情是设计实现RAL层，高效完成跨宿主（指tensorflow&#x2F;pytorch系统）适配。</p>
<blockquote>
<p>一些个人体会：</p>
<p>这种分层设计（宿主适配层-设备驱动层-内核层）使得开发者只需维护单一版本的核心逻辑，显著降低多框架支持成本，同时通过复用宿主原生接口和懒初始化策略（如首次调用时加载GPU内核），确保运行时性能接近原生框架水平。此外，新增宿主环境（如ONNX Runtime）时，仅需扩展适配层与驱动层，无需改动上层内核，为BladeDISC的跨平台部署提供了高度可扩展的技术基础。</p>
</blockquote>
<p>上述三个核心组件讲完之后，来介绍一下BladeDISC的运行时管理组件：</p>
<p><strong>RAL Context 与 Execution Context 的架构解析</strong></p>
<p>RAL Context 是 BladeDISC 运行时的核心管理单元，其设计紧密关联<strong>宿主适配层、设备驱动层和内核层</strong>，通过分层抽象实现跨平台兼容与高效执行。针对不同宿主环境（如 TensorFlow、PyTorch 或独立二进制），RAL 提供差异化的 Context 实现。例如，TensorFlow Context 直接操作 <code>tf::Tensor</code> 和 GPU 设备句柄，而 PyTorch Context 适配 <code>torch::Tensor</code> 及 <code>c10::Stream</code>，二者均通过宿主适配层的 I&#x2F;O 绑定接口完成数据转换，确保输入输出与宿主环境原生数据结构无缝对接。Context 的生命周期与编译后的二进制模块一致，负责全局资源管理（如 GPU 内核预加载、内存池初始化），其内部封装设备驱动层的多环境实现（如内存分配在 TensorFlow 中调用 <code>GPUDevice::Allocate</code>，在 PyTorch 中则使用 <code>CUDACachingAllocator</code>），使得编译器生成的代码无需感知底层差异。</p>
<p>RAL Execution Context 作为 Context 的轻量化派生实例，专为单次执行设计，负责运行时动态状态的簿记管理。每次调用编译后的二进制时，从 Context 创建独立的 Execution Context，记录此次执行的临时资源（如输入张量指针、异步计算流标识符）。例如，在 TensorFlow 多线程场景中，多个线程可并发创建各自的 Execution Context，共享 Context 的全局资源（如预加载的 GPU 内核），但独立维护执行状态（如当前 CUDA Stream），避免资源竞争。Execution Context 的生命周期仅限于单次调用，执行结束后立即释放临时状态，从而减少内存占用并支持高并发。这一机制通过设备驱动层的环境适配接口（如 <code>ral_gpu_stream_sync</code>）确保内核执行与宿主环境的原生异步模型兼容，例如 PyTorch 的 JIT 执行通过 Execution Context 传递 <code>c10::cuda::CUDAStream</code>，使自定义内核可直接利用框架管理的计算流。</p>
<h2 id="PyTorch运行时系统"><a href="#PyTorch运行时系统" class="headerlink" title="PyTorch运行时系统"></a><font color = brown>PyTorch运行时系统</font></h2><p>重点参考<a href="https://zhuanlan.zhihu.com/p/633169168">Pytorch CUDA runtime</a>和<a href="https://archwalker.github.io/blog/2019/05/27/pytorch-internals.html">Pytorch Internal</a>。</p>
<h2 id="BladeDISC-RAL源码解读"><a href="#BladeDISC-RAL源码解读" class="headerlink" title="BladeDISC RAL源码解读"></a><font color = brown>BladeDISC RAL源码解读</font></h2><p>这一部分的复现源码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch_blade</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyCell</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyCell, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.c = torch.randn(<span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        t1 = x + y</span><br><span class="line">        t2 = torch.matmul(t1, <span class="variable language_">self</span>.c)</span><br><span class="line">        t3 = torch.<span class="built_in">sum</span>(t2)</span><br><span class="line">        <span class="keyword">return</span> t3 * t3</span><br><span class="line"></span><br><span class="line">my_cell = MyCell()</span><br><span class="line">x = torch.rand(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">y = torch.rand(<span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    blade_cell = torch_blade.optimize(my_cell, allow_tracing=<span class="literal">True</span>, model_inputs=(x, y))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(blade_cell(x, y))</span><br></pre></td></tr></table></figure>

<p>注意开启</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// -----// IR Dump After DiscLowerToLibraryCallPass (disc-lower-to-library-call) //----- //</span></span><br><span class="line">func.func @<span class="built_in">main</span>(%arg0: !disc_ral.context) attributes &#123;tf.entry_function = &#123;input_placements = <span class="string">&quot;gpu&quot;</span>, inputs = <span class="string">&quot;input.1_&quot;</span>, output_placements = <span class="string">&quot;gpu&quot;</span>, outputs = <span class="string">&quot;8&quot;</span>&#125;&#125; &#123;</span><br><span class="line">  %<span class="number">0</span> = llvm.mlir.<span class="built_in">constant</span>(<span class="number">0</span> : i32) : i32</span><br><span class="line">  %<span class="literal">false</span> = arith.constant <span class="literal">false</span></span><br><span class="line">  %<span class="literal">true</span> = arith.constant <span class="literal">true</span></span><br><span class="line">  %c1 = arith.constant <span class="number">1</span> : index</span><br><span class="line">  %c4 = arith.constant <span class="number">4</span> : index</span><br><span class="line">  %c10 = arith.constant <span class="number">10</span> : index</span><br><span class="line">  %c0 = arith.constant <span class="number">0</span> : index</span><br><span class="line">  %<span class="number">1</span> = <span class="string">&quot;disc_ral.dispatch&quot;</span>(%arg0, %c0) &#123;backend_config = <span class="string">&quot;&quot;</span>, call_target_name = <span class="string">&quot;ral_recv_input&quot;</span>, device = <span class="string">&quot;cpu&quot;</span>, has_side_effect = <span class="literal">false</span>&#125; : (!disc_ral.context, index) -&gt; memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %dim = memref.dim %<span class="number">1</span>, %c0 : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc = memref.<span class="built_in">alloc</span>() : memref&lt;<span class="number">10</span>x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  <span class="string">&quot;lmhlo.constant&quot;</span>(%alloc) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>, value = dense_resource&lt;__elided__&gt; : tensor&lt;<span class="number">10</span>x10xf32&gt;&#125; : (memref&lt;<span class="number">10</span>x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">  %alloc_0 = memref.<span class="built_in">alloc</span>() : memref&lt;<span class="number">10</span>x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  <span class="string">&quot;lmhlo.constant&quot;</span>(%alloc_0) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>, value = dense_resource&lt;__elided__&gt; : tensor&lt;<span class="number">10</span>x10xf32&gt;&#125; : (memref&lt;<span class="number">10</span>x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">  %alloc_1 = memref.<span class="built_in">alloc</span>() : memref&lt;<span class="number">10</span>xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  <span class="string">&quot;lmhlo.constant&quot;</span>(%alloc_1) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>, value = dense&lt;[<span class="number">0.186997086</span>, <span class="number">0.235856801</span>, <span class="number">0.217500299</span>, <span class="number">0.25940907</span>, <span class="number">0.109970599</span>, <span class="number">-0.152944937</span>, <span class="number">0.137896746</span>, <span class="number">-0.189537019</span>, <span class="number">0.256005555</span>, <span class="number">0.235299528</span>]&gt; : tensor&lt;<span class="number">10</span>xf32&gt;&#125; : (memref&lt;<span class="number">10</span>xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">  %alloc_2 = memref.<span class="built_in">alloc</span>() : memref&lt;<span class="number">10</span>xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  <span class="string">&quot;lmhlo.constant&quot;</span>(%alloc_2) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>, value = dense&lt;[<span class="number">0.281509697</span>, <span class="number">-0.0671350583</span>, <span class="number">-0.291665494</span>, <span class="number">0.300998032</span>, <span class="number">-0.304899603</span>, <span class="number">0.23629041</span>, <span class="number">-0.111676671</span>, <span class="number">0.304613203</span>, <span class="number">0.107744612</span>, <span class="number">-0.118951075</span>]&gt; : tensor&lt;<span class="number">10</span>xf32&gt;&#125; : (memref&lt;<span class="number">10</span>xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">  %<span class="keyword">reinterpret_cast</span> = memref.<span class="keyword">reinterpret_cast</span> %<span class="number">1</span> to offset: [<span class="number">0</span>], sizes: [%dim, <span class="number">10</span>], strides: [<span class="number">10</span>, <span class="number">1</span>] &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt; to memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_3 = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">2</span> = llvm.inttoptr %<span class="number">0</span> : i32 to !llvm.ptr&lt;i8&gt;</span><br><span class="line">  <span class="string">&quot;disc_ral.dispatch&quot;</span>(%arg0, %<span class="number">2</span>, %<span class="keyword">reinterpret_cast</span>, %alloc_0, %alloc_3, %<span class="literal">false</span>, %<span class="literal">false</span>, %<span class="literal">true</span>) &#123;backend_config = <span class="string">&quot;&quot;</span>, call_target_name = <span class="string">&quot;ral_gemm&quot;</span>, device = <span class="string">&quot;gpu&quot;</span>, has_side_effect = <span class="literal">false</span>&#125; : (!disc_ral.context, !llvm.ptr&lt;i8&gt;, memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;10x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, i1, i1, i1) -&gt; ()</span></span><br><span class="line">  memref.dealloc %alloc_0 : memref&lt;<span class="number">10</span>x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloca = memref.<span class="built_in">alloca</span>() &#123;alignment = <span class="number">64</span> : i64&#125; : memref&lt;<span class="number">2</span>xindex&gt;</span><br><span class="line">  memref.store %dim, %alloca[%c0] : memref&lt;<span class="number">2</span>xindex&gt;</span><br><span class="line">  memref.store %c10, %alloca[%c1] : memref&lt;<span class="number">2</span>xindex&gt;</span><br><span class="line">  %<span class="number">3</span> = arith.muli %dim, %c10 : index</span><br><span class="line">  %<span class="number">4</span> = arith.remui %<span class="number">3</span>, %c4 : index</span><br><span class="line">  %<span class="number">5</span> = arith.cmpi eq, %<span class="number">4</span>, %c0 : index</span><br><span class="line">  %alloc_4 = memref.<span class="built_in">alloc</span>() : memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_5 = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_6 = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_7 = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_8 = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  scf.<span class="keyword">if</span> %<span class="number">5</span> &#123;</span><br><span class="line">    <span class="string">&quot;lmhlo.fusion&quot;</span>() (&#123;</span><br><span class="line">      <span class="string">&quot;lmhlo.constant&quot;</span>(%alloc_4) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>, value = dense&lt;<span class="number">0.000000e+00</span>&gt; : tensor&lt;f32&gt;&#125; : (memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.dynamic_broadcast_in_dim&quot;</span>(%alloc_1, %alloca, %alloc_5) &#123;broadcast_dimensions = dense&lt;<span class="number">1</span>&gt; : tensor&lt;<span class="number">1</span>xi64&gt;, disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;<span class="number">10</span>xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;2xindex&gt;</span>, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.add&quot;</span>(%alloc_3, %alloc_5, %alloc_6) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.dynamic_broadcast_in_dim&quot;</span>(%alloc_4, %alloca, %alloc_8) &#123;broadcast_dimensions = dense&lt;&gt; : tensor&lt;<span class="number">0</span>xi64&gt;, disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;2xindex&gt;</span>, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.maximum&quot;</span>(%alloc_6, %alloc_8, %alloc_7) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.terminator&quot;</span>() : () -&gt; ()</span><br><span class="line">    &#125;) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>, disc.fusion.name = <span class="string">&quot;main_kLoop_maximum__5_1_0&quot;</span>, disc.fusion.tag = <span class="string">&quot;Vec4&quot;</span>, disc.fusion_type = <span class="string">&quot;kLoop&quot;</span>, disc_vectorize_or_tile_hint = <span class="number">4</span> : i32&#125; : () -&gt; ()</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="string">&quot;lmhlo.fusion&quot;</span>() (&#123;</span><br><span class="line">      <span class="string">&quot;lmhlo.constant&quot;</span>(%alloc_4) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>, value = dense&lt;<span class="number">0.000000e+00</span>&gt; : tensor&lt;f32&gt;&#125; : (memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.dynamic_broadcast_in_dim&quot;</span>(%alloc_1, %alloca, %alloc_5) &#123;broadcast_dimensions = dense&lt;<span class="number">1</span>&gt; : tensor&lt;<span class="number">1</span>xi64&gt;, disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;<span class="number">10</span>xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;2xindex&gt;</span>, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.add&quot;</span>(%alloc_3, %alloc_5, %alloc_6) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.dynamic_broadcast_in_dim&quot;</span>(%alloc_4, %alloca, %alloc_8) &#123;broadcast_dimensions = dense&lt;&gt; : tensor&lt;<span class="number">0</span>xi64&gt;, disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;2xindex&gt;</span>, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.maximum&quot;</span>(%alloc_6, %alloc_8, %alloc_7) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.terminator&quot;</span>() : () -&gt; ()</span><br><span class="line">    &#125;) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>, disc.fusion.name = <span class="string">&quot;main_kLoop_maximum__5_1_0&quot;</span>, disc.fusion_type = <span class="string">&quot;kLoop&quot;</span>, disc_vectorize_or_tile_hint = <span class="number">1</span> : i32&#125; : () -&gt; ()</span><br><span class="line">  &#125;</span><br><span class="line">  memref.dealloc %alloc_8 : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %alloc_6 : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %alloc_5 : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %alloc_4 : memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %alloc_3 : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %alloc_1 : memref&lt;<span class="number">10</span>xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_9 = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">6</span> = llvm.inttoptr %<span class="number">0</span> : i32 to !llvm.ptr&lt;i8&gt;</span><br><span class="line">  <span class="string">&quot;disc_ral.dispatch&quot;</span>(%arg0, %<span class="number">6</span>, %alloc_7, %alloc, %alloc_9, %<span class="literal">false</span>, %<span class="literal">false</span>, %<span class="literal">true</span>) &#123;backend_config = <span class="string">&quot;&quot;</span>, call_target_name = <span class="string">&quot;ral_gemm&quot;</span>, device = <span class="string">&quot;gpu&quot;</span>, has_side_effect = <span class="literal">false</span>&#125; : (!disc_ral.context, !llvm.ptr&lt;i8&gt;, memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;10x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, i1, i1, i1) -&gt; ()</span></span><br><span class="line">  memref.dealloc %alloc_7 : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %alloc : memref&lt;<span class="number">10</span>x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_10 = memref.<span class="built_in">alloc</span>() : memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_11 = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_12 = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_13 = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_14 = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  scf.<span class="keyword">if</span> %<span class="number">5</span> &#123;</span><br><span class="line">    <span class="string">&quot;lmhlo.fusion&quot;</span>() (&#123;</span><br><span class="line">      <span class="string">&quot;lmhlo.constant&quot;</span>(%alloc_10) &#123;value = dense&lt;<span class="number">0.000000e+00</span>&gt; : tensor&lt;f32&gt;&#125; : (memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.dynamic_broadcast_in_dim&quot;</span>(%alloc_10, %alloca, %alloc_11) &#123;broadcast_dimensions = dense&lt;&gt; : tensor&lt;<span class="number">0</span>xi64&gt;, disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;2xindex&gt;</span>, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.dynamic_broadcast_in_dim&quot;</span>(%alloc_2, %alloca, %alloc_12) &#123;broadcast_dimensions = dense&lt;<span class="number">1</span>&gt; : tensor&lt;<span class="number">1</span>xi64&gt;, disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;<span class="number">10</span>xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;2xindex&gt;</span>, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.add&quot;</span>(%alloc_9, %alloc_12, %alloc_13) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.maximum&quot;</span>(%alloc_13, %alloc_11, %alloc_14) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.terminator&quot;</span>() : () -&gt; ()</span><br><span class="line">    &#125;) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>, disc.fusion.name = <span class="string">&quot;main_kLoop_maximum__5_1_1&quot;</span>, disc.fusion.tag = <span class="string">&quot;Vec4&quot;</span>, disc.fusion_type = <span class="string">&quot;kLoop&quot;</span>, disc_vectorize_or_tile_hint = <span class="number">4</span> : i32&#125; : () -&gt; ()</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="string">&quot;lmhlo.fusion&quot;</span>() (&#123;</span><br><span class="line">      <span class="string">&quot;lmhlo.constant&quot;</span>(%alloc_10) &#123;value = dense&lt;<span class="number">0.000000e+00</span>&gt; : tensor&lt;f32&gt;&#125; : (memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.dynamic_broadcast_in_dim&quot;</span>(%alloc_10, %alloca, %alloc_11) &#123;broadcast_dimensions = dense&lt;&gt; : tensor&lt;<span class="number">0</span>xi64&gt;, disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;2xindex&gt;</span>, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.dynamic_broadcast_in_dim&quot;</span>(%alloc_2, %alloca, %alloc_12) &#123;broadcast_dimensions = dense&lt;<span class="number">1</span>&gt; : tensor&lt;<span class="number">1</span>xi64&gt;, disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;<span class="number">10</span>xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;2xindex&gt;</span>, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.add&quot;</span>(%alloc_9, %alloc_12, %alloc_13) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.maximum&quot;</span>(%alloc_13, %alloc_11, %alloc_14) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.terminator&quot;</span>() : () -&gt; ()</span><br><span class="line">    &#125;) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>, disc.fusion.name = <span class="string">&quot;main_kLoop_maximum__5_1_1&quot;</span>, disc.fusion_type = <span class="string">&quot;kLoop&quot;</span>, disc_vectorize_or_tile_hint = <span class="number">1</span> : i32&#125; : () -&gt; ()</span><br><span class="line">  &#125;</span><br><span class="line">  memref.dealloc %alloc_13 : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %alloc_12 : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %alloc_11 : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %alloc_10 : memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %alloc_9 : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %alloc_2 : memref&lt;<span class="number">10</span>xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  <span class="string">&quot;disc_ral.dispatch&quot;</span>(%arg0, %c0, %alloc_14) &#123;backend_config = <span class="string">&quot;&quot;</span>, call_target_name = <span class="string">&quot;ral_send_output&quot;</span>, device = <span class="string">&quot;cpu&quot;</span>, has_side_effect = <span class="literal">false</span>&#125; : (!disc_ral.context, index, memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">  <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>编译技术</category>
        <category>机器学习编译</category>
        <category>动态shape</category>
      </categories>
      <tags>
        <tag>机器学习编译器</tag>
        <tag>mlir</tag>
        <tag>动态shape</tag>
      </tags>
  </entry>
  <entry>
    <title>AI编译器后端&amp;运行时</title>
    <url>/2025/04/16/AI%E7%BC%96%E8%AF%91%E5%99%A8%E5%90%8E%E7%AB%AF-%E8%BF%90%E8%A1%8C%E6%97%B6/</url>
    <content><![CDATA[<p><img src="/images/image-20250420234716898.png" alt="image-20250420234716898"></p>
<span id="more"></span>

<blockquote>
<p>在尝试将pytorch接入aie平台的过程中，发现自己对于AI编译器在机器学习系统中的定位所知甚少。由于先前经验大多只停留在将高级语言转换成硬件相关的ir代码，而忽略了后端以及运行时系统协作方面的知识，遇到瓶颈。因此本博客通过参考机器学习系统书籍，以及目前主流AI编译器的后端和运行时系统的协作，查漏补缺，补全自己的知识盲点。</p>
</blockquote>
<h2 id="AI编译器后端和运行时架构"><a href="#AI编译器后端和运行时架构" class="headerlink" title="AI编译器后端和运行时架构"></a><font color = brown>AI编译器后端和运行时架构</font></h2><p>通过参考<a href="https://openmlsys.github.io/chapter_backend_and_runtime/index.html">mlsys open book</a>和<a href="https://zhuanlan.zhihu.com/p/651036523">tensorflow xla解读</a>，对机器学习编译有了新的理解。机器学习编译技术需要与运行时系统相互写作，才能将一个机器学习模型完整的跑在GPU&#x2F;NPU上。下图是AI系统的一个软件栈示意图：</p>
<img src="/images/image-20250418172339205.png" alt="image-20250418172339205" style="zoom: 80%;" />

<p>上述优化流程，是针对静态图的全图编译运行流程（tensorflow XLA这种动态圈图有一定区别）。按照软件栈层级关系，可以分为编译层和运行时系统层。</p>
<ul>
<li>编译层：编译层的输入是整个静态计算图或是部分子图，因此首先需要编译器前端用户代码进行解析翻译得到计算图IR，并对其进行设备信息无关的优化，此时的优化并不考虑程序执行的底层硬件信息。编译器后端的主要职责是对前端下发的IR做进一步的计算图优化，让其更加贴合硬件，并为IR中的计算节点选择在硬件上执行的算子，然后为每个算子的输入输出分配硬件内存，最终生成一个可以在硬件上执行的任务序列。</li>
<li>运行时系统层：运行时系统设计一般均有一个虚拟机VM用以支持调度和快速部署（IREE架构的VM和Relay VM）。其输入是编译层生成的可在硬件上执行的任务序列，<strong>如果考虑跨硬件部署，则需要考虑使用函数类型擦除技术统一接口</strong>。然后，VM将任务序列转换成硬件设备api和host端代码，最终分别生成可执行。</li>
</ul>
<p>后续会详细解读每一层的流程，以及可参考文献。</p>
<h3 id="编译器后端"><a href="#编译器后端" class="headerlink" title="编译器后端"></a><font color = green>编译器后端</font></h3><h4 id="后端优化定位"><a href="#后端优化定位" class="headerlink" title="后端优化定位"></a>后端优化定位</h4><p>重点参考<a href="https://www.bilibili.com/video/BV1K84y1x7Be?spm_id_from=333.788.videopod.sections&vd_source=7cc24e214309f4db17f1dda017fc6683">zomi编译后端视频</a>。声明：如下内容以及图片slides基本来源于该视频。</p>
<img src="/images/image-20250502160730282.png" alt="image-20250502160730282" style="zoom:50%;" />

<p>上图是编译后端在整个AI编译框架的位置。大致总结一下前端优化和后端优化的区别：前端优化输入是计算图，关注计算图整体拓扑结构，而不关心算子的具体实现。其优化手段主要有对于算子节点进行融合、消除、化简等操作，是计算图计算和存储开销小。后端优化则专注于算子节点的内部具体实现，针对具体实现使得性能达到最优，重点关心节点的输入，输出内存循环方式和计算的逻辑。</p>
<h4 id="后端优化干了什么？"><a href="#后端优化干了什么？" class="headerlink" title="后端优化干了什么？"></a>后端优化干了什么？</h4><p><img src="/images/image-20250502173731704.png" alt="image-20250502173731704"></p>
<p>后端优化主要做了三件事：</p>
<ul>
<li><p>低级代码生成</p>
<p>如上图所示，是TVM的架构流程。Tensor IR是将计算图用张量表示。低级代码生成阶段是将该张量表示进一步降级。</p>
</li>
<li><p>后端优化</p>
<p>底层硬件不同，对于conv等算子的排布优化也会不同。后端优化主要针对各个算子做循环优化排布。</p>
</li>
<li><p>代码生成</p>
<p>针对不同backend生成机器码。在AI编译器中，以GPU为例，一种思路是GPU端直接生成cuda代码，CPU端生成LLVM，并由llvm基础设施编译可执行。另一个思路是生成NVVM IR，交由nvvm进行编译。</p>
</li>
</ul>
<p>所以，后端优化其实本质就是对于算子的编译优化。目前算子编译优化的挑战如下：</p>
<p><img src="/images/image-20250502174157055.png" alt="image-20250502174157055"></p>
<p>算子库因此应允而生：</p>
<p><img src="/images/image-20250502174216690.png" alt="image-20250502174216690"></p>
<p>算子库能够很好地压榨硬件性能，但是上层模型的变动和底层硬件的变动，算子库无法快速适配这些变化。因此AI编译器和算子库协作开发，由AI编译器提供灵活性，减少一部分算子库开发的人力投入。编译角度的后端优化可以分为两个路线：</p>
<ul>
<li>使用机器学习等模型进行auto tuning</li>
<li>利用数学表达式，做多面体优化</li>
</ul>
<p>这两个部分后续会专门开专题来解读。</p>
<h3 id="运行时系统"><a href="#运行时系统" class="headerlink" title="运行时系统"></a><font color = green>运行时系统</font></h3><p>这一章节关于运行时系统的描述，主要参考<a href="https://www.lei.chat/posts/single-node-ml-runtime-foundation/">Lei chat blog</a>，本章的描述均基于单节点来讨论，服务器等多节点暂不涉及。</p>
<p>首先，我们需要明确，什么是机器学习系统的runtime system。runtime system的主要工作有两点：</p>
<ul>
<li>管理资源（内存资源等）</li>
<li>调度执行</li>
</ul>
<p>根据实际语义，runtime亦有不同。多卡分布式训练场景下，runtime负责将训练任务分配调度到数据中心的不同节点。单节点上，runtime负责派发张量计算到特定的加速器。<strong>本章节关注单节点runtime系统，其一般运行在CPU上来管理资源并调度机器学习任务负载到一个或多个加速器上。</strong></p>
<h4 id="运行时抽象"><a href="#运行时抽象" class="headerlink" title="运行时抽象"></a>运行时抽象</h4><p>运行时系统给用户提供了三个抽象：</p>
<ul>
<li><p>主机设备分离</p>
<blockquote>
<p>摘自博客的解释：</p>
<p><img src="/images/image-20250421103410716.png" alt="image-20250421103410716"></p>
<p><img src="/images/image-20250421103520060.png" alt="image-20250421103520060"></p>
</blockquote>
</li>
<li><p>虚拟机抽象</p>
<p>这一部分是我们理解整个runtime的重点，具体的实际例子可以参考<a href="https://github.com/iree-org/iree">IREE </a>和<a href="https://zhuanlan.zhihu.com/p/504066888">tvm runtime</a>设计细节。针对机器学习系统以及我们的cpu加速器协同架构，我们会有如下insights：</p>
<ul>
<li>首先我们观察主机设备架构，会发现相比设备端，主机端的系统架构是比较统一的（ARM64， x86_64和RISC-V）。</li>
<li>观察机器学习的任务，发现机器学习任务一般是<a href="https://llvm.org/doxygen/classllvm_1_1LoopNest.html">完美循环</a>（定义见llvm官方文档）。因此<strong>访存方面用逻辑偏移即可，而非物理指针</strong>。</li>
<li>机器学习模型部署在端侧，因此轻量化也是一大考量。</li>
</ul>
<p>基于上述insights，主流ai编译器均定义一套主机端虚拟指令集和虚拟机来实现调度。计算密集任务放在加速器上，存储资源追踪管理以及维度形状计算等逻辑（适合cpu端计算的逻辑）放在虚拟机上。如下图所示是IREE的虚拟机架构图：</p>
<p><img src="/images/image-20250421105323107.png" alt="image-20250421105323107"></p>
<p>上述虚拟机设计，还有一大好处，那就是可以很自然的完成从mlir系统到虚拟机指令ISA的编译转换。因此整个调度控制流程可以自然地通过编译器来控制，进一步提供可优化点。</p>
</li>
<li><p>HAL硬件抽象层</p>
<p>针对边缘&#x2F;客户端设备的多样化硬件生态，最佳实践是基于现代图形&#x2F;计算API（如Vulkan、DirectX 12、Metal）构建硬件抽象层（HAL）。这些API提供低开销、跨平台支持、显式控制等优势，能统一管理不同厂商的GPU&#x2F;加速器（如NVIDIA&#x2F;AMD&#x2F;Apple）。虽然这些API直接使用复杂（如代码冗长），但通过编译器自动生成调用逻辑，可绕过开发难度，同时保留底层控制权。这种设计既能满足高性能需求（通过低层级优化），又能覆盖多平台&#x2F;硬件的兼容性（如Windows&#x2F;iOS&#x2F;Android），为碎片化设备提供统一且高效的计算支持。如下是iree的HAL，VM架构图。</p>
<p><img src="/images/image-20250421110831028.png" alt="image-20250421110831028"></p>
</li>
</ul>
<p>上述是runtime system的三个抽象，而实际生产中，runtime系统也要充分考虑性能问题：</p>
<blockquote>
<p>在GPU资源管理与同步中，核心挑战在于平衡计算吞吐量与系统整体效率。由于GPU内存性能常落后于算力提升，单纯优化单内核代码（如矩阵乘法）难以发挥理论峰值，需避免运行时因频繁创建&#x2F;销毁资源（如缓冲区、同步对象）或跨组件数据拷贝&#x2F;强制等待导致的性能损耗。为此，可借鉴图形应用（如游戏引擎）的成熟实践：<strong>预分配资源池</strong>减少动态开销、<strong>预录异步命令链</strong>最大化GPU并行度、<strong>细粒度同步</strong>基于资源生命周期。这些策略可通过编译器自动化实现（如依赖分析与指令提升）。IREE运行时采用<strong>异步优先</strong>设计，支持跨组件无缝传递缓冲区与同步原语，利用操作系统底层机制（如Linux的dma_fence）避免硬性边界，既保障ML模型的高效执行，又能嵌入复杂应用（如图像处理管线）而不成为性能瓶颈，兼顾性能与系统协同。</p>
</blockquote>
<p>上述关于运行时系统的描述，是比较普遍的，下面结合Open MLsys一书，详细解读不同运行时系统的分类。</p>
<blockquote>
<p>经过算子选择与内存分配之后，计算任务可以通过运行时完成计算的调度与在硬件上的执行。根据是否将算子编译为计算图，计算的调度可以分为单算子调度与计算图调度两种方式，例如在MindSpore中分别提供了PyNative模式和Graph模式。而根据硬件提供的能力差异，计算图的执行方式又可以分为逐算子下发执行的交互式执行以及将整个计算图或者部分子图一次性下发到硬件的下沉式执行两种模式。                                                                                                                                                                      —- 《Open Mlysy》</p>
</blockquote>
<h4 id="单算子调度"><a href="#单算子调度" class="headerlink" title="单算子调度"></a>单算子调度</h4><p>一行行算子line by line的交由python运行时执行，好处是方便debug和看到即时效果，坏处是只能顺序执行，丧失很多并行性，同时也无法支持编译优化。</p>
<h4 id="计算图调度"><a href="#计算图调度" class="headerlink" title="计算图调度"></a>计算图调度</h4><p>在一个典型的异构计算环境中，主要存在CPU、GPU以及NPU等多种计算设备，因此一张计算图可以由运行在不同设备上的算子组成为异构计算图。</p>
<p><img src="/images/image-20250418182641748.png" alt="image-20250418182641748"></p>
<p>所述计算图由如下几类异构硬件对应的算子组成：</p>
<ul>
<li><strong>CPU算子</strong>：由C++语言编写实现并在主机上通过CPU执行的算子，CPU计算的性能取决于是否能够充分利用CPU多核心的计算能力。</li>
<li><strong>GPU算子</strong>：以英伟达GPU芯片为例，通过在主机侧将GPU Kernel逐个下发到GPU设备上，由GPU芯片执行算子的计算逻辑，由于芯片上具备大量的并行执行单元，可以为高度并行的算法提供强大的加速能力。</li>
<li><strong>NPU算子</strong>：以华为Ascend芯片为例， Ascend是一个高度集成的SoC芯片，NPU的优势是支持将部分或整个计算图下沉到芯片中完成计算，计算过程中不与Host发生交互，因此具备较高的计算性能。</li>
<li><strong>Python算子</strong>：在执行模式上与CPU算子类似，都是由主机上的CPU执行计算，区别在于计算逻辑是由Python语言的运行时通过Python解释器解释执行。</li>
</ul>
<p>一般主流架构均提供了指定算子所在运行设备的能力，此处可以参考mindspore的具体实现。</p>
<p>完成计算图中算子对应设备的标记以后，计算图已经准备好被调度与执行，根据硬件能力的差异，可以将异构计算图的执行分为三种模式，分别是逐算子交互式执行，整图下沉执行与子图下沉执行。</p>
<ul>
<li>交互式执行主要针对CPU和GPU的场景，计算图中的算子按照输入和输出的依赖关系被逐个调度与执行；</li>
<li>而整图下沉执行模式主要是针对NPU芯片而言，这类芯片主要的优势是能够将整个神经网络的计算图一次性下发到设备上，无需借助主机的CPU能力而独立完成计算图中所有算子的调度与执行，减少了主机和芯片的交互次数，借助NPU的张量加速能力，提高了计算效率和性能；</li>
<li>子图下沉执行模式是前面两种执行模式的结合，由于计算图自身表达的灵活性，对于复杂场景的计算图在NPU芯片上进行整图下沉执行的效率不一定能达到最优，因此可以将对于NPU芯片执行效率低下的部分分离出来，交给CPU或者GPU等执行效率更高的设备处理，而将部分更适合NPU计算的子图下沉到NPU进行计算，这样可以兼顾性能和灵活性两方面。</li>
</ul>
<blockquote>
<p>虽然在计算图上可以充分表达算子间的并发关系，在实际代码中会产生由于并发而引起的一些不预期的副作用场景</p>
<img src="/images/image-20250418183157655.png" alt="image-20250418183157655" style="zoom: 50%;" />

<p>如上图所示，虚线是operator之间的隐含依赖关系，但是在计算图中难以表示。因此在实际执行计算图的过程中，会分为交互式下发和下沉式（执行方式区分）。</p>
</blockquote>
<h4 id="算子交互执行"><a href="#算子交互执行" class="headerlink" title="算子交互执行"></a>算子交互执行</h4><p>框架的运行时根据计算图中算子的依赖关系，按照某种执行序（例如广度优先序）逐个将算子下发到硬件上执行。我们处理的计算图一般为异构计算图（计算图中的硬件有多种）。解读异构计算图交互执行之前，先解读同构计算图，异构计算图需要先通过子图切分为同构计算图。</p>
<p>同构计算图有两种执行方式，如下表所示：</p>
<table>
<thead>
<tr>
<th align="right">执行方式</th>
<th align="right">串行执行</th>
<th align="right">并行执行</th>
</tr>
</thead>
<tbody><tr>
<td align="right">算子执行顺序</td>
<td align="right">固定</td>
<td align="right">不固定</td>
</tr>
<tr>
<td align="right">算子执行线程</td>
<td align="right">单线程</td>
<td align="right">多线程</td>
</tr>
<tr>
<td align="right">所需执行资源</td>
<td align="right">较低</td>
<td align="right">较高</td>
</tr>
</tbody></table>
<p>而对于异构计算图，<font color = red>一般来说计算图的优化都是基于非异构计算图来实现的，要求计算图中的算子为同一设备上的，方便算子间的融合替换等优化操作，因此需要将一张异构计算图切分为多个非异构计算图，这里切分就比较灵活了，可以定义各种切分规则，一般按照产生尽量少的子图的切分规则来切分，尽量将多的同一设备上的算子放在一张子图中。</font>如下图是一种可能的子图切分方式：</p>
<img src="/images/image-20250418185356799.png" alt="image-20250418185356799" style="zoom:50%;" />

<p>切分出子图后，下一步需要考虑的是子图执行。共可以分为两种策略：</p>
<ul>
<li><p><strong>子图拆分执行</strong>：将切分后的多个子图分开执行，即一个子图执行完再执行另一个子图，上一个子图的输出数据会传输给下一个子图的输入数据，并且下一个子图需要将输入数据拷贝为本图的device数据，如Graph_2_GPU需要将Graph_1_CPU的输出数据从CPU拷贝到GPU，反过来Graph_3_CPU需要将Graph2GPU的输出数据从GPU拷贝到CPU，子图之间互相切换执行有一定的开销。</p>
<img src="/images/image-20250418185513957.png" alt="image-20250418185513957" style="zoom: 50%;" />
</li>
<li><p><strong>子图合并执行</strong>：将切分后的多个子图进行合并，合并为一个整体的DAG执行，如 <a href="https://openmlsys.github.io/chapter_backend_and_runtime/compute_schedule_and_execute.html#graph-exec-7">图7.5.12</a>所示，通过算子的设备属性来插入拷贝算子以实现不同设备上的算子数据传输，并且拷贝算子也是进入整图中的，从而形成一个大的整图执行，减少子图之间的切换执行开销。</p>
<img src="/images/image-20250418185549176.png" alt="image-20250418185549176" style="zoom:50%;" /></li>
</ul>
<p>下表中对于这两种执行方式进行了对比：</p>
<table>
<thead>
<tr>
<th align="right">执行方式</th>
<th align="right">子图拆分</th>
<th align="right">子图合并</th>
</tr>
</thead>
<tbody><tr>
<td align="right">异构数据传输</td>
<td align="right">子图之间拷贝</td>
<td align="right">算子之间拷贝</td>
</tr>
<tr>
<td align="right">执行额外开销</td>
<td align="right">子图切换执行开销</td>
<td align="right">无</td>
</tr>
<tr>
<td align="right">执行并发粒度</td>
<td align="right">子图并发</td>
<td align="right">算子原生并发</td>
</tr>
</tbody></table>
<p>总结一下，主要是并发粒度的区别。<strong>子图切换执行开销是比较大的，因此子图合并是比较普适的做法。</strong></p>
<blockquote>
<p>mindspore的异构执行是子图合并，而非异构执行是并行执行，因此为子图合并并行执行（任意计算架构都有两个象限）。</p>
</blockquote>
<h4 id="下沉式执行"><a href="#下沉式执行" class="headerlink" title="下沉式执行"></a>下沉式执行</h4><p>下沉式执行是通过专用芯片的SoC架构，将整个或部分计算图一次性调度到芯片上以完成全量数据的计算。</p>
<h2 id="计算图编译器"><a href="#计算图编译器" class="headerlink" title="计算图编译器"></a><font color = brown>计算图编译器</font></h2><blockquote>
<p>先前一章节阐述的是目前主流成熟的AI编译器后端和运行时架构，而面向FPGA或是AIE这样的可重构硬件，往往无法直接复用ASIC NPU的编译流程（缺乏成熟的runtime系统）。本章节结合Efficient processing for DNN的chapter6，讲解如何将一个dataflow graph映射到这些硬件上去。</p>
</blockquote>
<p><img src="/images/image-20250419141303976.png" alt="image-20250419141303976"></p>
<p>上图摘自cornell的机器学习硬件课程slides。在众多参考资料中，将dataflow映射到FPGA或是特定NPU（如AIE），被称为Mapping，而不是Compiler。如下引用Efficient processing for DNN一书中的一段话，讲明两者的类比关系：</p>
<blockquote>
<p>In conventional computer systems, the compiler translates the program into machine-readable binary codes for execution; in the processing of DNNs, the mapper translates the desired DNN layer computation (i.e., problem specification) along with its shape and size4 into a hardwarecompatible mapping for execution. While the compiler usually optimizes just for performance, the mapper will typically optimize for performance and&#x2F;or energy efficiency.</p>
<img src="/images/image-20250419154151487.png" alt="image-20250419154151487" style="zoom:50%;" />

<img src="/images/image-20250419154218830.png" alt="image-20250419154218830" style="zoom:50%;" />

<p>总结一下，有如下异同点：</p>
<ul>
<li>输入方面，传统编译是程序，而DNN加速器映射是问题特性以及输入的shape。</li>
<li>Compiler的结果是可执行程序，而mapper是硬件的配置文件。</li>
<li>需要的硬件信息方面，编译器需要知道处理器的架构和微架构，而mapper需要知道DNN加速器的数据流和约束。</li>
</ul>
</blockquote>
<p><img src="/images/image-20250419141926408.png" alt="image-20250419141926408"></p>
<p>上图是计算图编译器的一个典型架构。</p>
<p><img src="/images/image-20250419143546984.png" alt="image-20250419143546984"></p>
<h2 id="典型编译器解读"><a href="#典型编译器解读" class="headerlink" title="典型编译器解读"></a><font color = brown>典型编译器解读</font></h2><h3 id="TVM-后端架构-Runtime"><a href="#TVM-后端架构-Runtime" class="headerlink" title="TVM 后端架构&amp;Runtime"></a><font color = green>TVM 后端架构&amp;Runtime</font></h3><h4 id="运行时框架图"><a href="#运行时框架图" class="headerlink" title="运行时框架图"></a>运行时框架图</h4><p>TVM编译器的一大优势是支持多硬件平台&#x2F;设备部署，因此其运行时系统设计是重中之重。其运行时系统可以大体拆解为两个部分：</p>
<ul>
<li>每个硬件继承自ModuleNode类，利用<a href="https://zhuanlan.zhihu.com/p/363991566">PackedFunc技术</a>将特定device的设备API封装成统一格式的指令（类似bladeDISC等编译器的类型擦除设计）。</li>
<li>一个Executor调度每一个指令并运行。该Executor分为GraphExecutor（针对静态图）和Relay VM（针对动态图）。</li>
</ul>
<p>如下图所示分别是TVM运行时框架和Relay VM示意图：</p>
<p><img src="/images/image-20250416230058925.png" alt="image-20250416230058925"></p>
<img src="/images/image-20250416231046705.png" alt="image-20250416231046705" style="zoom: 67%;" />

<h3 id="IREE-runtime设计"><a href="#IREE-runtime设计" class="headerlink" title="IREE runtime设计"></a><font color = green>IREE runtime设计</font></h3><p>这部分内容主要参考<a href="https://drive.google.com/file/d/1ikgOdZxnMz1ExqwrAiuTY9exbe3yMWbB/view">IREE runtime 设计slides</a>。IREE项目整体的设计思路十分完整庞大，设计巧妙，需要单独开一章节讲解。</p>
<h3 id="BladeDISC后端架构-Runtime"><a href="#BladeDISC后端架构-Runtime" class="headerlink" title="BladeDISC后端架构&amp;Runtime"></a><font color = green>BladeDISC后端架构&amp;Runtime</font></h3><h4 id="BladeDISC的key-feature"><a href="#BladeDISC的key-feature" class="headerlink" title="BladeDISC的key feature"></a>BladeDISC的key feature</h4><blockquote>
<p><img src="/images/image-20250427220408720.png" alt="image-20250427220408720"></p>
</blockquote>
<p>BladeDISC项目在编译后端和runtime上，有许多巧思（动态shape引发的编译和运行时协作代码生成）。具体地参考<a href="https://dl.acm.org/doi/10.1145/3617327">BladeDISC papaer</a>和<a href="https://github.com/alibaba/BladeDISC">BladeDISC github project</a>，以及<a href="https://bladedisc.oss-cn-hangzhou.aliyuncs.com/docs/bladedisc-intro-for-intel.pdf">BladeDISC内部分享</a>。</p>
<h4 id="后端代码生成"><a href="#后端代码生成" class="headerlink" title="后端代码生成"></a>后端代码生成</h4><blockquote>
<p>BladeDISC是tensorflow的backend来使用，因此其代码生成是针对融合后的算子做代码生成即可。运行时可以依赖于tensorflow的runtime调度器，launch 代码生成的kernel。</p>
</blockquote>
<p>在后端代码生成过程中，由于动态shape问题，主要有如下考虑：</p>
<ul>
<li><p>针对存储密集子图，考虑指令重排和重叠来减少存储overhead。由于shape是动态的，传统的unroll操作可以抵消动态shape的部分影响。</p>
</li>
<li><p>针对存储密集子图，<strong>多代码生成</strong>和<strong>运行时预测机制</strong>，来确保针对特定shape选择到最合适的生成代码。</p>
<p><img src="/images/image-20250427212344275.png" alt="image-20250427212344275"></p>
<p>如上图所示，CPU端在做GPU kernel launch的时候，由于需要运行时speculation，所以会有一定延迟（microsecond），该时间GPU kernel可以overlap掉。多代码生成会考虑如下三个因素：</p>
<ol>
<li>向量化版本 or 非向量化版本</li>
<li>隐藏broadcast删除 or 不删除</li>
<li>针对行归约操作，one block one row（行少列多，高并行，低tail latency） 或是 one warp one row（行多列少，低调度开销，高吞吐）的选择</li>
</ol>
</li>
<li><p>针对计算密集子图，有如下insight：</p>
<blockquote>
<p>计算密集操作后续一般紧跟elementwise操作，称elementwise为epilog。计算密集算子计算量相对更大，内存获取也更多，因此针对这种组合，主要针对计算密集做代码生成策略选择，epilog沿用即可。</p>
</blockquote>
<p>计算密集算子的<strong>bottleneck</strong>是动态shape下，不同shape的算子编写性能区别很大，因此编译时做面向不同类型shape多代码生成，运行时预测选择。其流程如图所示：</p>
<p><img src="/images/image-20250427214740408.png" alt="image-20250427214740408"></p>
</li>
</ul>
<p>​	可以看到，BladeDISC基于CUTLASS构建数据集，训练一个决策树模型，运行时决策出选用哪个生成代码。</p>
<h4 id="Runtime设计：RAL"><a href="#Runtime设计：RAL" class="headerlink" title="Runtime设计：RAL"></a>Runtime设计：RAL</h4><p><img src="/images/image-20250427215219613.png" alt="image-20250427215219613"></p>
<p>上图是RAL的完整设计，有如下特点：</p>
<ul>
<li>有别于其他AI编译器（TVM和IREE），BladeDISC通过整体代码生成（张量计算生成device代码，运行调度生成主机host代码），减少虚拟机的解释执行开销，同时也增加了编译和运行时协同优化可能。</li>
<li>前端，tensorflow和pytorch都有对应的IO来接入。后端，每个设备都有抽象，能够支持内存管理，任务调度同步等api。</li>
<li>不论是主机端还是设备端，最后都依靠llvm来生成对应架构的二进制文件。</li>
</ul>
<p>根据阿里PAI团队的slides做进一步理解：</p>
<blockquote>
<p><img src="/images/image-20250427220909637.png" alt="image-20250427220909637"></p>
</blockquote>
<blockquote>
<p><img src="/images/image-20250427221352121.png" alt="image-20250427221352121"></p>
</blockquote>
<p>上述slides阐述了如何对于编译生成的.exe做runtime，后续会结合代码进一步挖掘。</p>
<h4 id="整体流程"><a href="#整体流程" class="headerlink" title="整体流程"></a>整体流程</h4><p><img src="/images/image-20250427221149589.png" alt="image-20250427221149589"></p>
<p>可以清晰看到，分为独立模式和插件模式（深度集成Tensorflow和pytorch框架）。</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a><font color = brown>References</font></h2><ol>
<li><a href="https://www.iteye.com/blog/rednaxelafx-492667">虚拟机技术解读</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/504066888">TVM runtime系统讲解</a></li>
<li><a href="https://abdelfattah-class.github.io/ece5545/">Cornell: Machine learning hardware and system课程</a></li>
<li><a href="https://drive.google.com/file/d/1ikgOdZxnMz1ExqwrAiuTY9exbe3yMWbB/view">IREE运行时系统设计</a></li>
</ol>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>机器学习编译</category>
        <category>后端</category>
        <category>运行时</category>
      </categories>
      <tags>
        <tag>机器学习编译器</tag>
        <tag>后端</tag>
        <tag>运行时</tag>
      </tags>
  </entry>
  <entry>
    <title>BladeDISC初探</title>
    <url>/2025/04/01/BladeDISC%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<p><img src="/images/image-20250401195537142.png" alt="image-20250401195537142"></p>
<span id="more"></span>

<blockquote>
<p>在大模型训练推理场景中，一个十分大的瓶颈是动态shape问题。比如nlp领域，处理的句子长短不一，tensor的shape是动态变化的，到runtime才能确定。这给机器学习编译器带来很大的困扰，以XLA为首的sota编译器均是静态shape的，在性能上会有一定损失。BladeDISC是阿里提出的针对动态shape的机器学习编译器，并且经过大量实验和实际生产检验。本文重点关注BladeDISC的构建，pytorch使用方式以及基础架构解读。后续文章会讲解优化流程和论文解读。</p>
</blockquote>
<h2 id="源码构建"><a href="#源码构建" class="headerlink" title="源码构建"></a><font color = brown>源码构建</font></h2><h4 id="Build-from-source"><a href="#Build-from-source" class="headerlink" title="Build from source"></a>Build from source</h4><ul>
<li><p>下载<a href="https://hub.docker.com/r/bladedisc/bladedisc/tags?page=1&name=devel">BladeDisc</a>镜像</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker pull bladedisc/bladedisc:latest-devel-cu118</span><br></pre></td></tr></table></figure>

<ul>
<li>使用cu118版本</li>
</ul>
</li>
<li><p>运行该镜像</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker run --gpus all -it -v $PWD:/disc bladedisc/bladedisc:latest-devel-cu118 bash</span><br></pre></td></tr></table></figure>
</li>
<li><p>修改一下<code>pytorch_blade/scripts/build_pytorch_blade.sh</code>里面的<code>TORCH_BLADE_CI_BUILD_TORCH_VERSION</code>。修改为存在的<code>requirements.txt</code>即可。</p>
<blockquote>
<p>构建过程中，onnx由于带宽等问题，可能会报error，添加-i <a href="https://pypi.tuna.tsinghua.edu.cn/simple%E6%8C%87%E5%AE%9Apypi%E9%95%9C%E5%83%8F%E5%8D%B3%E5%8F%AF%E3%80%82">https://pypi.tuna.tsinghua.edu.cn/simple指定pypi镜像即可。</a></p>
</blockquote>
</li>
<li><p>pytorch版本构建</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd pytorch_blade &amp;&amp; bash ./scripts/build_pytorch_blade.sh</span><br><span class="line">python setup.py bdist_wheel</span><br><span class="line">pip install ./pytorch_blade/dist/torch_blade-0.2.0+2.0.1.cu118-cp38-cp38-linux_x86_64.whl</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a><font color = green>错误处理</font></h3><p>如果报错没有安全git，在docker中用：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git config --global --add safe.directory /disc</span><br></pre></td></tr></table></figure>

<h4 id="quick-install"><a href="#quick-install" class="headerlink" title="quick install"></a>quick install</h4><p>参考<a href="https://github.com/alibaba/BladeDISC/blob/main/docs/install_with_docker.md">docker install</a></p>
<h2 id="Pytorch部署BERT模型"><a href="#Pytorch部署BERT模型" class="headerlink" title="Pytorch部署BERT模型"></a><font color = brown>Pytorch部署BERT模型</font></h2><h3 id="Hugging-Face模型下载"><a href="#Hugging-Face模型下载" class="headerlink" title="Hugging Face模型下载"></a><font color = green>Hugging Face模型下载</font></h3><ul>
<li><p>手动下载模型（适合服务器联网不稳定的情况使用）</p>
<p>找到<a href="https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment/tree/main">Bert sentiment inference 模型</a>，主要手动下载如下几个文件：</p>
<p><img src="/images/image-20250306203839463.png" alt="image-20250306203839463"></p>
<p>在python代码中使用离线下载的模型：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model_path = <span class="string">&quot;./model&quot;</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_path)</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(model_path).cuda().<span class="built_in">eval</span>()</span><br></pre></td></tr></table></figure>
</li>
<li><p>直接通过transformers 包下载，该下载方式通过huggingface对应模型网页的<code>use this model</code>获取</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Load model directly</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>)</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(<span class="string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>确保环境有transformers包即可</p>
</li>
<li><p>通过<code>huggingface-cli</code>下载</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">huggingface-cli download nlptown/bert-base-multilingual-uncased-sentiment</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="做BERT-Inference的testbench"><a href="#做BERT-Inference的testbench" class="headerlink" title="做BERT Inference的testbench"></a><font color = green>做BERT Inference的testbench</font></h3><p>我的测试codes如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch_blade</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> (</span><br><span class="line">    pipeline,</span><br><span class="line">    AutoTokenizer,</span><br><span class="line">    AutoModelForSequenceClassification,</span><br><span class="line">    TextClassificationPipeline,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">############################################# download model from huggingface #############################################</span></span><br><span class="line">model_path = <span class="string">&quot;./model&quot;</span></span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_path)</span><br><span class="line"></span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(model_path).cuda().<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plain_tokenizer</span>(<span class="params">inputs_str, return_tensors</span>):</span><br><span class="line">    inputs = tokenizer(inputs_str, return_tensors=return_tensors, padding=<span class="literal">True</span>)</span><br><span class="line">    inputs = &#123;key: value.cuda() <span class="keyword">for</span> key, value <span class="keyword">in</span> inputs.items()&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># torch_blade.optimize 不支持 None 作为输入</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;token_type_ids&quot;</span> <span class="keyword">in</span> inputs <span class="keyword">and</span> inputs[<span class="string">&quot;token_type_ids&quot;</span>] <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">del</span> inputs[<span class="string">&quot;token_type_ids&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (inputs[<span class="string">&#x27;input_ids&#x27;</span>], inputs[<span class="string">&#x27;attention_mask&#x27;</span>], inputs.get(<span class="string">&#x27;token_type_ids&#x27;</span>, <span class="literal">None</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PlainTextClassificationPipeline</span>(<span class="title class_ inherited__">TextClassificationPipeline</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_forward</span>(<span class="params">self, model_inputs</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.model(*model_inputs)</span><br><span class="line"></span><br><span class="line">classifier = pipeline(</span><br><span class="line">    <span class="string">&#x27;sentiment-analysis&#x27;</span>,</span><br><span class="line">    model=model,</span><br><span class="line">    tokenizer=plain_tokenizer,</span><br><span class="line">    pipeline_class=PlainTextClassificationPipeline,</span><br><span class="line">    device=<span class="number">0</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">input_strs = [</span><br><span class="line">    <span class="string">&quot;We are very happy to show you the story.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;We hope you don&#x27;t hate it.&quot;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">results = classifier(input_strs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> inp_str, result <span class="keyword">in</span> <span class="built_in">zip</span>(input_strs, results):</span><br><span class="line">    <span class="built_in">print</span>(inp_str)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot; label: <span class="subst">&#123;result[<span class="string">&#x27;label&#x27;</span>]&#125;</span>, with a score: <span class="subst">&#123;<span class="built_in">round</span>(result[<span class="string">&#x27;score&#x27;</span>], <span class="number">4</span>)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">############################################# Use BladeDISC for optimization #############################################</span></span><br><span class="line">inputs_str = <span class="string">&quot;Hey, the cat is cute.&quot;</span></span><br><span class="line">inputs = plain_tokenizer(inputs_str, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"></span><br><span class="line">torch_config = torch_blade.config.Config()</span><br><span class="line">torch_config.enable_mlir_amp = <span class="literal">False</span>  <span class="comment"># disable mix-precision</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Ensure inputs are properly formatted for optimization</span></span><br><span class="line">model_inputs = <span class="built_in">tuple</span>(i <span class="keyword">for</span> i <span class="keyword">in</span> inputs <span class="keyword">if</span> i <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad(), torch_config:</span><br><span class="line">    optimized_ts = torch_blade.optimize(model, allow_tracing=<span class="literal">True</span>, model_inputs=model_inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Move optimized model to CUDA</span></span><br><span class="line">optimized_ts = optimized_ts.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save the optimized TorchScript model</span></span><br><span class="line">torch.jit.save(optimized_ts, <span class="string">&quot;opt.disc.pt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">############################################# testbench #############################################</span></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">benchmark</span>(<span class="params">model, inputs, num_iters=<span class="number">1000</span></span>):</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        model(*inputs)</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line"></span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        model(*inputs)</span><br><span class="line">    torch.cuda.synchronize()</span><br><span class="line">    end = time.time()</span><br><span class="line">    <span class="keyword">return</span> (end - start) / num_iters * <span class="number">1000.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bench_and_report</span>(<span class="params">input_strs</span>):</span><br><span class="line">    inputs = plain_tokenizer(input_strs, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line">    model_inputs = <span class="built_in">tuple</span>(i <span class="keyword">for</span> i <span class="keyword">in</span> inputs <span class="keyword">if</span> i <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">    avg_latency_baseline = benchmark(model, model_inputs)</span><br><span class="line">    avg_latency_bladedisc = benchmark(optimized_ts, model_inputs)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Seqlen: <span class="subst">&#123;[<span class="built_in">len</span>(s) <span class="keyword">for</span> s <span class="keyword">in</span> input_strs]&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Baseline: <span class="subst">&#123;avg_latency_baseline:<span class="number">.4</span>f&#125;</span> ms&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;BladeDISC: <span class="subst">&#123;avg_latency_bladedisc:<span class="number">.4</span>f&#125;</span> ms&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;BladeDISC speedup: <span class="subst">&#123;avg_latency_baseline / avg_latency_bladedisc:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">input_strs = [</span><br><span class="line">    <span class="string">&quot;We are very happy to show you the story.&quot;</span>,</span><br><span class="line">    <span class="string">&quot;We hope you don&#x27;t hate it.&quot;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">bench_and_report(input_strs)</span><br></pre></td></tr></table></figure>

<p>上述codes中，BladeDISC的核心如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad(), torch_config:</span><br><span class="line">    optimized_ts = torch_blade.optimize(model, allow_tracing=<span class="literal">True</span>, model_inputs=model_inputs)</span><br></pre></td></tr></table></figure>

<p>通过编译手段，生成优化后的pytorch script，注意：<strong>目前pytorch仅仅支持inference，尚不支持train</strong>。对于<code>HuggingFace</code>模型的pipeline有更深层兴趣的，参考<a href="https://huggingface.co/docs/transformers/quicktour">HuggingFace quick tour</a>。</p>
<p><img src="F:\LeonBlog\source\images\image-20250427225534818.png" alt="image-20250427225534818"></p>
<p>可以看到对比pytorch，有1.7倍左右的加速。</p>
<h3 id="Pytorch-WorkFlow"><a href="#Pytorch-WorkFlow" class="headerlink" title="Pytorch WorkFlow"></a><font color = green>Pytorch WorkFlow</font></h3><p><img src="/images/image-20250311211223452.png" alt="image-20250311211223452"></p>
<p><img src="/images/image-20250311220823276.png" alt="image-20250311220823276"></p>
<p>参考<a href="https://github.com/alibaba/BladeDISC/blob/main/docs/developers/bladedisc_torch_overview.md">Torch-Blade教程</a>即可。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color = brown>参考资料</font></h2><ol>
<li><a href="https://github.com/alibaba/BladeDISC">BladeDisc github</a></li>
</ol>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>机器学习编译</category>
        <category>动态shape</category>
      </categories>
      <tags>
        <tag>机器学习编译器</tag>
        <tag>mlir</tag>
        <tag>动态shape</tag>
      </tags>
  </entry>
  <entry>
    <title>BladeDISC: Pass pipeline overview</title>
    <url>/2025/04/29/BladeDISC-Pass-pipeline-overview/</url>
    <content><![CDATA[<p><img src="/images/image-20250430230905042.png" alt="image-20250430230905042"></p>
<span id="more"></span>

<blockquote>
<p>BladeDISC编译器是一个完整的端到端机器学习编译器，其整个流水线的亮点是动态shape支持和大尺度计算密集和内存密集算子融合支持。整个流水线参考XLA的设计，本博客结合实际测试case讲解BladeDISC的流水线设计。</p>
</blockquote>
<h2 id="Pass-Pipeline日志"><a href="#Pass-Pipeline日志" class="headerlink" title="Pass Pipeline日志"></a><font color = brown>Pass Pipeline日志</font></h2><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">===-------------------------------------------------------------------------===</span><br><span class="line">                         ... Execution time report ...</span><br><span class="line">===-------------------------------------------------------------------------===</span><br><span class="line">  Total Execution Time: 0.4353 seconds</span><br><span class="line"></span><br><span class="line">  ----Wall Time----  ----Name----</span><br><span class="line">    0.0001 (  0.0%)  ReviseArgumentsForStaticRankPass</span><br><span class="line">    0.0000 (  0.0%)  FunctionalControlFlowToRegionsPass</span><br><span class="line">    0.0328 (  7.5%)  Inliner</span><br><span class="line">    0.0004 (  0.1%)    (A) CallGraph</span><br><span class="line">    0.0098 (  2.2%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0098 (  2.2%)    Canonicalizer</span><br><span class="line">    0.0002 (  0.1%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0000 (  0.0%)    DropWhileShapeInvariantPass</span><br><span class="line">    0.0000 (  0.0%)    ReplicateTensorListInitOpsPass</span><br><span class="line">    0.0002 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0023 (  0.5%)  SCCP</span><br><span class="line">    0.0001 (  0.0%)  GuaranteeAllFuncsOneUsePass</span><br><span class="line">    0.0000 (  0.0%)    (A) CallGraph</span><br><span class="line">    0.0000 (  0.0%)  TensorFlowShapeInferencePass</span><br><span class="line">    0.0011 (  0.3%)  SCCP</span><br><span class="line">    0.0000 (  0.0%)  TensorListOpsDecompositionPass</span><br><span class="line">    0.0000 (  0.0%)  StackOpsDecompositionPass</span><br><span class="line">    0.0000 (  0.0%)  TensorArrayOpsDecompositionPass</span><br><span class="line">    0.0282 (  6.5%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0282 (  6.5%)    DecomposeResourceOpsPass</span><br><span class="line">    0.0000 (  0.0%)  PromoteResourcesToArgsPass</span><br><span class="line">    0.0000 (  0.0%)  SymbolDCE</span><br><span class="line">    0.0000 (  0.0%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0000 (  0.0%)    SinkConstantsToControlFlowPass</span><br><span class="line">    0.0000 (  0.0%)  TensorFlowShapeInferencePass</span><br><span class="line">    0.0223 (  5.1%)  StablehloLegalizeToHloPass</span><br><span class="line">    0.0224 (  5.2%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0121 (  2.8%)    DiscLowerTfPass</span><br><span class="line">    0.0103 (  2.4%)    LowerQuantizedPass</span><br><span class="line">    0.0001 (  0.0%)  LegalizeTfTypesPass</span><br><span class="line">    0.0933 ( 21.4%)  LegalizeTF</span><br><span class="line">    0.0055 (  1.3%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0054 (  1.2%)    DiscLowerTfPass</span><br><span class="line">    0.0001 (  0.0%)    InfeedOpsXlaAdjustLayout</span><br><span class="line">    0.0001 (  0.0%)  LegalizeTFCollective</span><br><span class="line">    0.0062 (  1.4%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0062 (  1.4%)    Canonicalizer</span><br><span class="line">    0.0001 (  0.0%)  TensorFlowShapeInferencePass</span><br><span class="line">    0.0007 (  0.2%)  LegalizeTF</span><br><span class="line">    0.0001 (  0.0%)  LegalizeTFCommunicationPass</span><br><span class="line">    0.0002 (  0.0%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0001 (  0.0%)    DiscDynamicSliceConverterPass</span><br><span class="line">    0.0001 (  0.0%)    SinkConstantsToControlFlowPass</span><br><span class="line">    0.2094 ( 48.1%)  Rest</span><br><span class="line">    0.4353 (100.0%)  Total</span><br></pre></td></tr></table></figure>

<figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">===-------------------------------------------------------------------------===</span><br><span class="line">                         ... Execution time report ...</span><br><span class="line">===-------------------------------------------------------------------------===</span><br><span class="line">  Total Execution Time: 2.9605 seconds</span><br><span class="line"></span><br><span class="line">  ----Wall Time----  ----Name----</span><br><span class="line">    // =================================== HLO Graph Opt ============================================</span><br><span class="line">    0.0002 (  0.0%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0002 (  0.0%)    DiscAlgebraicSimplifierPass</span><br><span class="line">    0.0001 (  0.0%)  DiscInputOutputAliasPass</span><br><span class="line">    0.0001 (  0.0%)  DiscShapePropagatePass</span><br><span class="line">    0.0007 (  0.0%)  Inliner</span><br><span class="line">    0.0000 (  0.0%)    (A) CallGraph</span><br><span class="line">    0.0002 (  0.0%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0002 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0061 (  0.2%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0001 (  0.0%)    DiscCollectiveOpsRewriterPass</span><br><span class="line">    0.0001 (  0.0%)    MhloDecompositionRewriterPass</span><br><span class="line">    0.0001 (  0.0%)    RemoveShapeConstraintsPass</span><br><span class="line">    0.0002 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0001 (  0.0%)    CSE</span><br><span class="line">    0.0000 (  0.0%)      (A) DominanceInfo</span><br><span class="line">    0.0002 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0001 (  0.0%)    DiscTranformWeightDataLayoutForWeightOnlyQuantPass</span><br><span class="line">    0.0002 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0001 (  0.0%)    DiscCustomCallRewriterPass</span><br><span class="line">    0.0001 (  0.0%)    DiscConvertFakeQuantOpPass</span><br><span class="line">    0.0001 (  0.0%)    DiscLowerGpuQuantizeAndDequantizePass</span><br><span class="line">    0.0048 (  0.2%)    ConvertShapeToStandardPass</span><br><span class="line">    0.0302 (  1.0%)  DiscShapeOptimizationPass</span><br><span class="line">    0.0067 (  0.2%)  &#x27;builtin.func&#x27; Pipeline</span><br><span class="line">    0.0067 (  0.2%)    CSE</span><br><span class="line">    0.0000 (  0.0%)      (A) DominanceInfo</span><br><span class="line">    0.0043 (  0.1%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0001 (  0.0%)    ConvertTensorToStandardPass</span><br><span class="line">    0.0001 (  0.0%)    ConvertHloToStandardPass</span><br><span class="line">    0.0003 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0001 (  0.0%)    CSE</span><br><span class="line">    0.0000 (  0.0%)      (A) DominanceInfo</span><br><span class="line">    0.0003 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0017 (  0.1%)    DiscAlgebraicSimplifierPass</span><br><span class="line">    0.0001 (  0.0%)    SplitLargeOpsPass</span><br><span class="line">    0.0017 (  0.1%)    DotRewriterPass</span><br><span class="line">    0.0039 (  0.1%)  DiscShapeOptimizationPass</span><br><span class="line">    0.0028 (  0.1%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0028 (  0.1%)    DiscDotMergePass</span><br><span class="line">    0.0049 (  0.2%)  DiscShapeOptimizationPass</span><br><span class="line">    0.0001 (  0.0%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0001 (  0.0%)    HloCanonicalizeReductionPass</span><br><span class="line">    0.0039 (  0.1%)  DiscShapeOptimizationPass</span><br><span class="line">    0.0020 (  0.1%)  DiscMarkShapeCalculationPass</span><br><span class="line">    0.0022 (  0.1%)  PlaceOpsPass</span><br><span class="line">    0.0010 (  0.0%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0003 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0000 (  0.0%)    CSE</span><br><span class="line">    0.0000 (  0.0%)      (A) DominanceInfo</span><br><span class="line">    0.0002 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0004 (  0.0%)    ElementTypeConverterPass</span><br><span class="line">    0.0041 (  0.1%)  DiscShapeOptimizationPass</span><br><span class="line">    0.0003 (  0.0%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0001 (  0.0%)    ReductionRewriterPass</span><br><span class="line">    0.0001 (  0.0%)    ConvRewriterPass</span><br><span class="line">    0.0000 (  0.0%)    ConvRewriterPass</span><br><span class="line">    0.0001 (  0.0%)    QuantizedDotRewriterPass</span><br><span class="line">    0.0041 (  0.1%)  DiscShapeOptimizationPass</span><br><span class="line">    0.0022 (  0.1%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0003 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0000 (  0.0%)    CSE</span><br><span class="line">    0.0000 (  0.0%)      (A) DominanceInfo</span><br><span class="line">    0.0003 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0015 (  0.1%)    TransposeSimplifierPass</span><br><span class="line">    0.0000 (  0.0%)    GpuConvPaddingLegalizationPass</span><br><span class="line">    0.0041 (  0.1%)  DiscShapeOptimizationPass</span><br><span class="line">    0.0001 (  0.0%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0001 (  0.0%)    DiscAlgebraicSimplifierPass</span><br><span class="line">    0.0054 (  0.2%)  DiscShapeOptimizationPass</span><br><span class="line">    0.0040 (  0.1%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0034 (  0.1%)    Canonicalizer</span><br><span class="line">    0.0001 (  0.0%)    CSE</span><br><span class="line">    0.0000 (  0.0%)      (A) DominanceInfo</span><br><span class="line">    0.0006 (  0.0%)    Canonicalizer</span><br><span class="line">    // =================================== Bufferize Passes ============================================</span><br><span class="line">    0.0169 (  0.6%)  FuncBufferize</span><br><span class="line">    0.0135 (  0.5%)  DiscHloLegalizeToLhloPass</span><br><span class="line">    0.0099 (  0.3%)  HloLegalizeToLhloPass</span><br><span class="line">    0.0049 (  0.2%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0049 (  0.2%)    Canonicalizer</span><br><span class="line">    0.0002 (  0.0%)  DiscLhloRewriterPass</span><br><span class="line">    0.0099 (  0.3%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0004 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0002 (  0.0%)    ConvertShapeToStandardPass</span><br><span class="line">    0.0004 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0001 (  0.0%)    CSE</span><br><span class="line">    0.0000 (  0.0%)      (A) DominanceInfo</span><br><span class="line">    0.0003 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0042 (  0.1%)    LegalizeToTensorOpPass</span><br><span class="line">    0.0042 (  0.1%)    Canonicalizer</span><br><span class="line">    0.0002 (  0.0%)    StdBufferizePass</span><br><span class="line">    0.0001 (  0.0%)  ArithBufferize</span><br><span class="line">    0.0120 (  0.4%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0041 (  0.1%)    TensorBufferize</span><br><span class="line">    0.0002 (  0.0%)    FinalizingBufferize</span><br><span class="line">    0.0042 (  0.1%)    Canonicalizer</span><br><span class="line">    0.0001 (  0.0%)    CSE</span><br><span class="line">    0.0000 (  0.0%)      (A) DominanceInfo</span><br><span class="line">    0.0004 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0031 (  0.1%)    DiscMemrefCanonicalizer</span><br><span class="line">    0.0044 (  0.1%)  DiscAssignMemorySpacePass</span><br><span class="line">    0.0750 (  2.5%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0081 (  0.3%)    DiscDuplicateComputationForFusionPass</span><br><span class="line">    // =================================== LHLO Graph Opt ============================================</span><br><span class="line">    0.0040 (  0.1%)    PromoteBuffersToStack</span><br><span class="line">    0.0003 (  0.0%)    DiscMemRefLoadStoreSimplifierPass</span><br><span class="line">    0.0162 (  0.5%)    DiscFusionPass</span><br><span class="line">    0.0001 (  0.0%)    DiscFuseSplatConstPass</span><br><span class="line">    0.0170 (  0.6%)    DiscSpecializeFusionWithSpeculationPass</span><br><span class="line">    0.0221 (  0.7%)    Canonicalizer</span><br><span class="line">    0.0066 (  0.2%)    CSE</span><br><span class="line">    0.0000 (  0.0%)      (A) DominanceInfo</span><br><span class="line">    0.0004 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0002 (  0.0%)  DiscOptimizationBarrierExpandPass</span><br><span class="line">    0.0009 (  0.0%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0004 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0001 (  0.0%)    CSE</span><br><span class="line">    0.0000 (  0.0%)      (A) DominanceInfo</span><br><span class="line">    0.0004 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0001 (  0.0%)  DiscOpSchedulePass</span><br><span class="line">    0.0140 (  0.5%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0065 (  0.2%)    DiscReduceBufferLiveRangePass</span><br><span class="line">    0.0074 (  0.2%)    BufferDeallocation</span><br><span class="line">    0.0002 (  0.0%)    DiscBufferDeallocationPass</span><br><span class="line">    // =================================== Extern Lib Call ============================================</span><br><span class="line">    0.0078 (  0.3%)  RalInjectExecutionContextPass</span><br><span class="line">    0.0339 (  1.1%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0339 (  1.1%)    DiscLowerToLibraryCallPass</span><br><span class="line">    0.0221 (  0.7%)  DiscConstToRALPass</span><br><span class="line">    // =================================== end gen ============================================</span><br><span class="line">    0.2155 (  7.3%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0088 (  0.3%)    DiscMemRefLoadStoreSimplifierPass</span><br><span class="line">    0.0128 (  0.4%)    DiscLhloLegalizeRootsToParallelLoopsPass</span><br><span class="line">    0.0004 (  0.0%)    ExpandOps</span><br><span class="line">    0.0005 (  0.0%)    UnhandledAtomicRMWConverterPass</span><br><span class="line">    0.0091 (  0.3%)    InputInlineFusionPass</span><br><span class="line">    0.0135 (  0.5%)    ForLoopUnrollInterleave</span><br><span class="line">    0.0125 (  0.4%)    DiscBF16ExpansionPass</span><br><span class="line">    0.0135 (  0.5%)    ArithExpandOps</span><br><span class="line">    0.0131 (  0.4%)    FoldMemRefAliasOps</span><br><span class="line">    0.0292 (  1.0%)    DiscFlattenMemrefAccessPass</span><br><span class="line">    0.0203 (  0.7%)    Canonicalizer</span><br><span class="line">    0.0136 (  0.5%)    CSE</span><br><span class="line">    0.0000 (  0.0%)      (A) DominanceInfo</span><br><span class="line">    0.0009 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0012 (  0.0%)    DiscMemRefCSEPass</span><br><span class="line">    0.0139 (  0.5%)    ConvertShapeToStandardPass</span><br><span class="line">    0.0115 (  0.4%)    Canonicalizer</span><br><span class="line">    0.0004 (  0.0%)    CSE</span><br><span class="line">    0.0000 (  0.0%)      (A) DominanceInfo</span><br><span class="line">    0.0008 (  0.0%)    Canonicalizer</span><br><span class="line">    // =================================== GPU Kernel gen ============================================</span><br><span class="line">    0.0003 (  0.0%)    ParallelLoopCollapsing</span><br><span class="line">    0.0122 (  0.4%)    SCFParallelLoopTiling</span><br><span class="line">    0.0125 (  0.4%)    GpuMapParallelLoopsPass</span><br><span class="line">    0.0146 (  0.5%)    ConvertParallelLoopToGpu</span><br><span class="line">    0.0008 (  0.0%)  &#x27;func&#x27; Pipeline</span><br><span class="line">    0.0008 (  0.0%)    CSE</span><br><span class="line">    0.0000 (  0.0%)      (A) DominanceInfo</span><br><span class="line">    0.0149 (  0.5%)  GpuLaunchSinkIndexComputations</span><br><span class="line">    0.0381 (  1.3%)  GpuKernelOutlining</span><br><span class="line">    0.0171 (  0.6%)  AssignKernelNamePass</span><br><span class="line">    0.0055 (  0.2%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0055 (  0.2%)    LhloFusionInlinerPass</span><br><span class="line">    0.0161 (  0.5%)  DiscArgsMutationExpandPass</span><br><span class="line">    0.0005 (  0.0%)  DiscCompIntensFusionToCUDASourcePass</span><br><span class="line">    0.0005 (  0.0%)  ReviseGpuKernelOutliningPass</span><br><span class="line">    // =================================== end gen ============================================</span><br><span class="line">    1.7000 ( 57.4%)  &#x27;gpu.module&#x27; Pipeline</span><br><span class="line">    0.0006 (  0.0%)    LoopInvariantCodeMotion</span><br><span class="line">    0.0008 (  0.0%)    &#x27;gpu.func&#x27; Pipeline</span><br><span class="line">    0.0008 (  0.0%)      SideEffectLoopInvariantCodeMotionPass</span><br><span class="line">    0.0004 (  0.0%)    LoopInvariantCodeMotion</span><br><span class="line">    0.0088 (  0.3%)    CSE</span><br><span class="line">    0.0000 (  0.0%)      (A) DominanceInfo</span><br><span class="line">    0.0011 (  0.0%)    &#x27;gpu.func&#x27; Pipeline</span><br><span class="line">    0.0004 (  0.0%)      DiscEraseBufferDeallocationPass</span><br><span class="line">    0.0007 (  0.0%)      ExpandStridedMetadata</span><br><span class="line">    0.0103 (  0.3%)    SCFToControlFlow</span><br><span class="line">    0.0090 (  0.3%)    ConvertAffineToStandard</span><br><span class="line">    0.0086 (  0.3%)    StripDebugInfo</span><br><span class="line">    0.0627 (  2.1%)    DiscLowerGpuOpsToNVVMOpsPass</span><br><span class="line">    0.0141 (  0.5%)    &#x27;llvm.func&#x27; Pipeline</span><br><span class="line">    0.0141 (  0.5%)      LLVMInsertValueSimplifierPass</span><br><span class="line">    0.0141 (  0.5%)    FunctionDeadArgumentEliminationPass</span><br><span class="line">    1.5690 ( 53.0%)    GpuKernelToBlobPass</span><br><span class="line">    0.0007 (  0.0%)  DiscGPUSourceToLibPass</span><br><span class="line">    0.0057 (  0.2%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0047 (  0.2%)    Canonicalizer</span><br><span class="line">    0.0001 (  0.0%)    CSE</span><br><span class="line">    0.0000 (  0.0%)      (A) DominanceInfo</span><br><span class="line">    0.0005 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0001 (  0.0%)    RemoveDeadBufferPass</span><br><span class="line">    0.0002 (  0.0%)    LinalgLowerToLoops</span><br><span class="line">    0.0721 (  2.4%)  SCFToControlFlow</span><br><span class="line">    0.0056 (  0.2%)  &#x27;func.func&#x27; Pipeline</span><br><span class="line">    0.0003 (  0.0%)    ExpandStridedMetadata</span><br><span class="line">    0.0047 (  0.2%)    Canonicalizer</span><br><span class="line">    0.0001 (  0.0%)    CSE</span><br><span class="line">    0.0000 (  0.0%)      (A) DominanceInfo</span><br><span class="line">    0.0005 (  0.0%)    Canonicalizer</span><br><span class="line">    0.0731 (  2.5%)  ConvertAffineToStandard</span><br><span class="line">    0.0727 (  2.5%)  StripDebugInfo</span><br><span class="line">    0.0725 (  2.4%)  DiscStripShapeConstraintOpsPass</span><br><span class="line">    0.1400 (  4.7%)  DiscToLLVMPass</span><br><span class="line">    0.1883 (  6.4%)  Rest</span><br><span class="line">    2.9605 (100.0%)  Total</span><br></pre></td></tr></table></figure>



<h2 id="Pipeline罗列"><a href="#Pipeline罗列" class="headerlink" title="Pipeline罗列"></a><font color = brown>Pipeline罗列</font></h2><p><img src="/images/pass_pipeline.png" alt="pass_pipeline"></p>
<p>以下是DISC动态形状编译器Pass管道的阶段划分及对应Passes：</p>
<h3 id="1-TF-to-HLO转换阶段"><a href="#1-TF-to-HLO转换阶段" class="headerlink" title="1. TF-to-HLO转换阶段"></a><strong>1. TF-to-HLO转换阶段</strong></h3><table>
<thead>
<tr>
<th align="center">Pass名称</th>
<th align="center">功能描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>-disc-tf-revise-args-for-static-rank</code></td>
<td align="center">为静态秩编译器修订参数，处理动态形状但静态秩的约束</td>
</tr>
<tr>
<td align="center"><code>-disc-lower-tf</code></td>
<td align="center">将TensorFlow操作转换为MHLO操作，处理自定义调用（如RandomUniform和TopK）</td>
</tr>
<tr>
<td align="center"><code>-tf-shape-inference</code></td>
<td align="center">执行形状推断，优化静态形状语义</td>
</tr>
<tr>
<td align="center"><code>-xla-legalize-tf</code></td>
<td align="center">将TensorFlow操作合法化为XLA&#x2F;HLO操作，分阶段处理部分&#x2F;完全转换</td>
</tr>
</tbody></table>
<hr>
<h3 id="2-HLO图优化阶段"><a href="#2-HLO图优化阶段" class="headerlink" title="2. HLO图优化阶段"></a><strong>2. HLO图优化阶段</strong></h3><h4 id="1-形状简化与约束"><a href="#1-形状简化与约束" class="headerlink" title="(1) 形状简化与约束"></a><strong>(1) 形状简化与约束</strong></h4><table>
<thead>
<tr>
<th align="center">Pass名称</th>
<th align="center">功能描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>ShapeSimplifierPass</code></td>
<td align="center">传播已知形状信息，消除不必要的未知维度（支持静态秩约束）</td>
</tr>
<tr>
<td align="center"><code>InsertTieShapePass</code></td>
<td align="center">插入<code>disc_shape.tie_shape</code>操作，显式表达形状约束</td>
</tr>
</tbody></table>
<h4 id="2-放置（Placement）-key"><a href="#2-放置（Placement）-key" class="headerlink" title="(2) 放置（Placement）:key:"></a><strong>(2) 放置（Placement）</strong>:key:</h4><ul>
<li><p><input checked="" disabled="" type="checkbox"> 
<code>DiscMarkShapeCalculationPass</code></p>
<pre class="mermaid">  graph TD
    A[开始] --> B[获取Module和主函数main]
    B --> C{主函数存在?}
    C -->|是| D[初始化标记集合shape_calc_ops]
    C -->|否| E[Pass失败终止]
    
    D --> F[直接标记阶段]
    F --> G[遍历基本块所有操作]
    G --> H{是否目标操作?}
    H -->|是| I{是GetDimensionSizeOp/FromElementsOp/ExtractOp?}
    H -->|否| G
    
    I -->|是 且 非WhereOp输入| J[加入标记集合]
    I -->|否| K[查表获取操作数索引]
    K --> L[遍历所有需标记的操作数]
    L --> M{操作数有效且符合条件?}
    M -->|是| N[加入标记集合]
    M -->|否| L
    
    F --> O[逆向传播阶段]
    O --> P[逆序遍历基本块操作]
    P --> Q{已标记操作?}
    Q -->|是| R[遍历所有操作数]
    R --> S{操作数有效且符合条件?}
    S -->|是| T{是DimOp/ShapeOfOp?}
    T -->|否| U{静态张量且元素>64?}
    U -->|否| V{来自WhereOp?}
    V -->|否| W[加入标记集合]
    
    O --> X[属性设置阶段]
    X --> Y[遍历标记集合中的操作]
    Y --> Z{输出是元组类型?}
    Z -->|是| AA[设置数组属性]
    Z -->|否| AB[设置布尔属性]
    X --> AC[Pass完成]
    
    style A fill:#90EE90,stroke:#333
    style E fill:#FF6347,stroke:#333
    style J fill:#87CEEB,stroke:#333
    style N fill:#87CEEB,stroke:#333
    style W fill:#87CEEB,stroke:#333
    style AA fill:#FFD700,stroke:#333
    style AB fill:#FFD700,stroke:#333
    style AC fill:#90EE90,stroke:#333</pre>
</li>
<li><p><input checked="" disabled="" type="checkbox"> 
<code>PlaceOpsPass</code></p>
<pre class="mermaid">  graph TD
    A[开始] --> B[初始化输入输出放置信息]
    B --> C[处理CustomCallV2操作]
    C --> D[处理i64标量返回操作]
    D --> E[处理形状计算操作]
    E --> F[处理i32类型操作]
    F --> G[设置默认设备放置]
    G --> H[插入内存拷贝节点]
    H --> I[结束]
    
    subgraph 初始化
    B[解析入口函数的输入输出属性<br>填充input_placements_和output_placements_]
    end
    
    subgraph 规则处理
    C[遍历CustomCallV2Op<br>根据device属性设置CPU/GPU]
    D[标记返回的i64标量操作<br>向上标记相关操作数]
    E[遍历所有操作<br>根据kDiscShapeCalcAttr标记CPU]
    F[遍历所有操作<br>处理32位整数相关操作规则]
    end
    
    subgraph 默认处理
    G[遍历所有未标记操作<br>设置默认GPU/CPU放置]
    end
    
    subgraph 跨设备处理
    H[遍历所有操作和返回节点<br>分析输入输出设备差异<br>插入H2D/D2H转换操作]
    end</pre></li>
</ul>
<table>
<thead>
<tr>
<th align="center">Pass名称</th>
<th align="center">功能描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>DiscMarkShapeCalculationPass</code></td>
<td align="center">标记形状计算操作（如<code>tensor.dim</code>）</td>
</tr>
<tr>
<td align="center"><code>PlaceOpsPass</code></td>
<td align="center">将形状计算操作显式分配到CPU，插入内存拷贝操作</td>
</tr>
</tbody></table>
<h4 id="3-图优化"><a href="#3-图优化" class="headerlink" title="(3) 图优化"></a><strong>(3) 图优化</strong></h4><table>
<thead>
<tr>
<th align="center">Pass名称</th>
<th align="center">功能描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center">代数简化器</td>
<td align="center">执行通用代数优化（如常量折叠、冗余消除）</td>
</tr>
</tbody></table>
<h4 id="4-其他转换"><a href="#4-其他转换" class="headerlink" title="(4) 其他转换"></a><strong>(4) 其他转换</strong></h4><table>
<thead>
<tr>
<th align="center">Pass名称</th>
<th align="center">功能描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>RemoveShapeConstraintsPass</code></td>
<td align="center">移除不再需要的形状约束操作</td>
</tr>
<tr>
<td align="center"><code>DotRewriterPass</code></td>
<td align="center">将<code>mhlo.dot</code>转换为<code>mhlo.dot_general</code>以支持更灵活的代码生成</td>
</tr>
</tbody></table>
<hr>
<h3 id="3-缓冲化（Bufferization）阶段"><a href="#3-缓冲化（Bufferization）阶段" class="headerlink" title="3. 缓冲化（Bufferization）阶段"></a><strong>3. 缓冲化（Bufferization）阶段</strong></h3><table>
<thead>
<tr>
<th align="center">Pass名称</th>
<th align="center">功能描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>HLO-to-LMHLO转换</code></td>
<td align="center">将MHLO操作转换为LMHLO（内存缓冲区形式）</td>
</tr>
<tr>
<td align="center"><code>ShapeRelatedOpsBufferization</code></td>
<td align="center">处理形状相关操作的缓冲化（如<code>tensor.from_elements</code>）</td>
</tr>
<tr>
<td align="center"><code>DiscAssignMemorySpacePass</code></td>
<td align="center">显式分配内存空间（CPU&#x2F;GPU）</td>
</tr>
<tr>
<td align="center"><code>PromoteBuffersToStack</code></td>
<td align="center">将小型CPU缓冲区提升为栈分配</td>
</tr>
<tr>
<td align="center"><code>BufferDeallocation</code></td>
<td align="center">插入显式的<code>memref.dealloc</code>操作</td>
</tr>
</tbody></table>
<hr>
<h3 id="4-LHLO图优化阶段-key"><a href="#4-LHLO图优化阶段-key" class="headerlink" title="4. LHLO图优化阶段:key:"></a><strong>4. LHLO图优化阶段</strong>:key:</h3><h4 id="1-融合（Fusion）"><a href="#1-融合（Fusion）" class="headerlink" title="(1) 融合（Fusion）"></a><strong>(1) 融合（Fusion）</strong></h4><p>这一部分内容，详情参考先前的一篇存储密集算子融合博客。</p>
<table>
<thead>
<tr>
<th align="center">Pass名称</th>
<th align="center">功能描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>DiscFusionPass</code></td>
<td align="center">基础融合策略（类似XLA的输入&#x2F;循环融合）</td>
</tr>
<tr>
<td align="center"><code>DiscStitchFusionPass</code></td>
<td align="center">激进融合策略（利用共享内存或缓存优化）</td>
</tr>
</tbody></table>
<h4 id="2-推测优化（Speculation）"><a href="#2-推测优化（Speculation）" class="headerlink" title="(2) 推测优化（Speculation）"></a><strong>(2) 推测优化（Speculation）</strong></h4><ul>
<li><input checked="" disabled="" type="checkbox"> <code>DiscSpecializeFusionWithSpeculationPass</code></li>
</ul>
<p><img src="/images/image-20250509170706617.png" alt="image-20250509170706617"></p>
<table>
<thead>
<tr>
<th align="center">Pass名称</th>
<th align="center">功能描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>DiscSpecializeFusionWithSpeculationPass</code></td>
<td align="center">生成多版本内核以适配不同运行时形状</td>
</tr>
</tbody></table>
<hr>
<h3 id="5-运行时与库调用相关阶段-key"><a href="#5-运行时与库调用相关阶段-key" class="headerlink" title="5. 运行时与库调用相关阶段:key:"></a><strong>5. 运行时与库调用相关阶段</strong>:key:</h3><ul>
<li><p><input checked="" disabled="" type="checkbox"> 
<code>RalInjectExecutionContextPass</code>（这个pass比较简单，所以看过代码即可）</p>
</li>
<li><p><input checked="" disabled="" type="checkbox"> 
<code>DiscLowerToLibraryCallPass</code>:fire:这个pass比较重要</p>
<p><img src="/images/image-20250509170648846.png" alt="image-20250509170648846"></p>
<p>这个pass有几个比较有趣的改写过程，接下来详细解读。</p>
<h4 id="CUDA-GPU-copy操作"><a href="#CUDA-GPU-copy操作" class="headerlink" title="CUDA GPU copy操作"></a>CUDA GPU copy操作</h4><p>首先是代码中对于gpu copy的处理。在cuda中，gpu copy可能有两个方向：H2D和D2H。cuda为了支持数据传输和kernel执行的并行性，引入<code>cuda stream</code>这个概念。可以参考<a href="https://leimao.github.io/blog/CUDA-Stream/">CUDA stream blog</a>来加强理解。</p>
<p>首先结合下面的图理解传输和kernel计算并行是如何带来性能收益的：</p>
<p><img src="/images/image-20250509150745571.png" alt="image-20250509150745571"></p>
<p>在cuda中，通过stream这个概念来指导并行执行：</p>
<blockquote>
<p>According to the <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#streams">CUDA programming guide</a>, a stream is a sequence of commands (possibly issued by different host threads) that execute in order. Different streams, on the other hand, may execute their commands out of order with respect to one another or concurrently.</p>
</blockquote>
<p>在cuda的默认模式中，不论是kernel执行，h2d的内存拷贝还是d2h的内存拷贝，都使用的默认stream（null stream或是0stream）。在早期版本中，default stream必须等待已经launch的stream完成才能开始，并在其他stream开始之前完成。<strong>显然，default stream是顺序模型</strong>。</p>
</li>
</ul>
<p>​	如下是一段sample code：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Convenience function for checking CUDA runtime API results</span></span><br><span class="line"><span class="comment">// can be wrapped around any runtime API call. No-op in release builds.</span></span><br><span class="line"><span class="function"><span class="keyword">inline</span></span></span><br><span class="line"><span class="function">cudaError_t <span class="title">checkCuda</span><span class="params">(cudaError_t result)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="meta">#<span class="keyword">if</span> defined(DEBUG) || defined(_DEBUG)</span></span><br><span class="line">  <span class="keyword">if</span> (result != cudaSuccess) &#123;</span><br><span class="line">    <span class="built_in">fprintf</span>(stderr, <span class="string">&quot;CUDA Runtime Error: %s\n&quot;</span>, <span class="built_in">cudaGetErrorString</span>(result));</span><br><span class="line">    <span class="built_in">assert</span>(result == cudaSuccess);</span><br><span class="line">  &#125;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line">  <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">kernel</span><span class="params">(<span class="type">float</span> *a, <span class="type">int</span> offset)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">// 一维数据，通过threadIdx.x以及offset结合完成索引工作</span></span><br><span class="line">  <span class="type">int</span> i = offset + threadIdx.x + blockIdx.x*blockDim.x;</span><br><span class="line">  <span class="type">float</span> x = (<span class="type">float</span>)i;</span><br><span class="line">  <span class="type">float</span> s = <span class="built_in">sinf</span>(x); </span><br><span class="line">  <span class="type">float</span> c = <span class="built_in">cosf</span>(x);</span><br><span class="line">  a[i] = a[i] + <span class="built_in">sqrtf</span>(s*s+c*c);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">maxError</span><span class="params">(<span class="type">float</span> *a, <span class="type">int</span> n)</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="type">float</span> maxE = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">    <span class="type">float</span> error = <span class="built_in">fabs</span>(a[i]<span class="number">-1.0f</span>);</span><br><span class="line">    <span class="keyword">if</span> (error &gt; maxE) maxE = error;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> maxE;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">// 定义每一个block有多大，以及有多少个stream</span></span><br><span class="line">  <span class="comment">// stream表示潜在并行性</span></span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> blockSize = <span class="number">256</span>, nStreams = <span class="number">4</span>;</span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> n = <span class="number">4</span> * <span class="number">1024</span> * blockSize * nStreams;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//</span></span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> streamSize = n / nStreams;</span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> streamBytes = streamSize * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">  <span class="type">const</span> <span class="type">int</span> bytes = n * <span class="built_in">sizeof</span>(<span class="type">float</span>);</span><br><span class="line">   </span><br><span class="line">  <span class="type">int</span> devId = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">if</span> (argc &gt; <span class="number">1</span>) devId = <span class="built_in">atoi</span>(argv[<span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">  cudaDeviceProp prop;</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaGetDeviceProperties</span>(&amp;prop, devId));</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Device : %s\n&quot;</span>, prop.name);</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaSetDevice</span>(devId) );</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// allocate pinned host memory and device memory</span></span><br><span class="line">  <span class="type">float</span> *a, *d_a;</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaMallocHost</span>((<span class="type">void</span>**)&amp;a, bytes) );      <span class="comment">// host pinned</span></span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaMalloc</span>((<span class="type">void</span>**)&amp;d_a, bytes) ); <span class="comment">// device</span></span><br><span class="line"></span><br><span class="line">  <span class="type">float</span> ms; <span class="comment">// elapsed time in milliseconds</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">// create events and streams</span></span><br><span class="line">  cudaEvent_t startEvent, stopEvent, dummyEvent;</span><br><span class="line">  cudaStream_t stream[nStreams];</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaEventCreate</span>(&amp;startEvent) );</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaEventCreate</span>(&amp;stopEvent) );</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaEventCreate</span>(&amp;dummyEvent) );</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 构建stream</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nStreams; ++i)</span><br><span class="line">    <span class="built_in">checkCuda</span>( <span class="built_in">cudaStreamCreate</span>(&amp;stream[i]) );</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// baseline case - sequential transfer and execute</span></span><br><span class="line">  <span class="built_in">memset</span>(a, <span class="number">0</span>, bytes);</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaEventRecord</span>(startEvent,<span class="number">0</span>) );</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaMemcpy</span>(d_a, a, bytes, cudaMemcpyHostToDevice) );</span><br><span class="line">  kernel&lt;&lt;&lt;n/blockSize, blockSize&gt;&gt;&gt;(d_a, <span class="number">0</span>);</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaMemcpy</span>(a, d_a, bytes, cudaMemcpyDeviceToHost) );</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaEventRecord</span>(stopEvent, <span class="number">0</span>) );</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaEventSynchronize</span>(stopEvent) );</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaEventElapsedTime</span>(&amp;ms, startEvent, stopEvent) );</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Time for sequential transfer and execute (ms): %f\n&quot;</span>, ms);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;  max error: %e\n&quot;</span>, <span class="built_in">maxError</span>(a, n));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// asynchronous version 1: loop over &#123;copy, kernel, copy&#125;</span></span><br><span class="line">  <span class="built_in">memset</span>(a, <span class="number">0</span>, bytes);</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaEventRecord</span>(startEvent,<span class="number">0</span>) );</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nStreams; ++i) &#123;</span><br><span class="line">    <span class="type">int</span> offset = i * streamSize;</span><br><span class="line">    <span class="built_in">checkCuda</span>( <span class="built_in">cudaMemcpyAsync</span>(&amp;d_a[offset], &amp;a[offset], </span><br><span class="line">                               streamBytes, cudaMemcpyHostToDevice, </span><br><span class="line">                               stream[i]) );</span><br><span class="line">    kernel&lt;&lt;&lt;streamSize/blockSize, blockSize, <span class="number">0</span>, stream[i]&gt;&gt;&gt;(d_a, offset);</span><br><span class="line">    <span class="built_in">checkCuda</span>( <span class="built_in">cudaMemcpyAsync</span>(&amp;a[offset], &amp;d_a[offset], </span><br><span class="line">                               streamBytes, cudaMemcpyDeviceToHost,</span><br><span class="line">                               stream[i]) );</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaEventRecord</span>(stopEvent, <span class="number">0</span>) );</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaEventSynchronize</span>(stopEvent) );</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaEventElapsedTime</span>(&amp;ms, startEvent, stopEvent) );</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Time for asynchronous V1 transfer and execute (ms): %f\n&quot;</span>, ms);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;  max error: %e\n&quot;</span>, <span class="built_in">maxError</span>(a, n));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// asynchronous version 2: </span></span><br><span class="line">  <span class="comment">// loop over copy, loop over kernel, loop over copy</span></span><br><span class="line">  <span class="built_in">memset</span>(a, <span class="number">0</span>, bytes);</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaEventRecord</span>(startEvent,<span class="number">0</span>) );</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nStreams; ++i)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="type">int</span> offset = i * streamSize;</span><br><span class="line">    <span class="built_in">checkCuda</span>( <span class="built_in">cudaMemcpyAsync</span>(&amp;d_a[offset], &amp;a[offset], </span><br><span class="line">                               streamBytes, cudaMemcpyHostToDevice,</span><br><span class="line">                               stream[i]) );</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nStreams; ++i)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="type">int</span> offset = i * streamSize;</span><br><span class="line">    kernel&lt;&lt;&lt;streamSize/blockSize, blockSize, <span class="number">0</span>, stream[i]&gt;&gt;&gt;(d_a, offset);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nStreams; ++i)</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="type">int</span> offset = i * streamSize;</span><br><span class="line">    <span class="built_in">checkCuda</span>( <span class="built_in">cudaMemcpyAsync</span>(&amp;a[offset], &amp;d_a[offset], </span><br><span class="line">                               streamBytes, cudaMemcpyDeviceToHost,</span><br><span class="line">                               stream[i]) );</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaEventRecord</span>(stopEvent, <span class="number">0</span>) );</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaEventSynchronize</span>(stopEvent) );</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaEventElapsedTime</span>(&amp;ms, startEvent, stopEvent) );</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;Time for asynchronous V2 transfer and execute (ms): %f\n&quot;</span>, ms);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;  max error: %e\n&quot;</span>, <span class="built_in">maxError</span>(a, n));</span><br><span class="line"></span><br><span class="line">  <span class="comment">// cleanup</span></span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaEventDestroy</span>(startEvent) );</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaEventDestroy</span>(stopEvent) );</span><br><span class="line">  <span class="built_in">checkCuda</span>( <span class="built_in">cudaEventDestroy</span>(dummyEvent) );</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; nStreams; ++i)</span><br><span class="line">    <span class="built_in">checkCuda</span>( <span class="built_in">cudaStreamDestroy</span>(stream[i]) );</span><br><span class="line">  <span class="built_in">cudaFree</span>(d_a);</span><br><span class="line">  <span class="built_in">cudaFreeHost</span>(a);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/images/image-20250509153548265.png" alt="image-20250509153548265"></p>
<p>从结果来看，stream有效提高并行性，降低等待延迟。<strong>至于V2相比V1，由于V1中的每个stream的H2D，kernel计算和D2H必须顺序执行，因此计算资源和dma带宽利用率不如V2</strong>。</p>
<p><font color = red>注意，目前的BladeDISC只支持单个stream（没有多流支持），因此性能上BladeDISC的传输效率有缺失。这里mark住，后续可以尝试改进。</font></p>
<pre><code>#### ConvolutionOp Convertor
</code></pre>
<p>这个convertor干的事情十分简单：将卷积操作变成显示的lib call。其主要完成的工作如下：</p>
<ul>
<li><p>提取出convolution操作的输入（两个）和输出（一个）</p>
</li>
<li><p>根据输出，判断该kernel是在cpu上计算还是gpu上计算</p>
</li>
<li><p>计算padding大小</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">Value <span class="title">GetPadding</span><span class="params">(ConvolutionOp op, PatternRewriter&amp; rewriter)</span> <span class="type">const</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 构建padding的type</span></span><br><span class="line">    Location loc = op.<span class="built_in">getLoc</span>();</span><br><span class="line">    Type field_type = rewriter.<span class="built_in">getI32Type</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取卷积维度信息，用来判断padding的数量</span></span><br><span class="line">    <span class="comment">// &lt;?x?x?x?&gt;四维矩阵，则需要填充宽高两个维度，并左右各一，所以是（rank-2）* 2 </span></span><br><span class="line">    <span class="type">int</span> rank = op.<span class="built_in">getOutput</span>().<span class="built_in">getType</span>().<span class="keyword">template</span> <span class="built_in">cast</span>&lt;ShapedType&gt;().<span class="built_in">getRank</span>();</span><br><span class="line">    <span class="type">int</span> num_metadata_fields = (rank - <span class="number">2</span>) * <span class="number">2</span>;</span><br><span class="line">    Value metadata_value = rewriter.<span class="built_in">create</span>&lt;memref::AllocaOp&gt;(</span><br><span class="line">        loc, MemRefType::<span class="built_in">get</span>(&#123;num_metadata_fields&#125;, field_type,</span><br><span class="line">                             <span class="built_in">MemRefLayoutAttrInterface</span>()));</span><br><span class="line">    <span class="comment">// padding</span></span><br><span class="line">    <span class="keyword">auto</span> padding = disc_ral::<span class="built_in">ConvertDenseIntAttr</span>(op.<span class="built_in">getPadding</span>());</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">const</span> <span class="keyword">auto</span>&amp;&amp; en : llvm::<span class="built_in">enumerate</span>(padding)) &#123;</span><br><span class="line">      Value value =</span><br><span class="line">          rewriter.<span class="built_in">create</span>&lt;arith::ConstantIntOp&gt;(loc, en.<span class="built_in">value</span>(), field_type);</span><br><span class="line">      Value offset = rewriter.<span class="built_in">create</span>&lt;arith::ConstantIndexOp&gt;(loc, en.<span class="built_in">index</span>());</span><br><span class="line">      <span class="function">SmallVector&lt;Value, 1&gt; <span class="title">ivs</span><span class="params">(<span class="number">1</span>, offset)</span></span>;</span><br><span class="line">      rewriter.<span class="built_in">create</span>&lt;memref::StoreOp&gt;(loc, value, metadata_value, ivs);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回[1,1,1,1]</span></span><br><span class="line">    <span class="keyword">return</span> metadata_value;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>获取卷积操作的metadata，有如下重要参数：</p>
<ul>
<li><strong>维度布局</strong>：输入、核、输出的维度顺序</li>
<li><strong>步长（Stride）</strong>：卷积核移动的步幅</li>
<li><strong>膨胀（Dilation）</strong>：卷积核元素的间隔</li>
<li><strong>核常量标记</strong>：指示核数据是否编译期常量</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"># 元数据内存布局（i32数组）</span><br><span class="line">[</span><br><span class="line">  # 输入布局</span><br><span class="line">  <span class="number">0</span>,  # N维度位置</span><br><span class="line">  <span class="number">3</span>,  # C维度位置</span><br><span class="line">  <span class="number">1</span>, <span class="number">2</span>,  # 空间维度H、W的位置  </span><br><span class="line">  # 核布局</span><br><span class="line">  <span class="number">2</span>,  # 输入通道（I）位置（HWC中的C）</span><br><span class="line">  <span class="number">3</span>,  # 输出通道（O）位置</span><br><span class="line">  <span class="number">0</span>, <span class="number">1</span>,  # 空间维度H、W的位置  </span><br><span class="line">  # 输出布局</span><br><span class="line">  <span class="number">0</span>,  # N维度位置</span><br><span class="line">  <span class="number">3</span>,  # C维度位置</span><br><span class="line">  <span class="number">1</span>, <span class="number">2</span>,  # 空间维度H、W的位置 </span><br><span class="line">  # 步长（H方向步长<span class="number">2</span>，W方向步长<span class="number">1</span>）</span><br><span class="line">  <span class="number">2</span>, <span class="number">1</span>,  </span><br><span class="line">  # 膨胀（H方向膨胀<span class="number">1</span>，W方向膨胀<span class="number">1</span>）</span><br><span class="line">  <span class="number">1</span>, <span class="number">1</span>,</span><br><span class="line">  # 核常量标记</span><br><span class="line">  <span class="number">1</span>  # 假设核是常量</span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p>其中膨胀这个概念比较陌生。其动机是为了扩大感受野的同时不增加计算量，具体地自行参考文献，不是本文重点：</p>
<p><img src="/images/image-20250509161455535.png" alt="image-20250509161455535"></p>
</li>
<li><p>构建operation</p>
</li>
</ul>
<p>完整的流程如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">LogicalResult <span class="title">matchAndRewrite</span><span class="params">(ConvolutionOp op,</span></span></span><br><span class="line"><span class="params"><span class="function">                                PatternRewriter&amp; rewriter)</span> <span class="type">const</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">    Location loc = op.<span class="built_in">getLoc</span>();</span><br><span class="line">    Value ctx = <span class="built_in">GetContextValueFromFunctionArguments</span>(op);</span><br><span class="line">    <span class="keyword">if</span> (!ctx) &#123;</span><br><span class="line">      <span class="keyword">return</span> op-&gt;<span class="built_in">emitOpError</span>()</span><br><span class="line">             &lt;&lt; <span class="string">&quot;the first argument of the function is not ral context type.&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果数据来源也是H2D，则是同一个stream（目前只支持单stream）</span></span><br><span class="line">    Value stream_handle = <span class="built_in">GetDefaultStreamHandle</span>(op, rewriter);</span><br><span class="line">    SmallVector&lt;Value, <span class="number">4</span>&gt; newOperands&#123;stream_handle&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// input</span></span><br><span class="line">    newOperands.<span class="built_in">push_back</span>(op.<span class="built_in">getOperand</span>(<span class="number">0</span>));</span><br><span class="line">    <span class="comment">// kernel</span></span><br><span class="line">    newOperands.<span class="built_in">push_back</span>(op.<span class="built_in">getOperand</span>(<span class="number">1</span>));</span><br><span class="line">    <span class="comment">// padding</span></span><br><span class="line">    newOperands.<span class="built_in">push_back</span>(<span class="built_in">GetPadding</span>(op, rewriter));</span><br><span class="line">    <span class="comment">// output</span></span><br><span class="line">    newOperands.<span class="built_in">push_back</span>(op.<span class="built_in">getOperand</span>(<span class="number">2</span>));</span><br><span class="line">    <span class="comment">// input &amp; kernel &amp; output layouts, strides and dilation</span></span><br><span class="line">    newOperands.<span class="built_in">push_back</span>(<span class="built_in">GetConvMetadata</span>(op, rewriter));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 判断输出的存储空间是否是GPU</span></span><br><span class="line">    <span class="type">bool</span> on_gpu = placement_utils::<span class="built_in">isGpuMemRef</span>(op-&gt;<span class="built_in">getOperand</span>(<span class="number">2</span>));</span><br><span class="line">    rewriter.<span class="built_in">replaceOpWithNewOp</span>&lt;DispatchOp&gt;(op, TypeRange&#123;&#125;, ctx, newOperands,</span><br><span class="line">                                            <span class="string">&quot;ral_conv&quot;</span>, <span class="literal">false</span>,</span><br><span class="line">                                            on_gpu ? <span class="string">&quot;gpu&quot;</span> : <span class="string">&quot;cpu&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">success</span>();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><p><input checked="" disabled="" type="checkbox"> 
<code>DiscConstToRALPass</code></p>
<ol>
<li><strong>跨平台兼容</strong>：通过<code>on_host</code>标志区分主机&#x2F;设备常量</li>
<li><strong>零冗余存储</strong>：相同常量数据只保存一份</li>
<li><strong>运行时高效查找</strong>：通过预生成索引加速常量加载</li>
<li><strong>类型安全</strong>：在名称中编码数据类型和形状信息</li>
</ol>
<p>该Pass成功将编译期常量转换为运行时加载机制，为支持动态常量更新和跨模型常量共享奠定了基础。具体作用目前尚不能理解清楚</p>
<p>这个pass的流程图如下所示：</p>
<pre class="mermaid">  graph TD
    A[开始] --> B[获取ModuleOp]
    B --> C[初始化元数据文件emitter]
    C --> D[遍历模块收集ConstantOp]
    D --> E{是否在Fusion或特定函数中?}
    E --> |是| F[跳过]
    E --> |否| G[加入工作列表]
    G --> H[遍历工作列表处理每个ConstantOp]
    H --> I[调用convertConstantOp]
    I --> J[生成唯一名称和元数据]
    J --> K[写入元数据文件]
    K --> L[创建全局字符串符号]
    L --> M[构建RAL调用DispatchOp]
    M --> N[替换原常量并删除]
    N --> O{全部处理完成?}
    O --> |否| H
    O --> |是| P[写入元数据尾部]
    P --> Q{成功?}
    Q --> |否| R[报错终止]
    Q --> |是| S[结束]
    
    subgraph convertConstantOp子流程
        I --> I1[转换i1类型到i8]
        I1 --> I2[提取常量数据]
        I2 --> I3[生成MD5哈希名称]
        I3 --> I4[判断主机/设备类型]
        I4 --> I5[更新元数据索引]
        I5 --> I6[创建LLVM全局符号]
        I6 --> I7[构建DispatchOp参数]
    end</pre></li>
</ul>
<table>
<thead>
<tr>
<th align="center">Pass名称</th>
<th align="center">功能描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>RalInjectExecutionContextPass</code></td>
<td align="center">注入RAL执行上下文参数</td>
</tr>
<tr>
<td align="center"><code>DiscLowerToLibraryCallPass</code></td>
<td align="center">将非代码生成操作（如GEMM&#x2F;Conv）转换为<code>disc_ral.dispatch</code>调用</td>
</tr>
<tr>
<td align="center"><code>DiscConstToRALPass</code></td>
<td align="center">将常量操作转换为RAL库调用，管理常量生命周期</td>
</tr>
</tbody></table>
<hr>
<h3 id="6-代码生成阶段-key"><a href="#6-代码生成阶段-key" class="headerlink" title="6. 代码生成阶段:key:"></a><strong>6. 代码生成阶段</strong>:key:</h3><p>这个阶段的最主要工作是将mhlo操作下降到嵌套循环中。<a href="https://github.com/alibaba/BladeDISC/blob/main/docs/developers/pass_pipeline.md#runtime---library-call-related-passes">BladeDISC pass pipeline guide</a>中指明代码生成阶段的设计思路：</p>
<p><img src="/images/image-20250509165358358.png" alt="image-20250509165358358"></p>
<h4 id="1-主干代码生成"><a href="#1-主干代码生成" class="headerlink" title="(1) 主干代码生成"></a><strong>(1) 主干代码生成</strong></h4><p>这两个pass在文档中称之为backend bone passes。</p>
<p><img src="/images/image-20250509170333963.png" alt="image-20250509170333963"></p>
<p>如上图所示，第一个pass负责将root操作变为并行循环function。第二个pass负责不断地把producer op做融合。</p>
<ul>
<li><p>第一个pass的并行循环涉及到<strong>调度策略</strong>，具体地调度策略见下表：</p>
<ul>
<li><strong>RowReductionSchedule1</strong>：使用两轮Warp Shuffle，适用于较大的归约维度（如行归约维度较大时，减少线程同步开销）。</li>
<li><strong>RowReductionSchedule2</strong>：使用一轮Warp Shuffle，适用于较小的归约维度（减少计算步骤，提升速度）。</li>
<li><strong>ColReductionSchedule</strong>：基于原子操作的列归约，兼容其他行归约调度（灵活性高，但性能可能受限）。</li>
<li><strong>ColReductionBlockTileSchedule</strong>：高性能列归约调度，无法与其他行归约调度共存（专为性能优化，牺牲兼容性）。</li>
<li><strong>LoopSchedule</strong>：普通循环融合调度，通用性强，可与其他调度组合。</li>
</ul>
<p>若同时存在行和列归约，优先选择行归约调度，列归约退化为原子操作实现，可能生成独立的初始化内核（避免数据竞争）。这一段代码比较有意思，下面重点分析（涉及代码生成的调度选择问题，是gpu launch比较有意思的话题）。</p>
<p><font color = brown><strong>GPU并行调度选择算法</strong></font></p>
<p>从test case来看：</p>
<p><img src="/images/image-20250509190401475.png" alt="image-20250509190401475"></p>
<p>将<code>lmhlo.fusion</code>操作的root操作做了循环展开，用<code>scf.parallel</code>来表示可并行的点，在后续的后端特定pass中会变成threadIdx.x和blockIdx.x的gpu访问模型。这里比较有趣的点是<code>scf.parallel</code>内部还嵌套一个<code>scf.for</code>循环，维度为4。这个分支是当维度为4的倍数时做的向量化，一个thread可以并行处理4的SIMD。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// -----// IR Dump After DiscMemRefLoadStoreSimplifierPass (disc-memref-load-store-simplifier) //----- //</span></span><br><span class="line">func.func @<span class="built_in">main</span>(%arg0: !disc_ral.context) attributes &#123;tf.entry_function = &#123;input_placements = <span class="string">&quot;gpu&quot;</span>, inputs = <span class="string">&quot;input.1_&quot;</span>, output_placements = <span class="string">&quot;gpu&quot;</span>, outputs = <span class="string">&quot;8&quot;</span>&#125;&#125; &#123;</span><br><span class="line">  %c3_i32 = arith.constant <span class="number">3</span> : i32</span><br><span class="line">  %c2_i32 = arith.constant <span class="number">2</span> : i32</span><br><span class="line">  %c1_i32 = arith.constant <span class="number">1</span> : i32</span><br><span class="line">  %c0_i32 = arith.constant <span class="number">0</span> : i32</span><br><span class="line">  %<span class="number">0</span> = llvm.mlir.<span class="built_in">constant</span>(<span class="number">0</span> : i32) : i32</span><br><span class="line">  %<span class="literal">false</span> = arith.constant <span class="literal">false</span></span><br><span class="line">  %<span class="literal">true</span> = arith.constant <span class="literal">true</span></span><br><span class="line">  %c1 = arith.constant <span class="number">1</span> : index</span><br><span class="line">  %c4 = arith.constant <span class="number">4</span> : index</span><br><span class="line">  %c10 = arith.constant <span class="number">10</span> : index</span><br><span class="line">  %c0 = arith.constant <span class="number">0</span> : index</span><br><span class="line">  %<span class="number">1</span> = <span class="string">&quot;disc_ral.dispatch&quot;</span>(%arg0, %c0) &#123;backend_config = <span class="string">&quot;&quot;</span>, call_target_name = <span class="string">&quot;ral_recv_input&quot;</span>, device = <span class="string">&quot;cpu&quot;</span>, has_side_effect = <span class="literal">false</span>&#125; : (!disc_ral.context, index) -&gt; memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %dim = memref.dim %<span class="number">1</span>, %c0 : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">2</span> = llvm.mlir.addressof @__global_const_0 : !llvm.ptr&lt;array&lt;<span class="number">43</span> x i8&gt;&gt;</span><br><span class="line">  %<span class="number">3</span> = llvm.getelementptr %<span class="number">2</span>[<span class="number">0</span>, <span class="number">0</span>] : (!llvm.ptr&lt;array&lt;<span class="number">43</span> x i8&gt;&gt;) -&gt; !llvm.ptr&lt;i8&gt;</span><br><span class="line">  %<span class="number">4</span> = llvm.inttoptr %<span class="number">0</span> : i32 to !llvm.ptr&lt;i8&gt;</span><br><span class="line">  %<span class="number">5</span> = <span class="string">&quot;disc_ral.dispatch&quot;</span>(%arg0, %<span class="number">4</span>, %<span class="number">3</span>, %c0_i32) &#123;backend_config = <span class="string">&quot;&quot;</span>, call_target_name = <span class="string">&quot;ral_const&quot;</span>, device = <span class="string">&quot;gpu&quot;</span>, has_side_effect = <span class="literal">false</span>&#125; : (!disc_ral.context, !llvm.ptr&lt;i8&gt;, !llvm.ptr&lt;i8&gt;, i32) -&gt; memref&lt;<span class="number">10</span>x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">6</span> = llvm.mlir.addressof @__global_const_1 : !llvm.ptr&lt;array&lt;<span class="number">43</span> x i8&gt;&gt;</span><br><span class="line">  %<span class="number">7</span> = llvm.getelementptr %<span class="number">6</span>[<span class="number">0</span>, <span class="number">0</span>] : (!llvm.ptr&lt;array&lt;<span class="number">43</span> x i8&gt;&gt;) -&gt; !llvm.ptr&lt;i8&gt;</span><br><span class="line">  %<span class="number">8</span> = llvm.inttoptr %<span class="number">0</span> : i32 to !llvm.ptr&lt;i8&gt;</span><br><span class="line">  %<span class="number">9</span> = <span class="string">&quot;disc_ral.dispatch&quot;</span>(%arg0, %<span class="number">8</span>, %<span class="number">7</span>, %c1_i32) &#123;backend_config = <span class="string">&quot;&quot;</span>, call_target_name = <span class="string">&quot;ral_const&quot;</span>, device = <span class="string">&quot;gpu&quot;</span>, has_side_effect = <span class="literal">false</span>&#125; : (!disc_ral.context, !llvm.ptr&lt;i8&gt;, !llvm.ptr&lt;i8&gt;, i32) -&gt; memref&lt;<span class="number">10</span>x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">10</span> = llvm.mlir.addressof @__global_const_2 : !llvm.ptr&lt;array&lt;<span class="number">40</span> x i8&gt;&gt;</span><br><span class="line">  %<span class="number">11</span> = llvm.getelementptr %<span class="number">10</span>[<span class="number">0</span>, <span class="number">0</span>] : (!llvm.ptr&lt;array&lt;<span class="number">40</span> x i8&gt;&gt;) -&gt; !llvm.ptr&lt;i8&gt;</span><br><span class="line">  %<span class="number">12</span> = llvm.inttoptr %<span class="number">0</span> : i32 to !llvm.ptr&lt;i8&gt;</span><br><span class="line">  %<span class="number">13</span> = <span class="string">&quot;disc_ral.dispatch&quot;</span>(%arg0, %<span class="number">12</span>, %<span class="number">11</span>, %c2_i32) &#123;backend_config = <span class="string">&quot;&quot;</span>, call_target_name = <span class="string">&quot;ral_const&quot;</span>, device = <span class="string">&quot;gpu&quot;</span>, has_side_effect = <span class="literal">false</span>&#125; : (!disc_ral.context, !llvm.ptr&lt;i8&gt;, !llvm.ptr&lt;i8&gt;, i32) -&gt; memref&lt;<span class="number">10</span>xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">14</span> = llvm.mlir.addressof @__global_const_3 : !llvm.ptr&lt;array&lt;<span class="number">40</span> x i8&gt;&gt;</span><br><span class="line">  %<span class="number">15</span> = llvm.getelementptr %<span class="number">14</span>[<span class="number">0</span>, <span class="number">0</span>] : (!llvm.ptr&lt;array&lt;<span class="number">40</span> x i8&gt;&gt;) -&gt; !llvm.ptr&lt;i8&gt;</span><br><span class="line">  %<span class="number">16</span> = llvm.inttoptr %<span class="number">0</span> : i32 to !llvm.ptr&lt;i8&gt;</span><br><span class="line">  %<span class="number">17</span> = <span class="string">&quot;disc_ral.dispatch&quot;</span>(%arg0, %<span class="number">16</span>, %<span class="number">15</span>, %c3_i32) &#123;backend_config = <span class="string">&quot;&quot;</span>, call_target_name = <span class="string">&quot;ral_const&quot;</span>, device = <span class="string">&quot;gpu&quot;</span>, has_side_effect = <span class="literal">false</span>&#125; : (!disc_ral.context, !llvm.ptr&lt;i8&gt;, !llvm.ptr&lt;i8&gt;, i32) -&gt; memref&lt;<span class="number">10</span>xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %<span class="keyword">reinterpret_cast</span> = memref.<span class="keyword">reinterpret_cast</span> %<span class="number">1</span> to offset: [<span class="number">0</span>], sizes: [%dim, <span class="number">10</span>], strides: [<span class="number">10</span>, <span class="number">1</span>] &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt; to memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">18</span> = llvm.inttoptr %<span class="number">0</span> : i32 to !llvm.ptr&lt;i8&gt;</span><br><span class="line">  <span class="string">&quot;disc_ral.dispatch&quot;</span>(%arg0, %<span class="number">18</span>, %<span class="keyword">reinterpret_cast</span>, %<span class="number">9</span>, %alloc, %<span class="literal">false</span>, %<span class="literal">false</span>, %<span class="literal">true</span>) &#123;backend_config = <span class="string">&quot;&quot;</span>, call_target_name = <span class="string">&quot;ral_gemm&quot;</span>, device = <span class="string">&quot;gpu&quot;</span>, has_side_effect = <span class="literal">false</span>&#125; : (!disc_ral.context, !llvm.ptr&lt;i8&gt;, memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;10x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, i1, i1, i1) -&gt; ()</span></span><br><span class="line">  memref.dealloc %<span class="number">9</span> : memref&lt;<span class="number">10</span>x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloca = memref.<span class="built_in">alloca</span>() &#123;alignment = <span class="number">64</span> : i64&#125; : memref&lt;<span class="number">2</span>xindex&gt;</span><br><span class="line">  memref.store %dim, %alloca[%c0] : memref&lt;<span class="number">2</span>xindex&gt;</span><br><span class="line">  memref.store %c10, %alloca[%c1] : memref&lt;<span class="number">2</span>xindex&gt;</span><br><span class="line">  %<span class="number">19</span> = arith.muli %dim, %c10 : index</span><br><span class="line">  %<span class="number">20</span> = arith.remui %<span class="number">19</span>, %c4 : index</span><br><span class="line">  %<span class="number">21</span> = arith.cmpi eq, %<span class="number">20</span>, %c0 : index</span><br><span class="line">  %alloc_0 = memref.<span class="built_in">alloc</span>() : memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_1 = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_2 = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_3 = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_4 = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  scf.<span class="keyword">if</span> %<span class="number">21</span> &#123;</span><br><span class="line">    <span class="string">&quot;lmhlo.fusion&quot;</span>() (&#123;</span><br><span class="line">      <span class="string">&quot;lmhlo.constant&quot;</span>(%alloc_0) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>, value = dense&lt;<span class="number">0.000000e+00</span>&gt; : tensor&lt;f32&gt;&#125; : (memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.dynamic_broadcast_in_dim&quot;</span>(%<span class="number">13</span>, %alloca, %alloc_1) &#123;broadcast_dimensions = dense&lt;<span class="number">1</span>&gt; : tensor&lt;<span class="number">1</span>xi64&gt;, disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;<span class="number">10</span>xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;2xindex&gt;</span>, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.add&quot;</span>(%alloc, %alloc_1, %alloc_2) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.dynamic_broadcast_in_dim&quot;</span>(%alloc_0, %alloca, %alloc_4) &#123;broadcast_dimensions = dense&lt;&gt; : tensor&lt;<span class="number">0</span>xi64&gt;, disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;2xindex&gt;</span>, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.maximum&quot;</span>(%alloc_2, %alloc_4, %alloc_3) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.terminator&quot;</span>() : () -&gt; ()</span><br><span class="line">    &#125;) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>, disc.fusion.name = <span class="string">&quot;main_kLoop_maximum__5_1_0&quot;</span>, disc.fusion.tag = <span class="string">&quot;Vec4&quot;</span>, disc.fusion_type = <span class="string">&quot;kLoop&quot;</span>, disc_vectorize_or_tile_hint = <span class="number">4</span> : i32&#125; : () -&gt; ()</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="string">&quot;lmhlo.fusion&quot;</span>() (&#123;</span><br><span class="line">      <span class="string">&quot;lmhlo.constant&quot;</span>(%alloc_0) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>, value = dense&lt;<span class="number">0.000000e+00</span>&gt; : tensor&lt;f32&gt;&#125; : (memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.dynamic_broadcast_in_dim&quot;</span>(%<span class="number">13</span>, %alloca, %alloc_1) &#123;broadcast_dimensions = dense&lt;<span class="number">1</span>&gt; : tensor&lt;<span class="number">1</span>xi64&gt;, disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;<span class="number">10</span>xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;2xindex&gt;</span>, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.add&quot;</span>(%alloc, %alloc_1, %alloc_2) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.dynamic_broadcast_in_dim&quot;</span>(%alloc_0, %alloca, %alloc_4) &#123;broadcast_dimensions = dense&lt;&gt; : tensor&lt;<span class="number">0</span>xi64&gt;, disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;2xindex&gt;</span>, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.maximum&quot;</span>(%alloc_2, %alloc_4, %alloc_3) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.terminator&quot;</span>() : () -&gt; ()</span><br><span class="line">    &#125;) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>, disc.fusion.name = <span class="string">&quot;main_kLoop_maximum__5_1_0&quot;</span>, disc.fusion_type = <span class="string">&quot;kLoop&quot;</span>, disc_vectorize_or_tile_hint = <span class="number">1</span> : i32&#125; : () -&gt; ()</span><br><span class="line">  &#125;</span><br><span class="line">  memref.dealloc %alloc_4 : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %alloc_2 : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %alloc_1 : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %alloc_0 : memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %alloc : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %<span class="number">13</span> : memref&lt;<span class="number">10</span>xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_5 = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">22</span> = llvm.inttoptr %<span class="number">0</span> : i32 to !llvm.ptr&lt;i8&gt;</span><br><span class="line">  <span class="string">&quot;disc_ral.dispatch&quot;</span>(%arg0, %<span class="number">22</span>, %alloc_3, %<span class="number">5</span>, %alloc_5, %<span class="literal">false</span>, %<span class="literal">false</span>, %<span class="literal">true</span>) &#123;backend_config = <span class="string">&quot;&quot;</span>, call_target_name = <span class="string">&quot;ral_gemm&quot;</span>, device = <span class="string">&quot;gpu&quot;</span>, has_side_effect = <span class="literal">false</span>&#125; : (!disc_ral.context, !llvm.ptr&lt;i8&gt;, memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;10x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, i1, i1, i1) -&gt; ()</span></span><br><span class="line">  memref.dealloc %alloc_3 : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %<span class="number">5</span> : memref&lt;<span class="number">10</span>x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_6 = memref.<span class="built_in">alloc</span>() : memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_7 = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_8 = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_9 = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  %alloc_10 = memref.<span class="built_in">alloc</span>(%dim) &#123;kDiscSymbolicDimAttr = [@S0, @C10]&#125; : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  scf.<span class="keyword">if</span> %<span class="number">21</span> &#123;</span><br><span class="line">    <span class="string">&quot;lmhlo.fusion&quot;</span>() (&#123;</span><br><span class="line">      <span class="string">&quot;lmhlo.constant&quot;</span>(%alloc_6) &#123;value = dense&lt;<span class="number">0.000000e+00</span>&gt; : tensor&lt;f32&gt;&#125; : (memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.dynamic_broadcast_in_dim&quot;</span>(%alloc_6, %alloca, %alloc_7) &#123;broadcast_dimensions = dense&lt;&gt; : tensor&lt;<span class="number">0</span>xi64&gt;, disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;2xindex&gt;</span>, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.dynamic_broadcast_in_dim&quot;</span>(%<span class="number">17</span>, %alloca, %alloc_8) &#123;broadcast_dimensions = dense&lt;<span class="number">1</span>&gt; : tensor&lt;<span class="number">1</span>xi64&gt;, disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;<span class="number">10</span>xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;2xindex&gt;</span>, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.add&quot;</span>(%alloc_5, %alloc_8, %alloc_9) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.maximum&quot;</span>(%alloc_9, %alloc_7, %alloc_10) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.terminator&quot;</span>() : () -&gt; ()</span><br><span class="line">    &#125;) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>, disc.fusion.name = <span class="string">&quot;main_kLoop_maximum__5_1_1&quot;</span>, disc.fusion.tag = <span class="string">&quot;Vec4&quot;</span>, disc.fusion_type = <span class="string">&quot;kLoop&quot;</span>, disc_vectorize_or_tile_hint = <span class="number">4</span> : i32&#125; : () -&gt; ()</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="string">&quot;lmhlo.fusion&quot;</span>() (&#123;</span><br><span class="line">      <span class="string">&quot;lmhlo.constant&quot;</span>(%alloc_6) &#123;value = dense&lt;<span class="number">0.000000e+00</span>&gt; : tensor&lt;f32&gt;&#125; : (memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.dynamic_broadcast_in_dim&quot;</span>(%alloc_6, %alloca, %alloc_7) &#123;broadcast_dimensions = dense&lt;&gt; : tensor&lt;<span class="number">0</span>xi64&gt;, disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;2xindex&gt;</span>, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.dynamic_broadcast_in_dim&quot;</span>(%<span class="number">17</span>, %alloca, %alloc_8) &#123;broadcast_dimensions = dense&lt;<span class="number">1</span>&gt; : tensor&lt;<span class="number">1</span>xi64&gt;, disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;<span class="number">10</span>xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;2xindex&gt;</span>, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.add&quot;</span>(%alloc_5, %alloc_8, %alloc_9) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.maximum&quot;</span>(%alloc_9, %alloc_7, %alloc_10) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>&#125; : (memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;, memref<span class="string">&lt;?x10xf32, #gpu.address_space&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">      <span class="string">&quot;lmhlo.terminator&quot;</span>() : () -&gt; ()</span><br><span class="line">    &#125;) &#123;disc.device = <span class="string">&quot;gpu&quot;</span>, disc.fusion.name = <span class="string">&quot;main_kLoop_maximum__5_1_1&quot;</span>, disc.fusion_type = <span class="string">&quot;kLoop&quot;</span>, disc_vectorize_or_tile_hint = <span class="number">1</span> : i32&#125; : () -&gt; ()</span><br><span class="line">  &#125;</span><br><span class="line">  memref.dealloc %alloc_9 : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %alloc_8 : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %alloc_7 : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %alloc_6 : memref&lt;f32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %alloc_5 : memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  memref.dealloc %<span class="number">17</span> : memref&lt;<span class="number">10</span>xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;</span></span><br><span class="line">  <span class="string">&quot;disc_ral.dispatch&quot;</span>(%arg0, %c0, %alloc_10) &#123;backend_config = <span class="string">&quot;&quot;</span>, call_target_name = <span class="string">&quot;ral_send_output&quot;</span>, device = <span class="string">&quot;cpu&quot;</span>, has_side_effect = <span class="literal">false</span>&#125; : (!disc_ral.context, index, memref&lt;?x10xf32, <span class="meta">#gpu.address_space<span class="string">&lt;global&gt;</span>&gt;) -&gt; ()</span></span><br><span class="line">  <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如上是整个代码段，该IR代码实现了一个包含两个全连接层（矩阵乘法）和ReLU激活函数的神经网络前向计算流程。具体数学表达式为：<code>output = ReLU(ReLU(input * W1 + b1) * W2 + b2)</code>。同时由于BladeDISC的编译speculation，runtime选择机制，针对是否可以向量化（&lt;?x10&gt;的维度是否可以乘除4），做了多版本代码生成。</p>
</li>
</ul>
<table>
<thead>
<tr>
<th align="center">Pass名称</th>
<th align="center">功能描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>DiscLhloLegalizeRootsToParallelLoopsPass</code></td>
<td align="center">将LMHLO根操作转换为并行循环</td>
</tr>
<tr>
<td align="center"><code>InputInlineFusionPass</code></td>
<td align="center">内联融合生产者操作到循环中</td>
</tr>
</tbody></table>
<h4 id="2-后端特定优化"><a href="#2-后端特定优化" class="headerlink" title="(2) 后端特定优化"></a><strong>(2) 后端特定优化</strong></h4><table>
<thead>
<tr>
<th align="center">Pass名称</th>
<th align="center">功能描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>DiscLowerGpuOpsToNVVMOpsPass</code></td>
<td align="center">将GPU操作转换为NVIDIA CUDA后端操作</td>
</tr>
<tr>
<td align="center"><code>DiscLowerGpuOpsToROCDLOpsPass</code></td>
<td align="center">将GPU操作转换为AMD ROCm后端操作</td>
</tr>
<tr>
<td align="center"><code>DiscOutlineCpuKernelPass</code></td>
<td align="center">生成CPU多线程内核包装函数</td>
</tr>
</tbody></table>
<h4 id="3-内存访问优化"><a href="#3-内存访问优化" class="headerlink" title="(3) 内存访问优化"></a><strong>(3) 内存访问优化</strong></h4><table>
<thead>
<tr>
<th align="center">Pass名称</th>
<th align="center">功能描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>DiscFlattenMemrefAccessPass</code></td>
<td align="center">扁平化内存访问模式</td>
</tr>
<tr>
<td align="center"><code>DiscMemRefCSEPass</code></td>
<td align="center">消除冗余内存访问</td>
</tr>
</tbody></table>
<hr>
<h3 id="7-GPU模块到二进制阶段"><a href="#7-GPU模块到二进制阶段" class="headerlink" title="7. GPU模块到二进制阶段"></a><strong>7. GPU模块到二进制阶段</strong></h3><table>
<thead>
<tr>
<th align="center">Pass名称</th>
<th align="center">功能描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>GpuKernelOutlining</code></td>
<td align="center">将<code>gpu.launch</code>分离为<code>gpu.launch_func</code>和<code>gpu.module</code></td>
</tr>
<tr>
<td align="center"><code>GpuKernelToBlobPass</code></td>
<td align="center">将LLVM IR编译为GPU二进制（CUBIN&#x2F;HSACO）</td>
</tr>
<tr>
<td align="center"><code>ReviseGpuKernelOutliningPass</code></td>
<td align="center">处理主机-设备内存参数传递</td>
</tr>
</tbody></table>
<hr>
<h3 id="8-主机侧编译阶段"><a href="#8-主机侧编译阶段" class="headerlink" title="8. 主机侧编译阶段"></a><strong>8. 主机侧编译阶段</strong></h3><table>
<thead>
<tr>
<th align="center">Pass名称</th>
<th align="center">功能描述</th>
</tr>
</thead>
<tbody><tr>
<td align="center"><code>DiscToLLVMPass</code></td>
<td align="center">将操作最终转换为LLVM Dialect</td>
</tr>
<tr>
<td align="center"><code>DiscCpuMapParallelLoopPass</code></td>
<td align="center">将<code>scf.parallel</code>映射到CPU多线程执行</td>
</tr>
</tbody></table>
<hr>
<h3 id="关键特征总结"><a href="#关键特征总结" class="headerlink" title="关键特征总结"></a><strong>关键特征总结</strong></h3><table>
<thead>
<tr>
<th align="center"><strong>阶段</strong></th>
<th align="center"><strong>核心目标</strong></th>
<th align="center"><strong>关键技术</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center">TF-to-HLO</td>
<td align="center">统一前端语义</td>
<td align="center">静态秩约束处理</td>
</tr>
<tr>
<td align="center">HLO优化</td>
<td align="center">形状传播与融合</td>
<td align="center">代数简化与约束分析</td>
</tr>
<tr>
<td align="center">缓冲化</td>
<td align="center">内存管理</td>
<td align="center">显式内存空间分配</td>
</tr>
<tr>
<td align="center">运行时集成</td>
<td align="center">跨平台抽象</td>
<td align="center">RAL上下文隔离</td>
</tr>
<tr>
<td align="center">代码生成</td>
<td align="center">动态形状代码生成</td>
<td align="center">推测多版本内核生成</td>
</tr>
<tr>
<td align="center">后端适配</td>
<td align="center">硬件优化</td>
<td align="center">GPU循环映射与CPU多线程</td>
</tr>
</tbody></table>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color = brown>参考资料</font></h2><ol>
<li><a href="https://github.com/alibaba/BladeDISC/blob/main/docs/developers/pass_pipeline.md#gpu-module-to-cubin">BladeDISC pass pipeline</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/622562160">XLA解读</a></li>
</ol>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>机器学习编译</category>
        <category>动态shape</category>
      </categories>
      <tags>
        <tag>机器学习编译器</tag>
        <tag>mlir</tag>
        <tag>动态shape</tag>
      </tags>
  </entry>
  <entry>
    <title>Allo项目初探</title>
    <url>/2025/03/29/Allo%E9%A1%B9%E7%9B%AE%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<p><img src="/images/image-20250330205827771.png" alt="image-20250330205827771"></p>
<span id="more"></span>

<h2 id="项目构建"><a href="#项目构建" class="headerlink" title="项目构建"></a><font color = brown>项目构建</font></h2><p>主要参考<a href="https://cornell-zhang.github.io/allo/setup/index.html">Allo doc</a>的set up章节，这里我选择源码构建的方式。项目构建流程如下：</p>
<ol>
<li><p>clone项目</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone --recursive git@github.com:micropuma/allo.git</span><br></pre></td></tr></table></figure>
</li>
<li><p>构建llvm子项目</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd allo/externals/llvm-project</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Apply our patch</span></span><br><span class="line">git apply ../llvm_patch</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">Python 3.12 is required</span></span><br><span class="line">mkdir -p build</span><br></pre></td></tr></table></figure>

<p>由于需要python bind11，以及后续需要pip install很多python库，因此这里推荐用conda创建虚拟环境：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">conda create -n allo python=3.12         # python版本一定是3.12</span><br><span class="line">conda activate allo</span><br><span class="line">pip install pybind11</span><br></pre></td></tr></table></figure>

<p>编写shell脚本，一键构建llvm：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd build</span><br><span class="line">cmake -G Ninja ../llvm \</span><br><span class="line">    -DLLVM_ENABLE_PROJECTS=&quot;clang;mlir;openmp&quot; \</span><br><span class="line">    -DLLVM_BUILD_EXAMPLES=ON \</span><br><span class="line">    -DLLVM_TARGETS_TO_BUILD=&quot;host&quot; \</span><br><span class="line">    -DCMAKE_BUILD_TYPE=Release \</span><br><span class="line">    -DLLVM_ENABLE_ASSERTIONS=ON \</span><br><span class="line">    -DLLVM_INSTALL_UTILS=ON \</span><br><span class="line">    -DMLIR_ENABLE_BINDINGS_PYTHON=ON \</span><br><span class="line">    -DPython3_EXECUTABLE=`which python3`</span><br><span class="line">ninja -j $(nproc)</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">export</span> environment variable</span></span><br><span class="line">export LLVM_BUILD_DIR=$(pwd)</span><br></pre></td></tr></table></figure>
</li>
<li><p>构建Allo项目，首先回退到项目根目录，运行如下命令：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">python3 -m pip install -v -e . -i https://pypi.tuna.tsinghua.edu.cn/simple  # 建议用清华镜像，否则容易拉取失败</span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="Allo项目使用"><a href="#Allo项目使用" class="headerlink" title="Allo项目使用"></a><font color = brown>Allo项目使用</font></h2><h3 id="Pytorch集成例子"><a href="#Pytorch集成例子" class="headerlink" title="Pytorch集成例子"></a><font color = green>Pytorch集成例子</font></h3><p>Allo项目除了使用ADL（加速定义语言）来描述计算任务外，也可以集成在pytorch中使用。具体参考<a href="https://cornell-zhang.github.io/allo/dive/pytorch.html">allo pytorch集成例子</a>。</p>
<p>具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        x = x + y</span><br><span class="line">        x = F.relu(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = Model()</span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> allo</span><br><span class="line">example_inputs = [torch.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">10</span>), torch.rand(<span class="number">1</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">10</span>)]</span><br><span class="line">llvm_mod = allo.frontend.from_pytorch(model, example_inputs=example_inputs)</span><br><span class="line"></span><br><span class="line">golden = model(*example_inputs)</span><br><span class="line">np_inputs = [x.detach().numpy() <span class="keyword">for</span> x <span class="keyword">in</span> example_inputs]</span><br><span class="line">res = llvm_mod(*np_inputs)</span><br><span class="line">torch.testing.assert_close(res, golden.detach().numpy())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Passed!&quot;</span>)</span><br><span class="line"></span><br><span class="line">mod = allo.frontend.from_pytorch(model, example_inputs=example_inputs, target=<span class="string">&quot;vhls&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(mod.hls_code)</span><br></pre></td></tr></table></figure>

<p>执行如下scirpt：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libstdc++.so.6 python test.py</span><br></pre></td></tr></table></figure>

<blockquote>
<p>:key:注意，由于conda环境中的libstdc++版本低，可能会在import allo的时候报错。强制指定系统libstdc++版本即可。</p>
</blockquote>
<h3 id="Allo-AIE集成"><a href="#Allo-AIE集成" class="headerlink" title="Allo AIE集成"></a><font color = green>Allo AIE集成</font></h3><p>Allo项目目前可以集成AIE后端了，目前只局限于aie2（NPU）。该流程是端到端的编译流程，支持从pytorch或是huggingface上的模型lower到aie后端，并执行。具体流程如下：</p>
<p><img src="/images/image-20250407111644037.png" alt="image-20250407111644037"></p>
<p>目前存在的流程问题：</p>
<ul>
<li><p>Pytorch只能target到cpu和fpga上，目前没有支持pytorch到aie的端到端流程。</p>
<p><img src="/images/image-20250407124451930.png" alt="image-20250407124451930"></p>
</li>
<li><p>对于aie，目前allo支持用python编写算子，并且只能支持aie2（NPU），没有支持VCK190。</p>
<p><img src="/images/image-20250407124501753.png" alt="image-20250407124501753"></p>
</li>
<li><p>Pytorch算子支持受局限。</p>
<p><img src="/images/image-20250407124509106.png" alt="image-20250407124509106"></p>
</li>
</ul>
<h2 id="Allo项目架构"><a href="#Allo项目架构" class="headerlink" title="Allo项目架构"></a><font color = brown>Allo项目架构</font></h2><p><img src="/images/image-20250331184126715.png" alt="image-20250331184126715"></p>
<h3 id="前端"><a href="#前端" class="headerlink" title="前端"></a><font color = green>前端</font></h3><p><code>Allo</code>项目在前端使用python语言编写，主要完成<strong>python系统</strong>和<strong>pytorch系统</strong>到mlir系统的接入。针对pytorch系统，用于可以使用<code>torch.compile(model, &quot;allo&quot;)</code>将pytorch代码编译成allo的中间表达形式，借助于TorchDynamo系统。具体的，基于<code>torch.fx</code>作为高层ir，将其中的每一个pytorch计算单元翻译成allo的function call，后续转换和优化交由allo来完成。硬件无关优化比如算子融合在torch系统中完成，而硬件相关优化则由allo完成，两者是正交关系。</p>
<h2 id="代码生成"><a href="#代码生成" class="headerlink" title="代码生成"></a><font color = green>代码生成</font></h2><p>用户通过<code>s.build(&lt;target&gt;)</code>来针对指定硬件生成代码。目前支持的后端硬件有：CPU，AMD的fpga以及AMD AIE（Ryzen AI）。</p>
<h2 id="中间优化"><a href="#中间优化" class="headerlink" title="中间优化"></a><font color = green>中间优化</font></h2><p>这一部分完全在mlir系统中完成所有的优化。</p>
<blockquote>
<p>Allo项目的框架是比较简单的，后续会结合源代码详细解读。</p>
</blockquote>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color = brown>参考资料</font></h2><ol>
<li><a href="https://cornell-zhang.github.io/allo/">Allo doc</a></li>
<li><a href="http://arxiv.org/abs/2404.04815">Allo paper</a></li>
<li><a href="https://github.com/cornell-zhang/allo">Allo project</a></li>
</ol>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>机器学习编译</category>
        <category>异构设计</category>
      </categories>
      <tags>
        <tag>机器学习编译器</tag>
        <tag>mlir</tag>
        <tag>异构计算系统</tag>
      </tags>
  </entry>
  <entry>
    <title>GPU 后端优化（一）</title>
    <url>/2025/05/11/GPU-%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    <content><![CDATA[<p><img src="/images/image-20250511230828794.png" alt="image-20250511230828794"></p>
<span id="more"></span>

<blockquote>
<p>为什么会有GPU后端优化博客？在阅读BladeDISC以及IREE等源码的过程中，发现当逐步接触后端代码生成&#x2F;运行时系统调度时，由于本人没有深入接触过GPU体系架构（基础太差），很多优化pass难以理解。这一过程使我认识到对于底层系统架构的深入理解往往比具体的编译技术，编译框架（MLIR&#x2F;LLVM）而言更为重要。本系列主要记录我补习GPU架构知识的学习历程（开个坑，主要用于激励我自学CUDA，不然太摆了）。目前初步的计划，该系列会有如下几篇博客：</p>
<ol>
<li>GPU后端优化（一）：主要包括GPU后端优化基础知识</li>
<li>GPU后端优化（二）：机器学习编译实战部分，以<a href="http://arxiv.org/abs/2108.13191">MLIR做GPU代码生成</a>论文为概览，结合IREE和BladeDISC开源代码分析</li>
<li>GPU后端优化（三）：结合<a href="https://siboehm.com/articles/22/CUDA-MMM">CUDA算子优化博客</a>，学习CUDA算子开发过程和对应算子编译手段</li>
<li>GPU后端优化（四）：高阶GPU体系结构，结合GPU架构书籍进行解读</li>
</ol>
</blockquote>
<h2 id="GPU后端优化基础知识"><a href="#GPU后端优化基础知识" class="headerlink" title="GPU后端优化基础知识"></a><font color = brown>GPU后端优化基础知识</font></h2><p>在日常学习中，由于我的工作和编译器关联度较大，因此一般都是在阅读开源项目的pass源码。这其中如果涉及GPU相关的优化pass，遇到不懂的可能直接询问chatgpt或是一些博客寻找答案。久而久之发现自己虽然好像能够读懂编译器的后端生成pass，但往深处探讨，为什么这些pass是这样的顺序，为什么GPU需要这些pass，很多问题其实都回答不上来。归根结底，是对于GPU优化的原理知识以及优化之间的关联细节没有真正去仔细思考和学习。</p>
<p>印度理工的学者的一篇利用MLIR搭建高性能GPU算子自动生成的文章，是我们开始的很好的起点。这篇<a href="http://arxiv.org/abs/2108.13191">文章</a>完全基于MLIR基础设施，并做少量修改，同时是一个完整的GPU优化生成流程，是十分值得学习的。本章节的目的就是尽量扫盲，使一些原本不那么清晰或是没涉及过的优化概念尽量明晰，为读懂该文章做好准备工作。</p>
<p><img src="/images/image-20250511233238659.png" alt="image-20250511233238659"></p>
<p>如上图所示是论文中的GPU算子生成流水线，也是我们补足GPU后端编译优化基础知识的概览。后续内容按照流水线展开，大体内容划分如下：</p>
<ul>
<li>Loop Tiling优化</li>
<li>存储优化（共享内存开辟，padding，coalesce内存）</li>
<li>tensor core适配（WMMA api）</li>
<li>global memory latency hiding</li>
<li>GPU流水线技术</li>
</ul>
<h3 id="Loop-Tiling优化"><a href="#Loop-Tiling优化" class="headerlink" title="Loop Tiling优化"></a><font color = green>Loop Tiling优化</font></h3><p>这一章节主要参考<a href="https://zhuanlan.zhihu.com/p/477023757">一篇文章了解 Loop Tiling 优化</a>，这篇博客讲解非常到位，强烈建议有兴趣的读者仔细阅读。本篇文章的解读顺序和原博客不太一致，选择先列出tiling算法步骤，然后以一个具体例子带着大家走一遍算法步骤，解读每一步原理。</p>
<h4 id="Tiling算法步骤"><a href="#Tiling算法步骤" class="headerlink" title="Tiling算法步骤"></a>Tiling算法步骤</h4><blockquote>
<p>算法：</p>
<p>设总共有 <code>N</code> 层循环，从外向内分别是 <code>0,1,...,N-1</code></p>
<ul>
<li><strong>从内向外</strong>，逐步分析 <code>N-2,N-3,...,0</code>（先排除掉最内层循环）</li>
<li>对于每层循环 <code>L</code> ，<strong>看其 <code>idx=0,1,...</code> 时，是否有 array 可能被反复读取</strong>。<ul>
<li>若有，则 tile 第 <code>L+1</code> 层循环。</li>
</ul>
</li>
</ul>
<p>都做完之后，再分析 <strong>基于新循环是否仍然有可 tile 的部分</strong></p>
</blockquote>
<p>上述算法初看比较抽象，选择通过一个矩阵乘法例子来走一遍该规则，以简化算法理解。</p>
<p>首先我们要tiling的计算任务是NxNxN的矩阵乘（现实场景由于矩阵的维度，cache的维度有很多dirty work和边界trick，这里默认矩阵是“完美的”）。伪代码如下图所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, N):            <span class="comment"># i,k,j，访问符合行优先原则</span></span><br><span class="line">  <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, N): </span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, N):</span><br><span class="line">      C[i][j] += A[i][k] * B[k][j]</span><br></pre></td></tr></table></figure>

<p>根据算法描述，首先分析k维度遍历过程中是否有array反复读取（k维度对应N-2维度）。发现<code>c[i][j]</code>的遍历和k维度无关，因此可以tile j这个维度，得到如下伪代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> j_o <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, N, T_j):                           <span class="comment"># j_i 对应j维度的外层维度，从0到N，步长为T</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, N):</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, N):</span><br><span class="line">            <span class="keyword">for</span> j_i <span class="keyword">in</span> <span class="built_in">range</span>(j_o, j_o+T_j):            <span class="comment"># j_o 对应j维度的内层循环</span></span><br><span class="line">                c[i][j_i] += A[i][k] * B[k][j_i]</span><br></pre></td></tr></table></figure>

<p>分析tiling后伪代码，通过选取合适的T（适配L1缓存大小），则<code>c[i][j_i]</code>可以做到在k迭代过程中一直缓存在cache中。继续我们的tiling算法，该尝试k的上一层维度，i维度。发现<code>B[k][j_i]</code>在遍历过程中和i的值无关，因此可以对下一层，即k做优化，使得<code>B[k][j_i]</code>适配缓存。如下是伪代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> k_o <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, N, T_k):</span><br><span class="line">  <span class="keyword">for</span> j_o <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, N, T_j):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, N):</span><br><span class="line">      <span class="keyword">for</span> k_i <span class="keyword">in</span> <span class="built_in">range</span>(k_o, k_o+T_k):</span><br><span class="line">        <span class="keyword">for</span> j_i <span class="keyword">in</span> <span class="built_in">range</span>(j_o, j_o+T_j):</span><br><span class="line">          C[i][j_i] += A[i][k_i] * B[k_i][j_i]</span><br><span class="line"><span class="comment"># 这样，k_i=0,1... 时，C[i_i][j] 始终在 cache 中；i=0,1.. 时，B[k_i][j_i] 也始终在 cache 中</span></span><br></pre></td></tr></table></figure>

<p>至此，走完算法的第一部分流程，来到第二部分：需要分析 <strong>基于新循环是否仍然有可 tile 的部分</strong>。这部分的逻辑是，对于k维度和j维度做的tiling分块，对于最顶层（初始的顶层）i维度产生影响，目前i维度已不是顶层维度。i维度之上的j_o遍历过程中，下一层i的遍历跨度是N，比较大，是难以存入cache中的，因此需要对i也做tiling优化。伪代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 又因为循环 i 从最外层循环变成了内循环，对于其上层循环 j_o 来说，当 j_o=0,1,...时，A[i][k_i] 是被反复读取的，因此也可以 tile 循环 i</span></span><br><span class="line"><span class="keyword">for</span> i_o <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, N, T_i):</span><br><span class="line">  <span class="keyword">for</span> k_o <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, N, T_k):</span><br><span class="line">    <span class="keyword">for</span> j_o <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, N, T_j):</span><br><span class="line">      <span class="keyword">for</span> i_i <span class="keyword">in</span> <span class="built_in">range</span>(i_o, i_o + T_i):</span><br><span class="line">        <span class="keyword">for</span> k_i <span class="keyword">in</span> <span class="built_in">range</span>(k_o, k_o + T_k):</span><br><span class="line">          <span class="keyword">for</span> j_i <span class="keyword">in</span> <span class="built_in">range</span>(j_o, j_o + T_j):</span><br><span class="line">            C[i_i][j_i] += A[i_i][k_i] * B[k_i][j_i]</span><br><span class="line"><span class="comment"># 注意，内三层循环仍然是按照 i_i,k_i,j_i 的顺序，因此仍然满足 RowMajor 的顺序读取</span></span><br></pre></td></tr></table></figure>

<p>至此，完成L1的tiling优化。可以看到，目前我的变换出的代码，在i_i，k_i和j_i维度都已经是tiling分块的。</p>
<p>结束了？Too Naive！！！:fire:</p>
<p>现代体系结构（从CPU到GPU到ASIC），均有多层级缓存体系。让我们继续研究一下代码，思考一下如果有L2 cache，该如何继续tiling？目前的矩阵乘运算已经适配L1缓存，可以看到j_o维度的循环，已经是完美缓存的。但是对于k_o循环而言，下一层j_o循环则成了严重的性能瓶颈。j_o循环总步长为N，这之间又是大量的cache miss。这一部分就需要L2缓存来加速存储交互了。继续我们的算法，以k_o维度为新的起点<strong>由内向外</strong>，分块的大小按照L2缓存来，继续一轮迭代，最终的结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> j_oo <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, N, T2_j):</span><br><span class="line">  <span class="keyword">for</span> i_o <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, N, T_i):</span><br><span class="line">    <span class="keyword">for</span> k_o <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, N, T_k):</span><br><span class="line">      <span class="keyword">for</span> j_oi <span class="keyword">in</span> <span class="built_in">range</span>(j_oo, j_oo + T2_j, T_j):</span><br><span class="line">        <span class="keyword">for</span> i_i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, N, T_i):</span><br><span class="line">          <span class="keyword">for</span> k_i <span class="keyword">in</span> <span class="built_in">range</span>(k_o, k_o + T_k):</span><br><span class="line">            <span class="keyword">for</span> j_i <span class="keyword">in</span> <span class="built_in">range</span>(j_oi, j_oi + T_j):</span><br><span class="line">              C[i_i][j_i] += A[i_i][k_i] * B[k_i][j_i]</span><br><span class="line"><span class="comment"># 这样的话，内三层循环 i_i, k_i, j_i （对应的 Tile size 为 T_i, T_j, T_k）放到 L1 Cache 里。</span></span><br><span class="line"><span class="comment"># 外层循环 j_oi （对应Tile size 为 T2_j） 就可以放到更大的 L2 Cache 里。于是便在 L2 里形成了对 C[i_i][j_i] 的复用</span></span><br></pre></td></tr></table></figure>

<p>同理，可以继续向上，做k_o和i_o的分块，不再详细阐述了。</p>
<h3 id="存储优化"><a href="#存储优化" class="headerlink" title="存储优化"></a><font color = green>存储优化</font></h3><p>这部分文章重点参考<a href="https://zhuanlan.zhihu.com/p/473133201">GPU global memory优化</a>，<a href="https://zhuanlan.zhihu.com/p/659142274">Bank Conflict优化博客</a>。</p>
<h4 id="Bank-Conflict"><a href="#Bank-Conflict" class="headerlink" title="Bank Conflict"></a>Bank Conflict</h4><p>参考<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html?highlight=bank#shared-memory-5-x">CUDA shared memory资料</a>。</p>
<blockquote>
<p>Shared memory has 32 banks that are organized such that successive 32-bit words map to successive banks. Each bank has a bandwidth of 32 bits per clock cycle.</p>
<p>A shared memory request for a warp does not generate a bank conflict between two threads that access any address within the same 32-bit word (even though the two addresses fall in the same bank). In that case, for read accesses, the word is broadcast to the requesting threads and for write accesses, each address is written by only one of the threads (which thread performs the write is undefined).</p>
</blockquote>
<blockquote>
<p>To achieve high memory bandwidth for concurrent accesses, shared memory is divided into equally sized memory modules (banks) that can be accessed simultaneously. Therefore, any memory load or store of n addresses that spans b distinct memory banks can be serviced simultaneously, yielding an effective bandwidth that is b times as high as the bandwidth of a single bank.<br>																		— 《Using Shared Memory in CUDA C&#x2F;C++》</p>
</blockquote>
<blockquote>
<p>However, if multiple threads’ requested addresses map to the same memory bank, the accesses are serialized. The hardware splits a conflicting memory request into as many separate conflict-free requests as necessary, decreasing the effective bandwidth by a factor equal to the number of colliding memory requests. An exception is the case where all threads in a warp address the same shared memory address, resulting in a broadcast. Devices of compute capability 2.0 and higher have the additional ability to multicast shared memory accesses, meaning that multiple accesses to the same location by any number of threads within a warp are served simultaneously.<br>																		— 《Using Shared Memory in CUDA C&#x2F;C++》</p>
</blockquote>
<blockquote>
<p>However, if multiple threads’ requested addresses map to the same memory bank, the accesses are serialized. The hardware splits a conflicting memory request into as many separate conflict-free requests as necessary, decreasing the effective bandwidth by a factor equal to the number of colliding memory requests. An exception is the case where all threads in a warp address the same shared memory address, resulting in a broadcast. Devices of compute capability 2.0 and higher have the additional ability to multicast shared memory accesses, meaning that multiple accesses to the same location by any number of threads within a warp are served simultaneously.<br>																		— 《Using Shared Memory in CUDA C&#x2F;C++》</p>
</blockquote>
<blockquote>
<ul>
<li>多个线程读同一个数据时，仅有一个线程读，然后broadcast到其他线程</li>
<li>多个线程写同一个数据时，仅会有一个线程写成功（不过这里没有提及是否会将写操作执行多次（即a. 多个线程写入，最后一个线程随机写完; or b. 随机挑选一个线程执行写入），具体流程存疑）</li>
</ul>
</blockquote>
<h3 id="GPU流水线技术"><a href="#GPU流水线技术" class="headerlink" title="GPU流水线技术"></a><font color = green>GPU流水线技术</font></h3><h2 id="CUDA编译流程"><a href="#CUDA编译流程" class="headerlink" title="CUDA编译流程"></a><font color = brown>CUDA编译流程</font></h2><p>前面几小节按照MLIR优化GEMM算子论文中的管线（pipeline），补充了面向GPU的相应优化技术点。这一小节则讲解在cuda中是如何做代码生成，讲解一个大概的流程框架。这个流程可以分为两部分来讲解，一是最常用的英伟达闭源工具nvcc编译器的编译流程，二是基于MLIR基础设施的编译流程，从mlir系统接入LLVM IR，然后借助于LLVM成熟编译器进行编译生成。</p>
<h3 id="NVCC编译流程"><a href="#NVCC编译流程" class="headerlink" title="NVCC编译流程"></a><font color = green>NVCC编译流程</font></h3><img src="/images/image-20250512174213715.png" alt="image-20250512174213715" style="zoom:50%;" />

<h3 id="NVVM-IR"><a href="#NVVM-IR" class="headerlink" title="NVVM IR"></a><font color = green>NVVM IR</font></h3><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color = brown>参考资料</font></h2><ol>
<li><a href="https://zhuanlan.zhihu.com/p/477023757">博客：Loop Tiling优化</a></li>
<li><a href="https://siboehm.com/articles/22/CUDA-MMM">博客：CUDA手写算子教程</a></li>
<li><a href="http://arxiv.org/abs/2108.13191">论文：MLIR 优化GEMM算子</a></li>
<li><a href="https://www.bilibili.com/video/BV1HgzuYvEXT/?spm_id_from=333.1387.upload.video_card.click&vd_source=7cc24e214309f4db17f1dda017fc6683">CUDA课程</a></li>
</ol>
]]></content>
      <categories>
        <category>GPU</category>
        <category>编译</category>
      </categories>
      <tags>
        <tag>GPU</tag>
        <tag>编译</tag>
      </tags>
  </entry>
  <entry>
    <title>ByteIR初探：Runtime系统设计与实现</title>
    <url>/2025/05/08/ByteIR%E5%88%9D%E6%8E%A2%EF%BC%9ARuntime%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>ByteIR 初探</title>
    <url>/2025/05/06/ByteIR-%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<p><img src="/images/image-20250506220408172.png" alt="image-20250506220408172"></p>
<span id="more"></span>

<h2 id="Why-ByteIR？"><a href="#Why-ByteIR？" class="headerlink" title="Why ByteIR？"></a><font color = brown>Why ByteIR？</font></h2><p>看了这么多端到端机器学习编译器（IREE，BladeDISC），为什么选择研究一下ByteIR？这个<a href="https://github.com/bytedance/byteir/issues/9">ByteIR issue9</a>的回答很好地指明了ByteIR相比成熟端到端编译器的区别：</p>
<ul>
<li>ByteIR有十分清晰的Frontend，Compiler和runtime，而且三者可以分离编译构建。</li>
<li>ByteIR在字节内部不同的开发人员都在使用。</li>
<li>ByteIR是十分简洁的，单纯的是一堆passes集合，辅助开发者完成端到端流程。</li>
</ul>
<p>上述三个特点，使得ByteIR十分适合针对性学习。</p>
<h2 id="源码构建"><a href="#源码构建" class="headerlink" title="源码构建"></a><font color = brown>源码构建</font></h2><h3 id="Frontend构建"><a href="#Frontend构建" class="headerlink" title="Frontend构建"></a><font color = green>Frontend构建</font></h3><p>ByteIR的前端支持pytorch，onnx和tensorflow三种框架。其流程都是将框架接入stableHLO再合法化到MHLO 方言中。笔者先前构建过torch-mlir和iree等项目，原本想省去构建前端，复用torch-mlir的前端，但是</p>
<h3 id="Compiler部分构建"><a href="#Compiler部分构建" class="headerlink" title="Compiler部分构建"></a><font color = green>Compiler部分构建</font></h3><ol>
<li><p>clone项目</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git clone git@github.com:bytedance/byteir.git</span><br></pre></td></tr></table></figure>
</li>
<li><p>构建虚拟环境</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">构建虚拟环境</span></span><br><span class="line">pip install pybind11</span><br><span class="line">pip install nanobind</span><br><span class="line">pip install numpy</span><br><span class="line">pip install lit</span><br><span class="line">pip install filecheck</span><br></pre></td></tr></table></figure>
</li>
<li><p>Build LLVM</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cd /path_to_byteir</span><br><span class="line">git submodule update --init external/llvm-project</span><br></pre></td></tr></table></figure>

<p>构建脚本如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">build llvm</span></span><br><span class="line">cd external/llvm-project</span><br><span class="line">cmake -H./llvm \</span><br><span class="line">      -B./build \</span><br><span class="line">      -GNinja \</span><br><span class="line">      -DLLVM_ENABLE_PROJECTS=mlir \</span><br><span class="line">      -DLLVM_TARGETS_TO_BUILD=&quot;X86;NVPTX&quot; \</span><br><span class="line">      -DCMAKE_BUILD_TYPE=Release \</span><br><span class="line">      -DLLVM_ENABLE_ASSERTIONS=ON \</span><br><span class="line">      -DLLVM_INSTALL_UTILS=ON \</span><br><span class="line">      -DLLVM_CCACHE_BUILD=OFF \</span><br><span class="line">      -DLLVM_ENABLE_TERMINFO=OFF \</span><br><span class="line">      -DMLIR_ENABLE_BINDINGS_PYTHON=ON \</span><br><span class="line">      -DCMAKE_INSTALL_PREFIX=$(pwd)/build/install</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">via -DCMAKE_C_COMPILER=gcc/clang and -DCMAKE_CXX_COMPILER=g++/clang++</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">to specify gcc&gt;=8.5 or clang&gt;=7</span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="keyword">for</span> Mac <span class="built_in">users</span>, <span class="built_in">set</span> -DLLVM_TARGETS_TO_BUILD=<span class="string">&quot;AArch64;NVPTX&quot;</span></span></span><br><span class="line"></span><br><span class="line">cmake --build ./build --target all --target install</span><br></pre></td></tr></table></figure>
</li>
<li><p>Build ByteIR compiler</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cmake -B./compiler/build \</span><br><span class="line">      -H./compiler/cmake \</span><br><span class="line">      -GNinja \</span><br><span class="line">      -DCMAKE_BUILD_TYPE=Release \</span><br><span class="line">      -DPython3_EXECUTABLE=/mnt/home/douliyang/mlir-workspace/byteir/venv/bin/python \</span><br><span class="line">      -DLLVM_INSTALL_PATH=$(pwd)/external/llvm-project/build/install \</span><br><span class="line">      -DLLVM_EXTERNAL_LIT=/mnt/home/douliyang/mlir-workspace/byteir/venv/bin/lit \</span><br><span class="line">      -Dpybind11_DIR=/mnt/home/douliyang/mlir-workspace/byteir/venv/lib/python3.11/site-packages/pybind11/share/cmake/pybind11 \</span><br><span class="line">      -Dnanobind_DIR=/mnt/home/douliyang/mlir-workspace/byteir/venv/lib/python3.11/site-packages/nanobind/cmake \</span><br><span class="line">      -DBYTEIR_ENABLE_BINDINGS_PYTHON=ON</span><br><span class="line"></span><br><span class="line">cmake --build ./compiler/build --target check-byteir</span><br><span class="line">cmake --build ./compiler/build --target byteir-python-pack</span><br></pre></td></tr></table></figure>

<p>上述一定要构建两个测试，第一个测试compiler的pipeline正确性，第二个测试python wrapper api正确性。</p>
<p>这里构建过程中有几处坑：lit测试由于我使用了虚拟环境，所以需要cmake中显示指定python3的可执行路径。pybind11和nanobind也是同理，需要显示指定路径。</p>
</li>
<li><p>测试正确性（一个端到端的测试case）</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">BYTEIR=&quot;/home/douliyang/large/mlir-workspace/byteir&quot;</span><br><span class="line"></span><br><span class="line">export PYTHONPATH=&quot;$&#123;BYTEIR&#125;/compiler/build/python_packages/byteir&quot;</span><br><span class="line"></span><br><span class="line">python3 -m byteir.tools.compiler -v \</span><br><span class="line">  &quot;$&#123;BYTEIR&#125;/compiler/test/E2E/CUDA/MLPInference/input.mlir&quot; \</span><br><span class="line">  -o out.mlir \</span><br><span class="line">  --entry_func forward \</span><br><span class="line"><span class="meta prompt_">  2&gt;</span><span class="language-bash">&amp;1 | <span class="built_in">tee</span> pipeline.log</span></span><br></pre></td></tr></table></figure></li>
</ol>
<p>​	<font color = red>注意，最终的wheel包生成在<code>build/python/dist</code>中，pip isntall即可。</font></p>
<h3 id="Runtime部分构建"><a href="#Runtime部分构建" class="headerlink" title="Runtime部分构建"></a><font color = green>Runtime部分构建</font></h3><p>首先确保compiler部分构建成功，runtime部分依赖compiler部分构建的LLVM项目。</p>
<ol>
<li><p>获取runtime的依赖</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git submodule update --init --recursive -f external/mlir-hlo external/cutlass external/date external/googletest external/pybind11</span><br></pre></td></tr></table></figure>
</li>
<li><p>构建runtime项目</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">cmake -H./runtime/cmake \</span><br><span class="line">      -B./runtime/build \</span><br><span class="line">      -G Ninja \</span><br><span class="line">      -DCMAKE_BUILD_TYPE=Release \</span><br><span class="line">      -DLLVM_INSTALL_PATH=$(pwd)/external/llvm-project/build/install \</span><br><span class="line">      -DCMAKE_INSTALL_PREFIX=&quot;$(pwd)/runtime/build/install&quot; \</span><br><span class="line">      -Dbrt_ENABLE_PYTHON_BINDINGS=ON \</span><br><span class="line">      -Dbrt_USE_CUDA=ON</span><br><span class="line"></span><br><span class="line">cmake --build ./runtime/build --target all --target install</span><br></pre></td></tr></table></figure>
</li>
<li><p>构建python wheel包，具体打包过程如下：</p>
<p>运行<code>python3 setup.py</code></p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">(base) root@leondou-y1fkbccpmlrb-main:/openbayes/home/byteir/byteir/runtime/python# python3 setup.py bdist_wheel</span><br><span class="line">running bdist_wheel</span><br><span class="line">running build</span><br><span class="line">running build_py</span><br><span class="line">creating build</span><br><span class="line">creating build/lib.linux-x86_64-cpython-38</span><br><span class="line">creating build/lib.linux-x86_64-cpython-38/brt</span><br><span class="line">copying /output/byteir/byteir/runtime/python/brt/backend.py -&gt; build/lib.linux-x86_64-cpython-38/brt</span><br><span class="line">copying /output/byteir/byteir/runtime/python/brt/__init__.py -&gt; build/lib.linux-x86_64-cpython-38/brt</span><br><span class="line">copying /output/byteir/byteir/runtime/python/brt/version.py -&gt; build/lib.linux-x86_64-cpython-38/brt</span><br><span class="line">copying /output/byteir/byteir/runtime/python/brt/utils.py -&gt; build/lib.linux-x86_64-cpython-38/brt</span><br><span class="line">running build_ext</span><br><span class="line">/usr/local/lib/python3.8/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.</span><br><span class="line">!!</span><br><span class="line"></span><br><span class="line">        ********************************************************************************</span><br><span class="line">        Please avoid running ``setup.py`` directly.</span><br><span class="line">        Instead, use pypa/build, pypa/installer or other</span><br><span class="line">        standards-based tools.</span><br><span class="line"></span><br><span class="line">        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.</span><br><span class="line">        ********************************************************************************</span><br><span class="line"></span><br><span class="line">!!</span><br><span class="line">  self.initialize_options()</span><br><span class="line">installing to build/bdist.linux-x86_64/wheel</span><br><span class="line">running install</span><br><span class="line">running install_lib</span><br><span class="line">creating build/bdist.linux-x86_64</span><br><span class="line">creating build/bdist.linux-x86_64/wheel</span><br><span class="line">creating build/bdist.linux-x86_64/wheel/brt</span><br><span class="line">copying build/lib.linux-x86_64-cpython-38/brt/backend.py -&gt; build/bdist.linux-x86_64/wheel/brt</span><br><span class="line">copying build/lib.linux-x86_64-cpython-38/brt/__init__.py -&gt; build/bdist.linux-x86_64/wheel/brt</span><br><span class="line">copying build/lib.linux-x86_64-cpython-38/brt/_brt.cpython-38-x86_64-linux-gnu.so -&gt; build/bdist.linux-x86_64/wheel/brt</span><br><span class="line">creating build/bdist.linux-x86_64/wheel/brt/lib</span><br><span class="line">copying build/lib.linux-x86_64-cpython-38/brt/lib/libbrt.so -&gt; build/bdist.linux-x86_64/wheel/brt/lib</span><br><span class="line">copying build/lib.linux-x86_64-cpython-38/brt/version.py -&gt; build/bdist.linux-x86_64/wheel/brt</span><br><span class="line">copying build/lib.linux-x86_64-cpython-38/brt/utils.py -&gt; build/bdist.linux-x86_64/wheel/brt</span><br><span class="line">running install_egg_info</span><br><span class="line">running egg_info</span><br><span class="line">creating /output/byteir/byteir/runtime/python/brt.egg-info</span><br><span class="line">writing /output/byteir/byteir/runtime/python/brt.egg-info/PKG-INFO</span><br><span class="line">writing dependency_links to /output/byteir/byteir/runtime/python/brt.egg-info/dependency_links.txt</span><br><span class="line">writing top-level names to /output/byteir/byteir/runtime/python/brt.egg-info/top_level.txt</span><br><span class="line">writing manifest file &#x27;/output/byteir/byteir/runtime/python/brt.egg-info/SOURCES.txt&#x27;</span><br><span class="line">reading manifest file &#x27;/output/byteir/byteir/runtime/python/brt.egg-info/SOURCES.txt&#x27;</span><br><span class="line">writing manifest file &#x27;/output/byteir/byteir/runtime/python/brt.egg-info/SOURCES.txt&#x27;</span><br><span class="line">Copying /output/byteir/byteir/runtime/python/brt.egg-info to build/bdist.linux-x86_64/wheel/brt-1.9.3.0+cpu-py3.8.egg-info</span><br><span class="line">running install_scripts</span><br><span class="line">creating build/bdist.linux-x86_64/wheel/brt-1.9.3.0+cpu.dist-info/WHEEL</span><br><span class="line">creating &#x27;dist/brt-1.9.3.0+cpu-cp38-cp38-linux_x86_64.whl&#x27; and adding &#x27;build/bdist.linux-x86_64/wheel&#x27; to it</span><br><span class="line">adding &#x27;brt/__init__.py&#x27;</span><br><span class="line">adding &#x27;brt/_brt.cpython-38-x86_64-linux-gnu.so&#x27;</span><br><span class="line">adding &#x27;brt/backend.py&#x27;</span><br><span class="line">adding &#x27;brt/utils.py&#x27;</span><br><span class="line">adding &#x27;brt/version.py&#x27;</span><br><span class="line">adding &#x27;brt/lib/libbrt.so&#x27;</span><br><span class="line">adding &#x27;brt-1.9.3.0+cpu.dist-info/METADATA&#x27;</span><br><span class="line">adding &#x27;brt-1.9.3.0+cpu.dist-info/WHEEL&#x27;</span><br><span class="line">adding &#x27;brt-1.9.3.0+cpu.dist-info/top_level.txt&#x27;</span><br><span class="line">adding &#x27;brt-1.9.3.0+cpu.dist-info/RECORD&#x27;</span><br><span class="line">removing build/bdist.linux-x86_64/wheel</span><br></pre></td></tr></table></figure></li>
</ol>
<p>​	<font color = red>注意：</font>最终的wheel包在&#x2F;openbayes&#x2F;home&#x2F;byteir&#x2F;byteir&#x2F;runtime&#x2F;python&#x2F;dist下，需要pip install一下才能使用。</p>
<blockquote>
<p>在组内服务器构建遇到的问题是，组内服务器没有cudnn，因此会报错<code>cudnn.h</code>头文件没有找到。目前暂时选择租服务器来完成构建流程，还没有解决这个问题（后续填坑）。</p>
</blockquote>
<p>至此成功构建好compiler部分和runtime部分（frontend部分不是研究重点，后续结合模型测试性能再研究）。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color = brown>参考资料</font></h2><ol>
<li><a href="https://byteir.ai/">ByteIR官方资料</a></li>
<li><a href="https://github.com/bytedance/byteir?tab=readme-ov-file">ByteIR github</a></li>
</ol>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>机器学习编译</category>
      </categories>
      <tags>
        <tag>机器学习编译器</tag>
        <tag>mlir</tag>
      </tags>
  </entry>
  <entry>
    <title>GPU 后端优化（二）</title>
    <url>/2025/05/12/GPU-%E5%90%8E%E7%AB%AF%E4%BC%98%E5%8C%96%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    <content><![CDATA[<p><img src="/images/image-20250512225158784.png" alt="image-20250512225158784"></p>
<span id="more"></span>

<blockquote>
<p>这是本系列的第二篇文章，主要结合印度理工学院搭建的面向GPU tensor core代码生成的论文，以及IREE开源项目，解读如何基于MLIR系统完成GPU的代码生成。</p>
</blockquote>
<h2 id="IREE-Tensor-Core代码生成"><a href="#IREE-Tensor-Core代码生成" class="headerlink" title="IREE  Tensor Core代码生成"></a><font color = brown>IREE  Tensor Core代码生成</font></h2><p>IREE是一个端到端的开源机器学习编译器，支持在边缘设备进行高效部署。本章节主要讲解IREE项目是如何做面向Nvidia GPU的代码生成的。</p>
<h3 id="IREE-后端代码生成管线概览"><a href="#IREE-后端代码生成管线概览" class="headerlink" title="IREE 后端代码生成管线概览"></a><font color = green>IREE 后端代码生成管线概览</font></h3><h3 id="详解matmul算子生成"><a href="#详解matmul算子生成" class="headerlink" title="详解matmul算子生成"></a><font color = green>详解matmul算子生成</font></h3><blockquote>
<p>注：这一部分解读主要参考<a href="https://zhuanlan.zhihu.com/p/712869828">IREE Codegen博客</a>。</p>
</blockquote>
<p>结合前一章节的论文解读，我们重点关注IREE的matmul代码生成部分。在<code>compiler/src/iree/compiler/Codegen/LLVMGPU/LLVMGPULowerExecutableTarget.cpp</code>代码中，有如下代码生成dispatch实现：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 根据translation info，选择对应的executable生成的dispatch lowering pipeline</span></span><br><span class="line">  <span class="keyword">switch</span> (translationInfo.<span class="built_in">getDispatchLoweringPassPipeline</span>()) &#123;</span><br><span class="line">  <span class="keyword">case</span> IREE::Codegen::DispatchLoweringPassPipeline::LLVMGPUDefault:</span><br><span class="line">    <span class="built_in">addGPUDefaultPassPipeline</span>(pipeline, pipelineOptions);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> IREE::Codegen::DispatchLoweringPassPipeline::LLVMGPUBaseLowering:</span><br><span class="line">    <span class="built_in">addGPUBaseLoweringPassPipeline</span>(pipeline);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> IREE::Codegen::DispatchLoweringPassPipeline::LLVMGPUDistribute:</span><br><span class="line">    <span class="built_in">addGPUSimpleDistributePassPipeline</span>(pipeline);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> IREE::Codegen::DispatchLoweringPassPipeline::LLVMGPUVectorize:</span><br><span class="line">    <span class="built_in">addGPUVectorizationPassPipeline</span>(pipeline);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> IREE::Codegen::DispatchLoweringPassPipeline::LLVMGPUWinogradVectorize:</span><br><span class="line">    <span class="built_in">addGPUWinogradVectorizePassPipeline</span>(pipeline);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> IREE::Codegen::DispatchLoweringPassPipeline::LLVMGPUMatmulSimt:</span><br><span class="line">    <span class="built_in">addGPUMatmulSimtPassPipeline</span>(pipeline, pipelineOptions);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> IREE::Codegen::DispatchLoweringPassPipeline::LLVMGPUMatmulTensorCore: &#123;</span><br><span class="line">    <span class="comment">// 这个pass pipeline比较有意思，可以debug一下</span></span><br><span class="line">    <span class="comment">// tensor core生成逻辑</span></span><br><span class="line">    <span class="comment">// Tile and distribute operations to workgroups</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 这个函数自动获取pipeline元信息</span></span><br><span class="line">    FailureOr&lt;<span class="type">int64_t</span>&gt; maybeDepth =</span><br><span class="line">        <span class="built_in">getSoftwarePipelineDepth</span>(translationInfo.<span class="built_in">getConfiguration</span>());</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">failed</span>(maybeDepth)) &#123;</span><br><span class="line">      funcOp.<span class="built_in">emitOpError</span>(</span><br><span class="line">          <span class="string">&quot;invalid matmul configuration without software pipelining config&quot;</span>);</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">signalPassFailure</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">addGPUMatmulTensorCorePassPipeline</span>(pipeline, pipelineOptions, *maybeDepth);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">case</span> IREE::Codegen::DispatchLoweringPassPipeline::</span><br><span class="line">      LLVMGPUMatmulTensorCoreMmaSync: &#123;</span><br><span class="line">    FailureOr&lt;<span class="type">int64_t</span>&gt; maybeDepth =</span><br><span class="line">        <span class="built_in">getSoftwarePipelineDepth</span>(translationInfo.<span class="built_in">getConfiguration</span>());</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">failed</span>(maybeDepth)) &#123;</span><br><span class="line">      funcOp.<span class="built_in">emitOpError</span>(</span><br><span class="line">          <span class="string">&quot;invalid matmul configuration without software pipelining config&quot;</span>);</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">signalPassFailure</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">addGPUMatmulTensorCoreMmaSyncPassPipeline</span>(pipeline, pipelineOptions,</span><br><span class="line">                                              *maybeDepth);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">case</span> IREE::Codegen::DispatchLoweringPassPipeline::LLVMGPUTransposeSharedMem:</span><br><span class="line">    <span class="built_in">addGPUTransposePassPipeline</span>(pipeline, pipelineOptions);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> IREE::Codegen::DispatchLoweringPassPipeline::LLVMGPUVectorDistribute:</span><br><span class="line">    <span class="built_in">addGPUVectorDistributePassPipeline</span>(pipeline, pipelineOptions,</span><br><span class="line">                                       <span class="comment">/*usePadToModelSharedMemcpy=*/</span><span class="literal">false</span>);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> IREE::Codegen::DispatchLoweringPassPipeline::</span><br><span class="line">      LLVMGPUPadAndVectorDistribute:</span><br><span class="line">    <span class="built_in">addGPUVectorDistributePassPipeline</span>(pipeline, pipelineOptions,</span><br><span class="line">                                       <span class="comment">/*usePadToModelSharedMemcpy=*/</span><span class="literal">true</span>);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> IREE::Codegen::DispatchLoweringPassPipeline::LLVMGPUWarpReduction:</span><br><span class="line">    <span class="built_in">addGPUWarpReductionPassPipeline</span>(pipeline);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> IREE::Codegen::DispatchLoweringPassPipeline::LLVMGPUPackUnPack:</span><br><span class="line">    <span class="built_in">addGPUPackUnPackPasses</span>(pipeline);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> IREE::Codegen::DispatchLoweringPassPipeline::LLVMGPUTileAndFuse:</span><br><span class="line">    <span class="built_in">addGPUTileAndFusePassPipeline</span>(pipeline, pipelineOptions);</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="comment">// no pipeline specified, nothing to do.</span></span><br><span class="line">  <span class="keyword">case</span> IREE::Codegen::DispatchLoweringPassPipeline::None:</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">default</span>:</span><br><span class="line">    funcOp.<span class="built_in">emitOpError</span>(<span class="string">&quot;unsupported pipeline on GPU target.&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">signalPassFailure</span>();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，结合不同的<code>translation info</code>，IREE针对每个可执行变种（IREE中叫execute.variant）派发不同的代码生成逻辑。这里我们重点关注<code>LLVMGPUMatmulTensorCore</code>，该管线是<strong>面向Nvidia gpu的tensor core硬件的适配编译流水线</strong>。</p>
<h4 id="测试输入"><a href="#测试输入" class="headerlink" title="测试输入"></a>测试输入</h4><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">#compilation0 = #iree_codegen.compilation_info&lt;</span><br><span class="line">  lowering_config = #iree_codegen.lowering_config&lt;tile_sizes = [[<span class="number">32</span>, <span class="number">32</span>, <span class="number">16</span>]]&gt;,</span><br><span class="line">  translation_info = #iree_codegen.translation_info&lt;pipeline = LLVMGPUMatmulTensorCore workgroup_size = [<span class="number">64</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">  ,</span><br><span class="line">  &#123; pipeline_depth = <span class="number">3</span>, store_stage = <span class="number">1</span>&#125;&gt;&gt;</span><br><span class="line">func.func @<span class="built_in">matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1</span>(%lhs: tensor&lt;<span class="number">512</span>x128xf16&gt;, %rhs: tensor&lt;<span class="number">128</span>x512xf16&gt;, %acc: tensor&lt;<span class="number">512</span>x512xf16&gt;) -&gt; tensor&lt;<span class="number">512</span>x512xf16&gt; &#123;</span><br><span class="line">  %result = linalg.matmul &#123;compilation_info = #compilation0&#125; <span class="built_in">ins</span>(%lhs, %rhs: tensor&lt;<span class="number">512</span>x128xf16&gt;, tensor&lt;<span class="number">128</span>x512xf16&gt;) <span class="built_in">outs</span>(%acc: tensor&lt;<span class="number">512</span>x512xf16&gt;) -&gt; tensor&lt;<span class="number">512</span>x512xf16&gt;</span><br><span class="line">  <span class="keyword">return</span> %result: tensor&lt;<span class="number">512</span>x512xf16&gt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个测试例子是一个简单的矩阵乘运算，维度是&lt;512x128xf16&gt; x &lt;128x512xf16&gt; &#x3D; &lt;512x512xf16&gt;。这段测试核心点事compiler配置attribute：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">#compilation0 = #iree_codegen.compilation_info&lt;</span><br><span class="line">  lowering_config = #iree_codegen.lowering_config&lt;tile_sizes = [[<span class="number">32</span>, <span class="number">32</span>, <span class="number">16</span>]]&gt;,</span><br><span class="line">  translation_info = #iree_codegen.translation_info&lt;pipeline = LLVMGPUMatmulTensorCore workgroup_size = [<span class="number">64</span>, <span class="number">2</span>, <span class="number">1</span>]</span><br><span class="line">  ,</span><br><span class="line">  &#123; pipeline_depth = <span class="number">3</span>, store_stage = <span class="number">1</span>&#125;&gt;&gt;</span><br></pre></td></tr></table></figure>

<p>其中，定义了如下元数据：</p>
<ul>
<li>workgroup（即threadBlock）维度是[64, 2, 1]。</li>
<li>tile分块的维度是[32, 32, 16]。</li>
<li>流水线深度为3，注意3是GPU最小流水线深度（存，取，执行）。</li>
<li>storage_stage是1，这个具体含义尚不明晰。</li>
</ul>
<p>针对这个测试，我们的测试脚本如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">iree-compile --iree-hal-target-backends=cuda \</span><br><span class="line">  --iree-cuda-target=sm_86 \</span><br><span class="line">  --mlir-disable-threading \</span><br><span class="line">  --mlir-elide-elementsattrs-if-larger=10 \</span><br><span class="line">  --mlir-print-ir-after-all \</span><br><span class="line">  matmul.mlir -o test.vmfb \</span><br><span class="line"><span class="meta prompt_">  2&gt;</span><span class="language-bash">&amp;1 | <span class="built_in">tee</span> output.dump</span></span><br></pre></td></tr></table></figure>

<h4 id="Tensor-Core-verifier"><a href="#Tensor-Core-verifier" class="headerlink" title="Tensor Core verifier"></a>Tensor Core verifier</h4><p>现在我们有了输入和测试脚本，IREE在根据我们的代码和配置做tensor core转换之前，会先检查一下参数是否适配。具体地参考<code>compiler/src/iree/compiler/Codegen/LLVMGPU/Verifiers.cpp</code>中关于<code>compilation_info</code>和<code>work_group</code>等参数的约束判断逻辑。这里通过几张表格的方式简单解释下各个参数背后的具体意义和相应约束：</p>
<p><strong>核心概念</strong></p>
<table>
<thead>
<tr>
<th align="center"><strong>概念</strong></th>
<th align="center"><strong>代码对应变量&#x2F;参数</strong></th>
<th align="center"><strong>含义与作用</strong></th>
<th align="center"><strong>约束来源</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center"><strong>Workgroup Size</strong></td>
<td align="center"><code>workgroup_size</code></td>
<td align="center">线程块（Thread Block）的维度配置（如<code>[64,2,1]</code>），总线程数 ≤ 1024</td>
<td align="center">CUDA硬件限制</td>
</tr>
<tr>
<td align="center"><strong>Tile Size</strong></td>
<td align="center"><code>tile_sizes</code></td>
<td align="center">矩阵分块尺寸（如<code>[32,32,16]</code>），决定每个线程块处理的数据块大小</td>
<td align="center">算法优化需求</td>
</tr>
<tr>
<td align="center"><strong>Thread Block Shape</strong></td>
<td align="center"><code>threadBlockShape</code></td>
<td align="center">线程块内各维度的分块尺寸（如<code>[32,32,16]</code>），与Tile Size直接相关</td>
<td align="center">分块策略与硬件匹配</td>
</tr>
<tr>
<td align="center"><strong>Warp数量</strong></td>
<td align="center"><code>numWarps</code></td>
<td align="center">线程块内各维度的Warp数量（如<code>[2,2,1]</code>），由Workgroup Size除以Warp Size（32）计算</td>
<td align="center">SM硬件架构</td>
</tr>
<tr>
<td align="center"><strong>Warp Shape</strong></td>
<td align="center"><code>warpShape</code></td>
<td align="center">单个Warp处理的分块子矩阵尺寸（如<code>[16,16,16]</code>）</td>
<td align="center">线程调度与指令级并行</td>
</tr>
<tr>
<td align="center"><strong>Instruction Shape</strong></td>
<td align="center"><code>instructionShape</code></td>
<td align="center">Tensor Core硬件指令支持的矩阵尺寸（如<code>FP16→16x16x16</code>，<code>FP32→16x16x8</code>）</td>
<td align="center">Tensor Core架构规范</td>
</tr>
</tbody></table>
<p><strong>TensorCore约束检测逻辑</strong></p>
<table>
<thead>
<tr>
<th align="center"><strong>验证项</strong></th>
<th align="center"><strong>验证条件</strong></th>
<th align="center"><strong>失败后果</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center">Workgroup总线程数</td>
<td align="center"><code>workgroupSize[X] * Y * Z ≤ 1024</code></td>
<td align="center">超过GPU线程块容量限制，无法执行</td>
</tr>
<tr>
<td align="center">Z维度线程数</td>
<td align="center"><code>workgroupSize[Z] == 1</code></td>
<td align="center">Tensor Core设计为二维计算，Z维度扩展会破坏数据局部性</td>
</tr>
<tr>
<td align="center">X维度线程数</td>
<td align="center"><code>workgroupSize[X] % 32 == 0</code></td>
<td align="center">Warp调度需要X维度为32的整数倍（每个Warp含32线程）</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th align="center"><strong>验证项</strong></th>
<th align="center"><strong>验证条件</strong></th>
<th align="center"><strong>失败后果</strong></th>
</tr>
</thead>
<tbody><tr>
<td align="center">矩阵尺寸对齐</td>
<td align="center"><code>matmulShape[M/N/K] % threadBlockShape[M/N/K] == 0</code></td>
<td align="center">分块无法均匀覆盖原矩阵，导致计算错误或性能下降</td>
</tr>
<tr>
<td align="center">Warp分块对齐</td>
<td align="center"><code>warpShape[M/N/K] % instructionShape[M/N/K] == 0</code></td>
<td align="center">Tensor Core指令无法覆盖Warp分块，硬件资源利用率不足</td>
</tr>
</tbody></table>
<p>这里有两个关键点：</p>
<ol>
<li><strong>tensor core的最小执行单元是一个warp，因此需要验证Warp是否分块对齐。</strong></li>
<li><strong>workgroup就是threadblock。tiesize表示一个threadblock完成的计算任务量，而workgroup则表示一个threadblock有多少个线程，并可以根据warpsize（默认32）计算一个workgroup可以有多少warps。这两个概念要分辨清楚</strong></li>
</ol>
<p>整个tensor core verifier的逻辑比较复杂，需要详细阅读下面代码的注释以获得全面的了理解：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Number of warps in x, y, and z dim.</span></span><br><span class="line"><span class="comment">// 计算一个workgroup的warp数目</span></span><br><span class="line"><span class="comment">// warp一般为32，如果一个workgroup是[64,2,1]，则这里得到[2,2,1]维度的warp</span></span><br><span class="line">SmallVector&lt;<span class="type">int64_t</span>&gt; numWarps&#123;workgroupSize[kDimX] / kWarpSize,</span><br><span class="line">                            workgroupSize[kDimY], workgroupSize[kDimZ]&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Matrix-multiply problem shape in number of elements in M, N, and K dim.</span></span><br><span class="line"><span class="comment">// matmulshape为[512, 512, 128]</span></span><br><span class="line"><span class="comment">// 获取矩阵最本元的MNK参数。</span></span><br><span class="line">SmallVector&lt;<span class="type">int64_t</span>&gt; matmulShape&#123;lhsShape[<span class="number">0</span>], rhsShape[<span class="number">1</span>], lhsShape[<span class="number">1</span>]&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Warp tile shape in number of elements in M, N, and K dim.</span></span><br><span class="line"><span class="comment">// Note that num warp in (x, y, z) dim are mapped to problem (M, N, K) dim as:</span></span><br><span class="line"><span class="comment">// DimY -&gt; ProblemDimM, DimX -&gt; ProblemDimN, DimZ -&gt; ProblemDimK.</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">将线程块（thread block）形状均匀划分给各个 warp。</span></span><br><span class="line"><span class="comment">注意：注释中说明了 warp 在 (x, y, z) 维度的分布映射到矩阵问题的 (M, N, K) 上，其中：</span></span><br><span class="line"><span class="comment">  y 维度 warp 对应问题的 M；</span></span><br><span class="line"><span class="comment">  x 维度 warp 对应问题的 N；</span></span><br><span class="line"><span class="comment">  z 维度 warp 对应问题的 K；</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">// [32/2, 32/2, 16/1] = [16, 16, 16]表征每个warp所要干的工作</span></span><br><span class="line">SmallVector&lt;<span class="type">int64_t</span>&gt; warpShape&#123;threadBlockShape[kM] / numWarps[kDimY],</span><br><span class="line">                             threadBlockShape[kN] / numWarps[kDimX],</span><br><span class="line">                             threadBlockShape[kK] / numWarps[kDimZ]&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Instruction shape in number of elements in M, N, and K dim.</span></span><br><span class="line"><span class="comment">// 获取tensor core指令形状</span></span><br><span class="line">SmallVector&lt;<span class="type">int64_t</span>&gt; instructionShape;</span><br><span class="line"><span class="comment">// f16 和 bf16 类型对应的指令形状为 &#123;16, 16, 16&#125;，而 f32 类型对应 &#123;16, 16, 8&#125; </span></span><br><span class="line"><span class="keyword">if</span> (<span class="built_in">failed</span>(<span class="built_in">getInstructionShape</span>(</span><br><span class="line">      op, pipeline, llvm::<span class="built_in">cast</span>&lt;ShapedType&gt;(lhsType).<span class="built_in">getElementType</span>(),</span><br><span class="line">      instructionShape))) &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">failure</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Verify that matmul problem shape can be tiled with the thread block shape.</span></span><br><span class="line"><span class="comment">// <span class="doctag">TODO:</span> This check should be relaxed as we allow unaligned matmul shapes.</span></span><br><span class="line"><span class="comment">// 要求矩阵问题的每个维度（M、N、K）能够被tile设定的维度整除，来决策是否可以做tiling运算</span></span><br><span class="line"><span class="comment">// 检测[512%32, 512%32, 128%16]</span></span><br><span class="line"><span class="keyword">if</span> (matmulShape[kM] % threadBlockShape[kM] != <span class="number">0</span> ||</span><br><span class="line">  matmulShape[kN] % threadBlockShape[kN] != <span class="number">0</span> ||</span><br><span class="line">  matmulShape[kK] % threadBlockShape[kK] != <span class="number">0</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> op-&gt;<span class="built_in">emitError</span>(<span class="string">&quot;Thread block shape &quot;</span>)</span><br><span class="line">       &lt;&lt; threadBlockShape &lt;&lt; <span class="string">&quot; cannot be tiled on matmul shape &quot;</span></span><br><span class="line">       &lt;&lt; matmulShape &lt;&lt; <span class="string">&quot; with compilation pipeline &quot;</span> &lt;&lt; pipelineName;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Verify that if warp shape can be tiled using warp-level Tensor core</span></span><br><span class="line"><span class="comment">// instruction shape.</span></span><br><span class="line"><span class="comment">// 确保每个 warp tile（即 warpShape）在 M、N、K 维度上均可以被对应的 Tensor Core 指令形状整除。</span></span><br><span class="line"><span class="comment">// 若不满足，则说明硬件的计算单元（Tensor Core）无法完美地覆盖 warp tile</span></span><br><span class="line"><span class="comment">// 以f16为例，为[16,16,16]，即每个warp的计算，是否可以生成Tensor Core指令</span></span><br><span class="line"><span class="keyword">if</span> (warpShape[kM] % instructionShape[kM] != <span class="number">0</span> ||</span><br><span class="line">  warpShape[kN] % instructionShape[kN] != <span class="number">0</span> ||</span><br><span class="line">  warpShape[kK] % instructionShape[kK] != <span class="number">0</span>) &#123;</span><br><span class="line"><span class="keyword">return</span> op-&gt;<span class="built_in">emitError</span>(<span class="string">&quot;Tensor Core instruction shape &quot;</span>)</span><br><span class="line">       &lt;&lt; instructionShape &lt;&lt; <span class="string">&quot; cannot be tiled on warp shape &quot;</span> &lt;&lt; warpShape</span><br><span class="line">       &lt;&lt; <span class="string">&quot; with compilation pipeline &quot;</span> &lt;&lt; pipelineName;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="矩阵乘tensor-core生成流程概览"><a href="#矩阵乘tensor-core生成流程概览" class="headerlink" title="矩阵乘tensor core生成流程概览"></a>矩阵乘tensor core生成流程概览</h4><p>讲解完Tensor Core管线的verifier流程后，我们逐渐接触tensor core生成的主体管线流程，代码如下所示：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">addGPUMatmulTensorCorePassPipeline</span><span class="params">(OpPassManager &amp;funcPassManager,</span></span></span><br><span class="line"><span class="params"><span class="function">                                        <span class="type">const</span> GPUPipelineOptions &amp;options,</span></span></span><br><span class="line"><span class="params"><span class="function">                                        <span class="type">unsigned</span> pipelineDepth)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">tileAndBufferize</span>(funcPassManager);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Distribute linalg onto warps within the workgroup.</span></span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(</span><br><span class="line">      <span class="built_in">createLLVMGPUTileAndDistributePass</span>(<span class="comment">/*distributeToWarp=*/</span><span class="literal">true</span>));</span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createRemoveSingleIterationLoopPass</span>());</span><br><span class="line">  <span class="keyword">if</span> (pipelineDepth &gt; <span class="number">1</span>) &#123;</span><br><span class="line">    funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createGPUMultiBufferingPass</span>(</span><br><span class="line">        GPUMultiBufferingPassOptions&#123;pipelineDepth&#125;));</span><br><span class="line">  &#125;</span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createCanonicalizerPass</span>());</span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createCSEPass</span>());</span><br><span class="line"></span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createRemoveSingleIterationLoopPass</span>());</span><br><span class="line"></span><br><span class="line">  ReorderWorkgroupsStrategy reorderStrategy =</span><br><span class="line">      <span class="built_in">getReorderWorkgroupsStrategy</span>(options.reorderStrategy);</span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(</span><br><span class="line">      <span class="built_in">createReorderWorkgroups</span>(reorderStrategy, canReorderWorkgroups));</span><br><span class="line"></span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createCanonicalizerPass</span>());</span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createCSEPass</span>());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Linalg -&gt; vector</span></span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(</span><br><span class="line">      <span class="built_in">createLLVMGPUTensorCoreVectorizationPass</span>(GPUTensorCoreType::WMMA));</span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(memref::<span class="built_in">createFoldMemRefAliasOpsPass</span>());</span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createCSEPass</span>());</span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createOptimizeVectorTransferPass</span>());</span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createOptimizeTensorInsertExtractSlicesPass</span>());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Distribute shared memory copies.</span></span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createMemrefCopyToLinalgPass</span>());</span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createGPUDistributeSharedMemoryCopyPass</span>());</span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createCanonicalizerPass</span>());</span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createCSEPass</span>());</span><br><span class="line">  <span class="keyword">if</span> (options.enableReduceSharedMemoryBankConflicts) &#123;</span><br><span class="line">    funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createGPUReduceBankConflictsPass</span>());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Vector -&gt; MMA ops</span></span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(memref::<span class="built_in">createFoldMemRefAliasOpsPass</span>());</span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createCanonicalizerPass</span>());</span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createCSEPass</span>());</span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(</span><br><span class="line">      <span class="built_in">createLLVMGPUVectorToGPUPass</span>(GPUTensorCoreType::WMMA));</span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createCanonicalizerPass</span>());</span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createCSEPass</span>());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Hoist loop invariant code to avoid pipelining it.</span></span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createIREELoopInvariantCodeMotionPass</span>());</span><br><span class="line">  <span class="comment">// Pipeline memory operations.</span></span><br><span class="line">  GPUPipeliningPassOptions pipelieningOptions = &#123;&#125;;</span><br><span class="line">  pipelieningOptions.epiloguePeeling = <span class="literal">false</span>;</span><br><span class="line">  pipelieningOptions.depth = pipelineDepth;</span><br><span class="line">  pipelieningOptions.scheduleIndex =</span><br><span class="line">      llvm::<span class="built_in">to_underlying</span>(PipeliningSchedulingStrategy::loadGlobalStage0);</span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createGPUPipeliningPass</span>(pipelieningOptions));</span><br><span class="line">  <span class="comment">// Optimize shared memory usage.</span></span><br><span class="line">  funcPassManager.<span class="built_in">addPass</span>(<span class="built_in">createLLVMGPUPackSharedMemoryAllocPass</span>());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述管线中，我们比较关心如下7个pass的流程：</p>
<ul>
<li><code>LLVMGPUTileAndDistribute</code></li>
<li><code>GPUMultiBufferingPass</code></li>
<li><code>LLVMGPUTensorCoreVectorizationPass</code></li>
<li><code>GPUDistributeSharedMemoryCopyPass</code></li>
<li><code>GPUReduceBankConflictsPass</code></li>
<li><code>LLVMGPUVectorToGPU</code></li>
<li><code>GPUPipeliningPass</code></li>
<li><code>LLVMGPUPackSharedMemoryAlloc</code></li>
</ul>
<p>我们接下来结合代码，一个一个pass的解读。</p>
<h4 id="LLVMGPUTileAndDistribute-Pass"><a href="#LLVMGPUTileAndDistribute-Pass" class="headerlink" title="LLVMGPUTileAndDistribute Pass"></a><font color = pink>LLVMGPUTileAndDistribute Pass</font></h4><p>LLVMGPUTileAndDistribute 这个 pass 主要是根据 lower_config 中的 <code>tile_sizes</code> 和 compilation info 中的 <code>workgroup size</code> 进行 tiling，并且在此过程中，使用了 <code>shared memory</code> 用作缓存进行访存的优化。</p>
<p>我们的workload可以表征为如下：C&lt;512x512&gt; &#x3D; A&lt;512x128&gt; x B&lt;128x512&gt;</p>
<p>具体的，可以拆解为下面几步：</p>
<ol>
<li>初步tile，将计算workload分块为&lt;32x128&gt; x &lt;128x32&gt;</li>
<li>Promote Memory1：将C矩阵放入shared_memory</li>
<li>持续tile化，化为指定的&lt;32x16&gt; x &lt;16x32&gt;</li>
<li>Promote Memory2：如果workgroup比warp大，那么需要将一部分的A和B也放入sharedmemory</li>
<li>根据warp size继续tile化。我们的thread block是[64,2,1]，warp size是32，所以可以拆成[2,2,1]个warp。针对此，我们的矩阵&lt;32x16&gt; x &lt;16x32&gt;可以进一步变成&lt;32&#x2F;2 x 16&gt; x &lt;16 x 32&#x2F;2&gt;</li>
</ol>
<p>接下来分别在debug过程中dump每个步骤生成的中间表示代码。</p>
<h5 id="Step1-i，j维度分块"><a href="#Step1-i，j维度分块" class="headerlink" title="Step1 i，j维度分块"></a><font color = brown>Step1 i，j维度分块</font></h5><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">func.func @<span class="built_in">matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16</span>() attributes &#123;translation_info = #iree_codegen.translation_info&lt;pipeline = LLVMGPUMatmulTensorCore workgroup_size = [<span class="number">64</span>, <span class="number">2</span>, <span class="number">1</span>], &#123;pipeline_depth = <span class="number">3</span> : i64, store_stage = <span class="number">1</span> : i64&#125;&gt;&#125; &#123;</span><br><span class="line">  %c0 = arith.constant <span class="number">0</span> : index</span><br><span class="line">  %<span class="number">0</span> = hal.interface.binding.subspan <span class="built_in">layout</span>(&lt;bindings = [<span class="meta">#hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, Indirect&gt;</span>], flags = Indirect&gt;) binding(0) alignment(64) offset(%c0) flags(<span class="string">&quot;ReadOnly|Indirect&quot;</span>) : memref<span class="string">&lt;512x128xf16, #hal.descriptor_type&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  memref.assume_alignment %<span class="number">0</span>, <span class="number">64</span> : memref&lt;<span class="number">512</span>x128xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">1</span> = hal.interface.binding.subspan <span class="built_in">layout</span>(&lt;bindings = [<span class="meta">#hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, Indirect&gt;</span>], flags = Indirect&gt;) binding(1) alignment(64) offset(%c0) flags(<span class="string">&quot;ReadOnly|Indirect&quot;</span>) : memref<span class="string">&lt;128x512xf16, #hal.descriptor_type&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  memref.assume_alignment %<span class="number">1</span>, <span class="number">64</span> : memref&lt;<span class="number">128</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">2</span> = hal.interface.binding.subspan <span class="built_in">layout</span>(&lt;bindings = [<span class="meta">#hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, Indirect&gt;</span>], flags = Indirect&gt;) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<span class="string">&lt;512x512xf16, #hal.descriptor_type&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  memref.assume_alignment %<span class="number">2</span>, <span class="number">64</span> : memref&lt;<span class="number">512</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %workgroup_id_x = hal.interface.workgroup.id[<span class="number">0</span>] : index</span><br><span class="line">  %workgroup_id_y = hal.interface.workgroup.id[<span class="number">1</span>] : index</span><br><span class="line">  %<span class="number">3</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0] -&gt; (s0 * <span class="number">32</span>)&gt;()[%workgroup_id_y]</span><br><span class="line">  %subview = memref.subview %<span class="number">0</span>[%<span class="number">3</span>, <span class="number">0</span>] [<span class="number">32</span>, <span class="number">128</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">512</span>x128xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x128xf16, strided&lt;[128, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">4</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0] -&gt; (s0 * <span class="number">32</span>)&gt;()[%workgroup_id_x]</span><br><span class="line">  %subview_0 = memref.subview %<span class="number">1</span>[<span class="number">0</span>, %<span class="number">4</span>] [<span class="number">128</span>, <span class="number">32</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">128</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;128x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %subview_1 = memref.subview %<span class="number">2</span>[%<span class="number">3</span>, %<span class="number">4</span>] [<span class="number">32</span>, <span class="number">32</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">512</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  linalg.matmul &#123;lowering_config = #iree_codegen.lowering_config&lt;tile_sizes = [[<span class="number">32</span>, <span class="number">32</span>, <span class="number">16</span>]]&gt;&#125; <span class="built_in">ins</span>(%subview, %subview_0 : memref&lt;<span class="number">32</span>x128xf16, strided&lt;[<span class="number">128</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;, memref<span class="string">&lt;128x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;) outs(%subview_1 : memref<span class="string">&lt;32x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;)</span></span><br><span class="line">  <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这段代码在 <strong>GPU 上实现了高效的 512×512 半精度矩阵乘法</strong>，并进行了如下优化：</p>
<ol>
<li><strong>使用 Tensor Core 加速计算</strong>（<code>tile_sizes = [[32, 32, 16]]</code>）。</li>
<li>分块计算（Tile-based Computation）<ul>
<li>将矩阵 A、B、C 分割成 <strong>32×128、128×32、32×32</strong> 子矩阵。</li>
<li>使用 <code>workgroup_id_x/y</code> 进行索引计算，每个 <code>workgroup</code> 处理固定区域。</li>
</ul>
</li>
<li>存储优化<ul>
<li><strong>存储对齐（alignment 64）</strong>：提高 GPU 访存性能。</li>
<li><strong>使用 Subview 提取矩阵块</strong>，减少数据移动，提升数据局部性。</li>
</ul>
</li>
</ol>
<h5 id="Step2（做promotion优化）"><a href="#Step2（做promotion优化）" class="headerlink" title="Step2（做promotion优化）"></a><font color = brown>Step2（做promotion优化）</font></h5><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//========================================== LLVM GPU Tile And Distibute ========================================</span></span><br><span class="line"><span class="comment">// Step1</span></span><br><span class="line">func.func @<span class="built_in">matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16</span>() attributes &#123;translation_info = #iree_codegen.translation_info&lt;pipeline = LLVMGPUMatmulTensorCore workgroup_size = [<span class="number">64</span>, <span class="number">2</span>, <span class="number">1</span>], &#123;pipeline_depth = <span class="number">3</span> : i64, store_stage = <span class="number">1</span> : i64&#125;&gt;&#125; &#123;</span><br><span class="line">  %c32 = arith.constant <span class="number">32</span> : index</span><br><span class="line">  %c0 = arith.constant <span class="number">0</span> : index</span><br><span class="line">  %alloc = memref.<span class="built_in">alloc</span>() : memref&lt;<span class="number">32</span>x32xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">  <span class="comment">// 获取interface的binding，这是数组A </span></span><br><span class="line">  %<span class="number">0</span> = hal.interface.binding.subspan <span class="built_in">layout</span>(&lt;bindings = [<span class="meta">#hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, Indirect&gt;</span>], flags = Indirect&gt;) binding(0) alignment(64) offset(%c0) flags(<span class="string">&quot;ReadOnly|Indirect&quot;</span>) : memref<span class="string">&lt;512x128xf16, #hal.descriptor_type&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  memref.assume_alignment %<span class="number">0</span>, <span class="number">64</span> : memref&lt;<span class="number">512</span>x128xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  <span class="comment">// 这是数组B</span></span><br><span class="line">  %<span class="number">1</span> = hal.interface.binding.subspan <span class="built_in">layout</span>(&lt;bindings = [<span class="meta">#hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, Indirect&gt;</span>], flags = Indirect&gt;) binding(1) alignment(64) offset(%c0) flags(<span class="string">&quot;ReadOnly|Indirect&quot;</span>) : memref<span class="string">&lt;128x512xf16, #hal.descriptor_type&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  memref.assume_alignment %<span class="number">1</span>, <span class="number">64</span> : memref&lt;<span class="number">128</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  <span class="comment">// 这是数组C</span></span><br><span class="line">  <span class="comment">// 注意返回的是memref&lt;512x512xf16, #hal.descriptor_type&lt;storage_buffer&gt;&gt;</span></span><br><span class="line">  %<span class="number">2</span> = hal.interface.binding.subspan <span class="built_in">layout</span>(&lt;bindings = [<span class="meta">#hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, Indirect&gt;</span>], flags = Indirect&gt;) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<span class="string">&lt;512x512xf16, #hal.descriptor_type&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  memref.assume_alignment %<span class="number">2</span>, <span class="number">64</span> : memref&lt;<span class="number">512</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 获取workgroup中的thread id，是二维的</span></span><br><span class="line">  %workgroup_id_x = hal.interface.workgroup.id[<span class="number">0</span>] : index</span><br><span class="line">  %workgroup_id_y = hal.interface.workgroup.id[<span class="number">1</span>] : index</span><br><span class="line">  %<span class="number">3</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0] -&gt; (s0 * <span class="number">32</span>)&gt;()[%workgroup_id_y]</span><br><span class="line">  %subview = memref.subview %<span class="number">0</span>[%<span class="number">3</span>, <span class="number">0</span>] [<span class="number">32</span>, <span class="number">128</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">512</span>x128xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x128xf16, strided&lt;[128, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">4</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0] -&gt; (s0 * <span class="number">32</span>)&gt;()[%workgroup_id_x]</span><br><span class="line">  %subview_0 = memref.subview %<span class="number">1</span>[<span class="number">0</span>, %<span class="number">4</span>] [<span class="number">128</span>, <span class="number">32</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">128</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;128x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  <span class="comment">// C的subview，这个是external memory</span></span><br><span class="line">  %subview_1 = memref.subview %<span class="number">2</span>[%<span class="number">3</span>, %<span class="number">4</span>] [<span class="number">32</span>, <span class="number">32</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">512</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %subview_2 = memref.subview %alloc[<span class="number">0</span>, <span class="number">0</span>] [%c32, %c32] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">32</span>x32xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt; to memref<span class="string">&lt;?x?xf16, strided&lt;[32, 1]&gt;</span>, #gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">  <span class="comment">// 将C的subview拷贝到workgroup memory中，subview_2是workgroup memory</span></span><br><span class="line">  memref.copy %subview_1, %subview_2 &#123;__internal_linalg_transform__ = <span class="string">&quot;copy_to_workgroup_memory&quot;</span>&#125; : memref&lt;<span class="number">32</span>x32xf16, strided&lt;[<span class="number">512</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;?x?xf16, strided&lt;[32, 1]&gt;</span>, #gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">  <span class="comment">// 计算matmul，核心计算逻辑</span></span><br><span class="line">  linalg.matmul &#123;__internal_linalg_transform__ = <span class="string">&quot;workgroup_memory&quot;</span>, lowering_config = #iree_codegen.lowering_config&lt;tile_sizes = [[<span class="number">32</span>, <span class="number">32</span>, <span class="number">16</span>]]&gt;&#125; <span class="built_in">ins</span>(%subview, %subview_0 : memref&lt;<span class="number">32</span>x128xf16, strided&lt;[<span class="number">128</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;, memref<span class="string">&lt;128x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;) outs(%subview_2 : memref<span class="string">&lt;?x?xf16, strided&lt;[32, 1]&gt;</span>, #gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;)</span></span><br><span class="line">  <span class="comment">// 将C的结果拷贝回到external memory中</span></span><br><span class="line">  memref.copy %subview_2, %subview_1 &#123;__internal_linalg_transform__ = <span class="string">&quot;copy_to_workgroup_memory&quot;</span>&#125; : memref&lt;?x?xf16, strided&lt;[<span class="number">32</span>, <span class="number">1</span>]&gt;, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt; to memref<span class="string">&lt;32x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>重点是：将C矩阵存储在workgroup memory中。</p>
<p><img src="/images/image-20250513230338216.png" alt="image-20250513230338216"></p>
<p>在上图的代码比对中，可以看到如下变化：</p>
<ul>
<li>显示的<code>memref.alloc</code>操作，并且<code>address_space</code>设置为<code>workgroup</code>。</li>
<li>由于shared memory而导致的memref.copy（从shared mem显示加载）和memref.subview操作。</li>
</ul>
<blockquote>
<p> 为什么这一步仅仅提升C矩阵，而不提升A,B矩阵，具体原因参考<a href="http://arxiv.org/abs/2108.13191">MLIR GPU代码生成</a>论文，这里截取一下论文的原文辅助理解：</p>
<p><img src="/images/image-20250513182833242.png" alt="image-20250513182833242"></p>
</blockquote>
<h5 id="Step3（继续tiling，将K从128降维成16）"><a href="#Step3（继续tiling，将K从128降维成16）" class="headerlink" title="Step3（继续tiling，将K从128降维成16）"></a><font color = brown>Step3（继续tiling，将K从128降维成16）</font></h5><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">func.func @<span class="built_in">matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16</span>() attributes &#123;translation_info = #iree_codegen.translation_info&lt;pipeline = LLVMGPUMatmulTensorCore workgroup_size = [<span class="number">64</span>, <span class="number">2</span>, <span class="number">1</span>], &#123;pipeline_depth = <span class="number">3</span> : i64, store_stage = <span class="number">1</span> : i64&#125;&gt;&#125; &#123;</span><br><span class="line">  %c16 = arith.constant <span class="number">16</span> : index</span><br><span class="line">  %c128 = arith.constant <span class="number">128</span> : index</span><br><span class="line">  %c0 = arith.constant <span class="number">0</span> : index</span><br><span class="line">  %alloc = memref.<span class="built_in">alloc</span>() : memref&lt;<span class="number">32</span>x32xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">0</span> = hal.interface.binding.subspan <span class="built_in">layout</span>(&lt;bindings = [<span class="meta">#hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, Indirect&gt;</span>], flags = Indirect&gt;) binding(0) alignment(64) offset(%c0) flags(<span class="string">&quot;ReadOnly|Indirect&quot;</span>) : memref<span class="string">&lt;512x128xf16, #hal.descriptor_type&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  memref.assume_alignment %<span class="number">0</span>, <span class="number">64</span> : memref&lt;<span class="number">512</span>x128xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">1</span> = hal.interface.binding.subspan <span class="built_in">layout</span>(&lt;bindings = [<span class="meta">#hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, Indirect&gt;</span>], flags = Indirect&gt;) binding(1) alignment(64) offset(%c0) flags(<span class="string">&quot;ReadOnly|Indirect&quot;</span>) : memref<span class="string">&lt;128x512xf16, #hal.descriptor_type&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  memref.assume_alignment %<span class="number">1</span>, <span class="number">64</span> : memref&lt;<span class="number">128</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">2</span> = hal.interface.binding.subspan <span class="built_in">layout</span>(&lt;bindings = [<span class="meta">#hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, Indirect&gt;</span>], flags = Indirect&gt;) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<span class="string">&lt;512x512xf16, #hal.descriptor_type&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  memref.assume_alignment %<span class="number">2</span>, <span class="number">64</span> : memref&lt;<span class="number">512</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %workgroup_id_x = hal.interface.workgroup.id[<span class="number">0</span>] : index</span><br><span class="line">  %workgroup_id_y = hal.interface.workgroup.id[<span class="number">1</span>] : index</span><br><span class="line">  %<span class="number">3</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0] -&gt; (s0 * <span class="number">32</span>)&gt;()[%workgroup_id_y]</span><br><span class="line">  %subview = memref.subview %<span class="number">0</span>[%<span class="number">3</span>, <span class="number">0</span>] [<span class="number">32</span>, <span class="number">128</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">512</span>x128xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x128xf16, strided&lt;[128, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">4</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0] -&gt; (s0 * <span class="number">32</span>)&gt;()[%workgroup_id_x]</span><br><span class="line">  %subview_0 = memref.subview %<span class="number">1</span>[<span class="number">0</span>, %<span class="number">4</span>] [<span class="number">128</span>, <span class="number">32</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">128</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;128x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %subview_1 = memref.subview %<span class="number">2</span>[%<span class="number">3</span>, %<span class="number">4</span>] [<span class="number">32</span>, <span class="number">32</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">512</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  memref.copy %subview_1, %alloc &#123;__internal_linalg_transform__ = <span class="string">&quot;copy_to_workgroup_memory&quot;</span>&#125; : memref&lt;<span class="number">32</span>x32xf16, strided&lt;[<span class="number">512</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x32xf16, #gpu.address_space&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">  scf.<span class="keyword">for</span> %arg0 = %c0 to %c128 step %c16 &#123;</span><br><span class="line">    %subview_2 = memref.subview %subview[<span class="number">0</span>, %arg0] [<span class="number">32</span>, <span class="number">16</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">32</span>x128xf16, strided&lt;[<span class="number">128</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x16xf16, strided&lt;[128, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">    %subview_3 = memref.subview %subview_0[%arg0, <span class="number">0</span>] [<span class="number">16</span>, <span class="number">32</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">128</span>x32xf16, strided&lt;[<span class="number">512</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;16x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">    linalg.matmul &#123;__internal_linalg_transform__ = <span class="string">&quot;workgroup_k_tiled&quot;</span>, lowering_config = #iree_codegen.lowering_config&lt;tile_sizes = [[<span class="number">32</span>, <span class="number">32</span>, <span class="number">16</span>]]&gt;&#125; <span class="built_in">ins</span>(%subview_2, %subview_3 : memref&lt;<span class="number">32</span>x16xf16, strided&lt;[<span class="number">128</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;, memref<span class="string">&lt;16x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;) outs(%alloc : memref<span class="string">&lt;32x32xf16, #gpu.address_space&lt;workgroup&gt;</span>&gt;)</span></span><br><span class="line">  &#125;</span><br><span class="line">  memref.copy %alloc, %subview_1 &#123;__internal_linalg_transform__ = <span class="string">&quot;copy_to_workgroup_memory&quot;</span>&#125; : memref&lt;<span class="number">32</span>x32xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt; to memref<span class="string">&lt;32x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>继续完成k维度的tiling，从128变成16。</p>
<p><img src="/images/image-20250513230638654.png" alt="image-20250513230638654"></p>
<p>如上图所示，reduction维度变成显示的循环，以及删除冗余的memref.copy操作。</p>
<p>这个处理有趣的点是将linalg.matmul的workgroup_memory变成了workgroup_k_tiled，具体阅读源码中的<code>tileReductionLoops</code>函数：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">/// Tiles to workgroup level. Workgroup tiling is done at the flow level but we</span></span><br><span class="line"><span class="comment">/// may have extra tiling for the reduction dimension. Therefore we tile again</span></span><br><span class="line"><span class="comment">/// without distributing.</span></span><br><span class="line"><span class="function"><span class="type">static</span> LogicalResult <span class="title">tileReductionLoops</span><span class="params">(mlir::FunctionOpInterface funcOp)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">auto</span> tileSizesFn = [](OpBuilder &amp;builder,</span><br><span class="line">                        Operation *op) -&gt; SmallVector&lt;OpFoldResult&gt; &#123;</span><br><span class="line">    <span class="keyword">auto</span> interfaceOp = <span class="built_in">cast</span>&lt;PartitionableLoopsInterface&gt;(*op);      <span class="comment">// 获取可分块的循环组</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Returns the loop IDs (0 being outermost) that are partitionable.</span></span><br><span class="line">    <span class="comment">// </span></span><br><span class="line">    <span class="comment">// If `maxNumPartitionedLoops` is passed the size of the vector returned</span></span><br><span class="line">    <span class="comment">// is always lesser than the value.</span></span><br><span class="line">    <span class="keyword">auto</span> partitionedLoops =</span><br><span class="line">        interfaceOp.<span class="built_in">getPartitionableLoops</span>(kNumMaxParallelDims);                <span class="comment">// 获取已在workgroup中分块的循环         </span></span><br><span class="line">    SmallVector&lt;OpFoldResult&gt; tileSizes =</span><br><span class="line">        <span class="built_in">getAsIndexOpFoldResult</span>(op-&gt;<span class="built_in">getContext</span>(), <span class="built_in">getTileSizes</span>(op, <span class="number">0</span>));</span><br><span class="line">    <span class="keyword">auto</span> zeroAttr = builder.<span class="built_in">getIndexAttr</span>(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> depth : partitionedLoops) &#123;             </span><br><span class="line">      <span class="keyword">if</span> (depth &lt; tileSizes.<span class="built_in">size</span>()) &#123;                <span class="comment">// 将已分配到工作组的并行循环平铺尺寸设为0（跳过）</span></span><br><span class="line">        tileSizes[depth] = zeroAttr;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> numLoops = <span class="built_in">cast</span>&lt;TilingInterface&gt;(op).<span class="built_in">getLoopIteratorTypes</span>().<span class="built_in">size</span>();                       <span class="comment">//  确保分块尺寸向量（tileSizes）的长度与操作的循环维度数量一致，并为未明确指定分块大小的维度设置默认不分块（即分块大小为0）       </span></span><br><span class="line">    tileSizes.<span class="built_in">resize</span>(numLoops, zeroAttr);</span><br><span class="line">    <span class="keyword">return</span> tileSizes;</span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> tilingOptions =</span><br><span class="line">      scf::<span class="built_in">SCFTilingOptions</span>().<span class="built_in">setTileSizeComputationFunction</span>(tileSizesFn);</span><br><span class="line"></span><br><span class="line">  MLIRContext *context = funcOp.<span class="built_in">getContext</span>();</span><br><span class="line">  <span class="function">LinalgTransformationFilter <span class="title">filter</span><span class="params">(                                                                   <span class="comment">//  处理源操作中带有workgroup memory标记的操作</span></span></span></span><br><span class="line"><span class="params"><span class="function">      ArrayRef&lt;StringAttr&gt;&#123;</span></span></span><br><span class="line"><span class="params"><span class="function">          StringAttr::get(context, getWorkgroupMemoryMarker())&#125;,</span></span></span><br><span class="line"><span class="params"><span class="function">      StringAttr::get(context, getWorkgroupKTiledMarker()))</span></span>;                        <span class="comment">// 处理好后的打上tilemarker</span></span><br><span class="line">  filter.<span class="built_in">setMatchByDefault</span>();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">tileLinalgOpsWithFilter</span>(funcOp, tilingOptions, filter);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><img src="/images/image-20250514205936633.png" alt="image-20250514205936633"></p>
<p>上图大体总结了一下<code>tileToSerialLoops</code>的调用链。这段代码比较重要的一个函数是<code>tileSCF()</code>，该函数底层调用<code>tileUsingSCF()</code>函数实现，感兴趣读者可以自行阅读<code>mlir/lib/Dialect/SCF/Transforms/TileUsingInterface.cpp</code>中的源码实现。</p>
<h5 id="Step4（当workgroup中的线程数大于warp，则A和B也可以做提升）"><a href="#Step4（当workgroup中的线程数大于warp，则A和B也可以做提升）" class="headerlink" title="Step4（当workgroup中的线程数大于warp，则A和B也可以做提升）"></a><font color = brown>Step4（当workgroup中的线程数大于warp，则A和B也可以做提升）</font></h5><p>这一步骤主要完成如下操作：</p>
<ul>
<li>额外在 shared memory 中申请了两块儿 buffer，用来缓存 A ，B 矩阵</li>
<li>在所有涉及到 shared memory 读写的地方（memref.copy）前后加上 <code>gpu.barrier</code>。</li>
</ul>
<p>接下来结合具体的代码逻辑进行解读：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">SmallVector&lt;<span class="type">int64_t</span>&gt; workgroupSize = maybeWorkgroupSize.<span class="built_in">value</span>();</span><br><span class="line"><span class="type">int64_t</span> flatWorkgroupSize =</span><br><span class="line">    workgroupSize[<span class="number">0</span>] * workgroupSize[<span class="number">1</span>] * workgroupSize[<span class="number">2</span>];</span><br><span class="line"><span class="comment">// Only promote to workgroup size if there are multiple warps.</span></span><br><span class="line"><span class="keyword">if</span> (flatWorkgroupSize &gt; kWarpSize) &#123;                             <span class="comment">// 以[64,2,1]为例，flatWorkgroupSize=128时，表示有2x2个warp，所以切分成warp是有必要的</span></span><br><span class="line">  <span class="function">RewritePatternSet <span class="title">promotionPatterns</span><span class="params">(&amp;getContext())</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">populateContractPromotionPatterns</span>(promotionPatterns, &#123;<span class="number">0</span>, <span class="number">1</span>&#125;);     <span class="comment">// 0和1表示对A/B矩阵进行提升</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (<span class="built_in">failed</span>(<span class="built_in">applyPatternsAndFoldGreedily</span>(funcOp,</span><br><span class="line">                                          std::<span class="built_in">move</span>(promotionPatterns)))) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">signalPassFailure</span>();</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Insert barriers before and after copies to workgroup memory.</span></span><br><span class="line">  <span class="built_in">insertBarriersAroundSharedMemoryCopy</span>(funcOp);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这段代码的判断条件是workgroup的[0]，[1]，[2]相乘是否大于warp，大于说明workgroup内部也需要做warp的tiling。这也导致A和B会有重用的可能性，因此最好也存入shared memory中。这一步骤的逻辑具体参考<code>populateContractPromotionPatterns()</code>：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">/// 这段代码是 MLIR（Multi-Level Intermediate Representation）中的一个 重写模式（Rewrite Pattern），</span></span><br><span class="line"><span class="comment">/// 它用于优化 linalg::MatmulOp、linalg::BatchMatmulOp 和 linalg::GenericOp，</span></span><br><span class="line"><span class="comment">/// 具体是对某些操作数（operands）进行优化提升（promotion），使其在 workgroup memory（工作组共享内存） 中进行计算，</span></span><br><span class="line"><span class="comment">/// 以提高计算效率（主要面向 GPU 计算） </span></span><br><span class="line"><span class="comment">/// 整个pass都是基于mlir提供的基于linalg的优化而来，需要系统学习linalg的优化。</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">populateContractPromotionPatterns</span><span class="params">(RewritePatternSet &amp;patterns,</span></span></span><br><span class="line"><span class="params"><span class="function">                                       ArrayRef&lt;<span class="type">int64_t</span>&gt; operandsToPromote)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 一个整数数组，表示要提升（promote）到工作组共享内存的操作数索引（通常是 A, B 矩阵）</span></span><br><span class="line">  MLIRContext *context = patterns.<span class="built_in">getContext</span>();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 将linalg::MatmulOp，linalg::BatchMatmulOp，linalg::GenericOp这三种op的operandsToPromote进行优化</span></span><br><span class="line">  <span class="comment">// insert写法具体看PatternMatch.h的写法。</span></span><br><span class="line">  <span class="comment">// LinalgPromotionPattern提供普适的linalg pattern的优化 </span></span><br><span class="line">  patterns.insert&lt;LinalgPromotionPattern&lt;linalg::MatmulOp&gt;,</span><br><span class="line">                  LinalgPromotionPattern&lt;linalg::BatchMatmulOp&gt;,</span><br><span class="line">                  LinalgPromotionPattern&lt;linalg::GenericOp&gt;&gt;(</span><br><span class="line">      context,</span><br><span class="line">      <span class="comment">// 设定优化参数</span></span><br><span class="line">      linalg::<span class="built_in">LinalgPromotionOptions</span>()</span><br><span class="line">          <span class="comment">// 设置内存释放函数</span></span><br><span class="line">          .<span class="built_in">setAllocationDeallocationFns</span>(allocateWorkgroupMemory,</span><br><span class="line">                                        deallocateWorkgroupMemory)</span><br><span class="line">          <span class="comment">// 设置拷贝函数</span></span><br><span class="line">          .<span class="built_in">setCopyInOutFns</span>(copyToWorkgroupMemory, copyToWorkgroupMemory)</span><br><span class="line">          <span class="comment">// 指定要提升的操作数</span></span><br><span class="line">          .<span class="built_in">setOperandsToPromote</span>(operandsToPromote)</span><br><span class="line">          <span class="comment">// 设置提升的维度</span></span><br><span class="line">          .<span class="built_in">setUseFullTileBuffers</span>(&#123;<span class="literal">false</span>, <span class="literal">false</span>&#125;),</span><br><span class="line">      <span class="built_in">LinalgTransformationFilter</span>(</span><br><span class="line">          &#123;StringAttr::<span class="built_in">get</span>(context, <span class="built_in">getWorkgroupKTiledMarker</span>())&#125;,</span><br><span class="line">          StringAttr::<span class="built_in">get</span>(context, <span class="built_in">getWorkgroupMemoryMarker</span>()))</span><br><span class="line">          .<span class="built_in">setMatchByDefault</span>()</span><br><span class="line">          .<span class="built_in">addFilter</span>(contractOpFilter));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这段代码很好地体现MLIR基础设施的可复用性的强大之处以及Google团队的良好封装。代码中最重要的点是<code>LinalgPromotionPattern</code>，这是IREE对于MLIR的<code>RewritePattern</code>的封装和扩展。具体的依赖关系如下所示：</p>
<p><img src="/images/image-20250514214336495.png" alt="image-20250514214336495"></p>
<p>我们先来解读第一层封装：<code>LinalgBasePromotionPatter</code></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//===----------------------------------------------------------------------===//</span></span><br><span class="line"><span class="comment">// Transformations exposed as patterns, moved from upstream MLIR as IREE still</span></span><br><span class="line"><span class="comment">// heavily relies on patterns that compose through filters.</span></span><br><span class="line"><span class="comment">// <span class="doctag">TODO:</span> Deprecate all the code below.</span></span><br><span class="line"><span class="comment">//===----------------------------------------------------------------------===//</span></span><br><span class="line"><span class="comment">///</span></span><br><span class="line"><span class="comment">/// Linalg promotion patterns.</span></span><br><span class="line"><span class="comment">///</span></span><br><span class="line"><span class="comment">/// Apply the `promoteSubViews` transformation as a pattern.</span></span><br><span class="line"><span class="comment">/// `filter` controls LinalgTransformMarker matching and update when specified.</span></span><br><span class="line"><span class="comment">/// See `promoteSubViews` for more details.</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">LinalgBasePromotionPattern</span> : <span class="keyword">public</span> RewritePattern &#123;</span><br><span class="line">  <span class="comment">/// Entry point to match any LinalgOp</span></span><br><span class="line">  <span class="comment">/// OpInterface. MatchAnyOpTag-based constructor</span></span><br><span class="line">  <span class="comment">/// with a mandatory `filter`.</span></span><br><span class="line">  <span class="built_in">LinalgBasePromotionPattern</span>(</span><br><span class="line">      MLIRContext *context, LinalgTransformationFilter f,</span><br><span class="line">      linalg::LinalgPromotionOptions options = linalg::<span class="built_in">LinalgPromotionOptions</span>(),</span><br><span class="line">      PatternBenefit benefit = <span class="number">1</span>)</span><br><span class="line">      : <span class="built_in">RewritePattern</span>(<span class="built_in">MatchAnyOpTypeTag</span>(), benefit, context),</span><br><span class="line">        <span class="built_in">filter</span>(std::<span class="built_in">move</span>(f)), <span class="built_in">options</span>(std::<span class="built_in">move</span>(options)) &#123;&#125;</span><br><span class="line">  <span class="comment">/// Entry point to match a specific Linalg op.</span></span><br><span class="line">  <span class="built_in">LinalgBasePromotionPattern</span>(</span><br><span class="line">      StringRef opName, MLIRContext *context,</span><br><span class="line">      linalg::LinalgPromotionOptions options,</span><br><span class="line">      LinalgTransformationFilter f = <span class="built_in">LinalgTransformationFilter</span>(),</span><br><span class="line">      PatternBenefit benefit = <span class="number">1</span>)</span><br><span class="line">      : <span class="built_in">RewritePattern</span>(opName, benefit, context, &#123;&#125;), <span class="built_in">filter</span>(std::<span class="built_in">move</span>(f)),</span><br><span class="line">        <span class="built_in">options</span>(std::<span class="built_in">move</span>(options)) &#123;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">LogicalResult <span class="title">matchAndRewrite</span><span class="params">(Operation *op,</span></span></span><br><span class="line"><span class="params"><span class="function">                                PatternRewriter &amp;rewriter)</span> <span class="type">const</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">failed</span>(filter.<span class="built_in">checkAndNotify</span>(rewriter, op)))</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">failure</span>();</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">failed</span>(<span class="built_in">promoteSubviewsPrecondition</span>(op, options)))</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">failure</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> We cannot use root update here. This</span></span><br><span class="line">    <span class="comment">// pattern is creating other ops, so if the</span></span><br><span class="line">    <span class="comment">// promotion fails, those need to be cleaned</span></span><br><span class="line">    <span class="comment">// up, which doesnt seem to be happening here.</span></span><br><span class="line">    <span class="comment">// So to fail properly, we should be cloning</span></span><br><span class="line">    <span class="comment">// the op and deleting the previous op. This</span></span><br><span class="line">    <span class="comment">// needs more investigation.</span></span><br><span class="line">    rewriter.<span class="built_in">startOpModification</span>(op);</span><br><span class="line">    std::optional&lt;linalg::LinalgOp&gt; promotedOp =</span><br><span class="line">        <span class="built_in">promoteSubViews</span>(rewriter, <span class="built_in">cast</span>&lt;linalg::LinalgOp&gt;(op), options);</span><br><span class="line">    <span class="keyword">if</span> (!promotedOp) &#123;</span><br><span class="line">      rewriter.<span class="built_in">cancelOpModification</span>(op);</span><br><span class="line">      <span class="keyword">return</span> op-&gt;<span class="built_in">emitError</span>(<span class="string">&quot;subview promotion failed&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    rewriter.<span class="built_in">finalizeOpModification</span>(op);</span><br><span class="line">    filter.<span class="built_in">replaceLinalgTransformationFilter</span>(rewriter, op);</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">success</span>();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  <span class="comment">/// LinalgTransformMarker handles special</span></span><br><span class="line">  <span class="comment">/// attribute manipulations.</span></span><br><span class="line">  LinalgTransformationFilter filter;</span><br><span class="line">  <span class="comment">/// Promotion options.</span></span><br><span class="line">  linalg::LinalgPromotionOptions options;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>可以看到，在类的构造中，最重要的是<code>LinalgTransformationFilter</code>和<code>LinalgPromotionOptions</code>。这两个具体内容在patterns.insert中定义好了：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">patterns.insert&lt;LinalgPromotionPattern&lt;linalg::MatmulOp&gt;,</span><br><span class="line">                  LinalgPromotionPattern&lt;linalg::BatchMatmulOp&gt;,</span><br><span class="line">                  LinalgPromotionPattern&lt;linalg::GenericOp&gt;&gt;(</span><br><span class="line">      context,</span><br><span class="line">      <span class="comment">// 设定优化参数</span></span><br><span class="line">      linalg::<span class="built_in">LinalgPromotionOptions</span>()</span><br><span class="line">          <span class="comment">// 设置内存释放函数</span></span><br><span class="line">          .<span class="built_in">setAllocationDeallocationFns</span>(allocateWorkgroupMemory,</span><br><span class="line">                                        deallocateWorkgroupMemory)</span><br><span class="line">          <span class="comment">// 设置拷贝函数</span></span><br><span class="line">          .<span class="built_in">setCopyInOutFns</span>(copyToWorkgroupMemory, copyToWorkgroupMemory)</span><br><span class="line">          <span class="comment">// 指定要提升的操作数</span></span><br><span class="line">          .<span class="built_in">setOperandsToPromote</span>(operandsToPromote)</span><br><span class="line">          <span class="comment">// 设置提升的维度</span></span><br><span class="line">          .<span class="built_in">setUseFullTileBuffers</span>(&#123;<span class="literal">false</span>, <span class="literal">false</span>&#125;),</span><br><span class="line">      <span class="built_in">LinalgTransformationFilter</span>(</span><br><span class="line">          &#123;StringAttr::<span class="built_in">get</span>(context, <span class="built_in">getWorkgroupKTiledMarker</span>())&#125;,</span><br><span class="line">          StringAttr::<span class="built_in">get</span>(context, <span class="built_in">getWorkgroupMemoryMarker</span>()))</span><br><span class="line">          .<span class="built_in">setMatchByDefault</span>()</span><br><span class="line">          .<span class="built_in">addFilter</span>(contractOpFilter));</span><br></pre></td></tr></table></figure>

<p>其中，<code>LinalgTransformationFilter</code>和我们之前遇到的作用一样，将<code>WorkgroupKTiledMark</code>变成<code>WorkgroupMemoryMark</code>。而<code>LinalgPromotionOptions</code>的定义特别有趣，显示指定了如何做内存释放，如何设置拷贝函数，要提升的操作数是哪个（0,1对应lianalg.xxOp的operand 0和1，在我们的case中是A和B）以及提升的维度。追溯<code>LinalgPromotionOptions</code>的定义，可以看到该类是LLVM公共class，通过回调的方式，允许用户自定义这些操作具体的执行逻辑。</p>
<p>讲解完这个类的构造，另一个重点就是<code>matchAndRewrite</code>实际的改写逻辑。阅读代码注释即可明白原理：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">LogicalResult <span class="title">matchAndRewrite</span><span class="params">(Operation *op, PatternRewriter &amp;rewriter)</span> <span class="type">const</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 步骤1: 检查过滤器和提升前提条件</span></span><br><span class="line">  <span class="keyword">if</span> (<span class="built_in">failed</span>(filter.<span class="built_in">checkAndNotify</span>(rewriter, op)) || </span><br><span class="line">      <span class="built_in">failed</span>(<span class="built_in">promoteSubviewsPrecondition</span>(op, options)))</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">failure</span>();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 步骤2: 尝试执行子视图提升</span></span><br><span class="line">  rewriter.<span class="built_in">startOpModification</span>(op);</span><br><span class="line">  std::optional&lt;linalg::LinalgOp&gt; promotedOp = </span><br><span class="line">      <span class="built_in">promoteSubViews</span>(rewriter, <span class="built_in">cast</span>&lt;linalg::LinalgOp&gt;(op), options);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 步骤3: 处理提升结果</span></span><br><span class="line">  <span class="keyword">if</span> (!promotedOp) &#123;</span><br><span class="line">    rewriter.<span class="built_in">cancelOpModification</span>(op); <span class="comment">// 失败时回滚</span></span><br><span class="line">    <span class="keyword">return</span> op-&gt;<span class="built_in">emitError</span>(<span class="string">&quot;subview promotion failed&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  rewriter.<span class="built_in">finalizeOpModification</span>(op);</span><br><span class="line">  filter.<span class="built_in">replaceLinalgTransformationFilter</span>(rewriter, op); <span class="comment">// 更新属性</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">success</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这段代码特别好的点是，这里的所有诸如<code>promoteSubviewsPrecondition()</code>函数均是Linalg Dialect的transform原生支持的。<strong>因此这段IREE的封装是很好的处理memory promotion问题的范例，可以直接移植入用在别的项目。</strong></p>
<p>解读好第一层封装，我们来到最上层封装：<code>LinalgPromotionPattern</code></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> OpTy&gt;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">LinalgPromotionPattern</span> : <span class="keyword">public</span> LinalgBasePromotionPattern &#123;</span><br><span class="line">  <span class="comment">/// SFINAE: This constructor can only trigger for</span></span><br><span class="line">  <span class="comment">/// concrete ops that have a static</span></span><br><span class="line">  <span class="comment">/// `getOperationName` method.</span></span><br><span class="line">  <span class="keyword">template</span> &lt;<span class="keyword">typename</span> ConcreateOpTy = OpTy&gt;</span><br><span class="line">  <span class="built_in">LinalgPromotionPattern</span>(</span><br><span class="line">      MLIRContext *context, linalg::LinalgPromotionOptions options,</span><br><span class="line">      LinalgTransformationFilter f = <span class="built_in">LinalgTransformationFilter</span>(),</span><br><span class="line">      PatternBenefit benefit = <span class="number">1</span>)</span><br><span class="line">      : <span class="built_in">LinalgBasePromotionPattern</span>(OpTy::<span class="built_in">getOperationName</span>(), context, options,</span><br><span class="line">                                   f, benefit) &#123;&#125;</span><br><span class="line">  <span class="comment">/// This constructor is available to anyone.</span></span><br><span class="line">  <span class="built_in">LinalgPromotionPattern</span>(</span><br><span class="line">      StringRef opName, MLIRContext *context,</span><br><span class="line">      linalg::LinalgPromotionOptions options,</span><br><span class="line">      LinalgTransformationFilter f = <span class="built_in">LinalgTransformationFilter</span>(),</span><br><span class="line">      PatternBenefit benefit = <span class="number">1</span>)</span><br><span class="line">      : <span class="built_in">LinalgBasePromotionPattern</span>(opName, context, options, f, benefit) &#123;&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>这段代码唯一的trick是，第一个构造函数的模板参数typename ConcreateOpTy &#x3D; OpTy配合OpTy::getOperationName()形成**<a href="https://mq-b.github.io/Modern-Cpp-templates-tutorial/md/%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/10%E4%BA%86%E8%A7%A3%E4%B8%8E%E5%88%A9%E7%94%A8SFINAE">SFINAE约束</a>**。当OpTy不包含静态getOperationName()方法时，该构造函数被丢弃，不会导致编译错误，起到保护机制。</p>
<p>这一阶段生成的最终IR如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">func.func @<span class="built_in">matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16</span>() attributes &#123;translation_info = #iree_codegen.translation_info&lt;pipeline = LLVMGPUMatmulTensorCore workgroup_size = [<span class="number">64</span>, <span class="number">2</span>, <span class="number">1</span>], &#123;pipeline_depth = <span class="number">3</span> : i64, store_stage = <span class="number">1</span> : i64&#125;&gt;&#125; &#123;</span><br><span class="line">  %c0 = arith.constant <span class="number">0</span> : index</span><br><span class="line">  %c128 = arith.constant <span class="number">128</span> : index</span><br><span class="line">  %c16 = arith.constant <span class="number">16</span> : index</span><br><span class="line">  %alloc = memref.<span class="built_in">alloc</span>() : memref&lt;<span class="number">16</span>x32xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">  %alloc_0 = memref.<span class="built_in">alloc</span>() : memref&lt;<span class="number">32</span>x16xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">  %alloc_1 = memref.<span class="built_in">alloc</span>() : memref&lt;<span class="number">32</span>x32xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">0</span> = hal.interface.binding.subspan <span class="built_in">layout</span>(&lt;bindings = [<span class="meta">#hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, Indirect&gt;</span>], flags = Indirect&gt;) binding(0) alignment(64) offset(%c0) flags(<span class="string">&quot;ReadOnly|Indirect&quot;</span>) : memref<span class="string">&lt;512x128xf16, #hal.descriptor_type&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  memref.assume_alignment %<span class="number">0</span>, <span class="number">64</span> : memref&lt;<span class="number">512</span>x128xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">1</span> = hal.interface.binding.subspan <span class="built_in">layout</span>(&lt;bindings = [<span class="meta">#hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, Indirect&gt;</span>], flags = Indirect&gt;) binding(1) alignment(64) offset(%c0) flags(<span class="string">&quot;ReadOnly|Indirect&quot;</span>) : memref<span class="string">&lt;128x512xf16, #hal.descriptor_type&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  memref.assume_alignment %<span class="number">1</span>, <span class="number">64</span> : memref&lt;<span class="number">128</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">2</span> = hal.interface.binding.subspan <span class="built_in">layout</span>(&lt;bindings = [<span class="meta">#hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, Indirect&gt;</span>], flags = Indirect&gt;) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<span class="string">&lt;512x512xf16, #hal.descriptor_type&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  memref.assume_alignment %<span class="number">2</span>, <span class="number">64</span> : memref&lt;<span class="number">512</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %workgroup_id_x = hal.interface.workgroup.id[<span class="number">0</span>] : index</span><br><span class="line">  %workgroup_id_y = hal.interface.workgroup.id[<span class="number">1</span>] : index</span><br><span class="line">  %<span class="number">3</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0] -&gt; (s0 * <span class="number">32</span>)&gt;()[%workgroup_id_y]</span><br><span class="line">  %subview = memref.subview %<span class="number">0</span>[%<span class="number">3</span>, <span class="number">0</span>] [<span class="number">32</span>, <span class="number">128</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">512</span>x128xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x128xf16, strided&lt;[128, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">4</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0] -&gt; (s0 * <span class="number">32</span>)&gt;()[%workgroup_id_x]</span><br><span class="line">  %subview_2 = memref.subview %<span class="number">1</span>[<span class="number">0</span>, %<span class="number">4</span>] [<span class="number">128</span>, <span class="number">32</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">128</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;128x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %subview_3 = memref.subview %<span class="number">2</span>[%<span class="number">3</span>, %<span class="number">4</span>] [<span class="number">32</span>, <span class="number">32</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">512</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  gpu.barrier</span><br><span class="line">  memref.copy %subview_3, %alloc_1 &#123;__internal_linalg_transform__ = <span class="string">&quot;copy_to_workgroup_memory&quot;</span>&#125; : memref&lt;<span class="number">32</span>x32xf16, strided&lt;[<span class="number">512</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x32xf16, #gpu.address_space&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">  gpu.barrier</span><br><span class="line">  scf.<span class="keyword">for</span> %arg0 = %c0 to %c128 step %c16 &#123;</span><br><span class="line">    %subview_4 = memref.subview %subview[<span class="number">0</span>, %arg0] [<span class="number">32</span>, <span class="number">16</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">32</span>x128xf16, strided&lt;[<span class="number">128</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x16xf16, strided&lt;[128, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">    %subview_5 = memref.subview %subview_2[%arg0, <span class="number">0</span>] [<span class="number">16</span>, <span class="number">32</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">128</span>x32xf16, strided&lt;[<span class="number">512</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;16x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">    gpu.barrier</span><br><span class="line">    memref.copy %subview_4, %alloc_0 &#123;__internal_linalg_transform__ = <span class="string">&quot;copy_to_workgroup_memory&quot;</span>&#125; : memref&lt;<span class="number">32</span>x16xf16, strided&lt;[<span class="number">128</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x16xf16, #gpu.address_space&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">    memref.copy %subview_5, %alloc &#123;__internal_linalg_transform__ = <span class="string">&quot;copy_to_workgroup_memory&quot;</span>&#125; : memref&lt;<span class="number">16</span>x32xf16, strided&lt;[<span class="number">512</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;16x32xf16, #gpu.address_space&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">    gpu.barrier</span><br><span class="line">    linalg.matmul &#123;__internal_linalg_transform__ = <span class="string">&quot;workgroup_memory&quot;</span>, lowering_config = #iree_codegen.lowering_config&lt;tile_sizes = [[<span class="number">32</span>, <span class="number">32</span>, <span class="number">16</span>]]&gt;&#125; <span class="built_in">ins</span>(%alloc_0, %alloc : memref&lt;<span class="number">32</span>x16xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;, memref<span class="string">&lt;16x32xf16, #gpu.address_space&lt;workgroup&gt;</span>&gt;) outs(%alloc_1 : memref<span class="string">&lt;32x32xf16, #gpu.address_space&lt;workgroup&gt;</span>&gt;)</span></span><br><span class="line">  &#125;</span><br><span class="line">  gpu.barrier</span><br><span class="line">  memref.copy %alloc_1, %subview_3 &#123;__internal_linalg_transform__ = <span class="string">&quot;copy_to_workgroup_memory&quot;</span>&#125; : memref&lt;<span class="number">32</span>x32xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt; to memref<span class="string">&lt;32x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  gpu.barrier</span><br><span class="line">  <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>和上一阶段相比，其不同是显而易见的，具体为A和B 提升到共享内存，更多的显示copy，以及gpu.barrier同步shared memory拷贝。</p>
<p><img src="/images/image-20250514220954451.png" alt="image-20250514220954451"></p>
<p><img src="/images/image-20250514221013687.png" alt="image-20250514221013687"></p>
<h5 id="Step5（针对warp进一步分块）"><a href="#Step5（针对warp进一步分块）" class="headerlink" title="Step5（针对warp进一步分块）"></a><font color= brown>Step5（针对warp进一步分块）</font></h5><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">func.func @<span class="built_in">matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16</span>() attributes &#123;translation_info = #iree_codegen.translation_info&lt;pipeline = LLVMGPUMatmulTensorCore workgroup_size = [<span class="number">64</span>, <span class="number">2</span>, <span class="number">1</span>], &#123;pipeline_depth = <span class="number">3</span> : i64, store_stage = <span class="number">1</span> : i64&#125;&gt;&#125; &#123;</span><br><span class="line">  %c0 = arith.constant <span class="number">0</span> : index</span><br><span class="line">  %c128 = arith.constant <span class="number">128</span> : index</span><br><span class="line">  %c16 = arith.constant <span class="number">16</span> : index</span><br><span class="line">  %alloc = memref.<span class="built_in">alloc</span>() : memref&lt;<span class="number">16</span>x32xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">  %alloc_0 = memref.<span class="built_in">alloc</span>() : memref&lt;<span class="number">32</span>x16xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">  %alloc_1 = memref.<span class="built_in">alloc</span>() : memref&lt;<span class="number">32</span>x32xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">0</span> = hal.interface.binding.subspan <span class="built_in">layout</span>(&lt;bindings = [<span class="meta">#hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, Indirect&gt;</span>], flags = Indirect&gt;) binding(0) alignment(64) offset(%c0) flags(<span class="string">&quot;ReadOnly|Indirect&quot;</span>) : memref<span class="string">&lt;512x128xf16, #hal.descriptor_type&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  memref.assume_alignment %<span class="number">0</span>, <span class="number">64</span> : memref&lt;<span class="number">512</span>x128xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">1</span> = hal.interface.binding.subspan <span class="built_in">layout</span>(&lt;bindings = [<span class="meta">#hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, Indirect&gt;</span>], flags = Indirect&gt;) binding(1) alignment(64) offset(%c0) flags(<span class="string">&quot;ReadOnly|Indirect&quot;</span>) : memref<span class="string">&lt;128x512xf16, #hal.descriptor_type&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  memref.assume_alignment %<span class="number">1</span>, <span class="number">64</span> : memref&lt;<span class="number">128</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">2</span> = hal.interface.binding.subspan <span class="built_in">layout</span>(&lt;bindings = [<span class="meta">#hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, Indirect&gt;</span>], flags = Indirect&gt;) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<span class="string">&lt;512x512xf16, #hal.descriptor_type&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  memref.assume_alignment %<span class="number">2</span>, <span class="number">64</span> : memref&lt;<span class="number">512</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %workgroup_id_x = hal.interface.workgroup.id[<span class="number">0</span>] : index</span><br><span class="line">  %workgroup_id_y = hal.interface.workgroup.id[<span class="number">1</span>] : index</span><br><span class="line">  %<span class="number">3</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0] -&gt; (s0 * <span class="number">32</span>)&gt;()[%workgroup_id_y]</span><br><span class="line">  %subview = memref.subview %<span class="number">0</span>[%<span class="number">3</span>, <span class="number">0</span>] [<span class="number">32</span>, <span class="number">128</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">512</span>x128xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x128xf16, strided&lt;[128, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">4</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0] -&gt; (s0 * <span class="number">32</span>)&gt;()[%workgroup_id_x]</span><br><span class="line">  %subview_2 = memref.subview %<span class="number">1</span>[<span class="number">0</span>, %<span class="number">4</span>] [<span class="number">128</span>, <span class="number">32</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">128</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;128x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %subview_3 = memref.subview %<span class="number">2</span>[%<span class="number">3</span>, %<span class="number">4</span>] [<span class="number">32</span>, <span class="number">32</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">512</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  gpu.barrier</span><br><span class="line">  memref.copy %subview_3, %alloc_1 &#123;__internal_linalg_transform__ = <span class="string">&quot;copy_to_workgroup_memory&quot;</span>&#125; : memref&lt;<span class="number">32</span>x32xf16, strided&lt;[<span class="number">512</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x32xf16, #gpu.address_space&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">  gpu.barrier</span><br><span class="line">  scf.<span class="keyword">for</span> %arg0 = %c0 to %c128 step %c16 &#123;</span><br><span class="line">    %subview_4 = memref.subview %subview[<span class="number">0</span>, %arg0] [<span class="number">32</span>, <span class="number">16</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">32</span>x128xf16, strided&lt;[<span class="number">128</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x16xf16, strided&lt;[128, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">    %subview_5 = memref.subview %subview_2[%arg0, <span class="number">0</span>] [<span class="number">16</span>, <span class="number">32</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">128</span>x32xf16, strided&lt;[<span class="number">512</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;16x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">    gpu.barrier</span><br><span class="line">    memref.copy %subview_4, %alloc_0 &#123;__internal_linalg_transform__ = <span class="string">&quot;copy_to_workgroup_memory&quot;</span>&#125; : memref&lt;<span class="number">32</span>x16xf16, strided&lt;[<span class="number">128</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x16xf16, #gpu.address_space&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">    memref.copy %subview_5, %alloc &#123;__internal_linalg_transform__ = <span class="string">&quot;copy_to_workgroup_memory&quot;</span>&#125; : memref&lt;<span class="number">16</span>x32xf16, strided&lt;[<span class="number">512</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;16x32xf16, #gpu.address_space&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">    gpu.barrier</span><br><span class="line">    %c0_6 = arith.constant <span class="number">0</span> : index</span><br><span class="line">    %c16_7 = arith.constant <span class="number">16</span> : index</span><br><span class="line">    %c16_8 = arith.constant <span class="number">16</span> : index</span><br><span class="line">    %thread_id_x = gpu.thread_id  x</span><br><span class="line">    %<span class="number">5</span> = affine.apply <span class="built_in">affine_map</span>&lt;(d0) -&gt; (d0 floordiv <span class="number">32</span>)&gt;(%thread_id_x)</span><br><span class="line">    %c2 = arith.constant <span class="number">2</span> : index</span><br><span class="line">    %thread_id_y = gpu.thread_id  y</span><br><span class="line">    %c2_9 = arith.constant <span class="number">2</span> : index</span><br><span class="line">    %c0_10 = arith.constant <span class="number">0</span> : index</span><br><span class="line">    %c32 = arith.constant <span class="number">32</span> : index</span><br><span class="line">    %c16_11 = arith.constant <span class="number">16</span> : index</span><br><span class="line">    %c0_12 = arith.constant <span class="number">0</span> : index</span><br><span class="line">    %c32_13 = arith.constant <span class="number">32</span> : index</span><br><span class="line">    %c16_14 = arith.constant <span class="number">16</span> : index</span><br><span class="line">    %<span class="number">6</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0, s1] -&gt; (s0 * s1)&gt;()[%thread_id_y, %c16_11]</span><br><span class="line">    %<span class="number">7</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0, s1] -&gt; (s0 + s1)&gt;()[%<span class="number">6</span>, %c0_10]</span><br><span class="line">    %<span class="number">8</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0, s1] -&gt; (s0 * s1)&gt;()[%c2_9, %c16_11]</span><br><span class="line">    scf.<span class="keyword">for</span> %arg1 = %<span class="number">7</span> to %c32 step %<span class="number">8</span> &#123;</span><br><span class="line">      %<span class="number">9</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0, s1] -&gt; (s0 * s1)&gt;()[%<span class="number">5</span>, %c16_14]</span><br><span class="line">      %<span class="number">10</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0, s1] -&gt; (s0 + s1)&gt;()[%<span class="number">9</span>, %c0_12]</span><br><span class="line">      %<span class="number">11</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0, s1] -&gt; (s0 * s1)&gt;()[%c2, %c16_14]</span><br><span class="line">      scf.<span class="keyword">for</span> %arg2 = %<span class="number">10</span> to %c32_13 step %<span class="number">11</span> &#123;</span><br><span class="line">        %subview_15 = memref.subview %alloc_0[%arg1, <span class="number">0</span>] [<span class="number">16</span>, <span class="number">16</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">32</span>x16xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt; to memref<span class="string">&lt;16x16xf16, strided&lt;[16, 1], offset: ?&gt;</span>, #gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">        %subview_16 = memref.subview %alloc[<span class="number">0</span>, %arg2] [<span class="number">16</span>, <span class="number">16</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">16</span>x32xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt; to memref<span class="string">&lt;16x16xf16, strided&lt;[32, 1], offset: ?&gt;</span>, #gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">        %subview_17 = memref.subview %alloc_1[%arg1, %arg2] [<span class="number">16</span>, <span class="number">16</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">32</span>x32xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt; to memref<span class="string">&lt;16x16xf16, strided&lt;[32, 1], offset: ?&gt;</span>, #gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">        linalg.matmul &#123;__internal_linalg_transform__ = <span class="string">&quot;vectorize&quot;</span>, lowering_config = #iree_codegen.lowering_config&lt;tile_sizes = [[<span class="number">32</span>, <span class="number">32</span>, <span class="number">16</span>]]&gt;&#125; <span class="built_in">ins</span>(%subview_15, %subview_16 : memref&lt;<span class="number">16</span>x16xf16, strided&lt;[<span class="number">16</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;, memref<span class="string">&lt;16x16xf16, strided&lt;[32, 1], offset: ?&gt;</span>, #gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;) outs(%subview_17 : memref<span class="string">&lt;16x16xf16, strided&lt;[32, 1], offset: ?&gt;</span>, #gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;)</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  gpu.barrier</span><br><span class="line">  memref.copy %alloc_1, %subview_3 &#123;__internal_linalg_transform__ = <span class="string">&quot;copy_to_workgroup_memory&quot;</span>&#125; : memref&lt;<span class="number">32</span>x32xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt; to memref<span class="string">&lt;32x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  gpu.barrier</span><br><span class="line">  <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这一步是为后续将<code>linalg.matmul</code>算子变成<code>WMMA API</code>做准备。workgroup size（表示有多少个线程） 以 warp 粒度（32个线程一个warp）对 thread block（[32,32,16]） 进行进一步的 tiling分块。我们的workgroup size是[64,2,1]，而warp是32，因此应该有[2,2,1]个warp。对thread blcok需要做[2,2,1]的tile拆分，为[32&#x2F;2, 32&#x2F;2, 16&#x2F;1] &#x3D; [16,16,16]。由于我们的代码是f16，根据<code>compiler/src/iree/compiler/Codegen/LLVMGPU/Verifiers.cpp</code>中的<code>getInstructionShape()</code>方法：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">/// Returns the shape of the math instruction for the given pipeline and input</span></span><br><span class="line"><span class="comment">/// element type.</span></span><br><span class="line"><span class="comment">/// 例如，对于 Tensor Core 管线，f16 和 bf16 类型对应的指令形状为 &#123;16, 16, 16&#125;，而 f32 类型对应 &#123;16, 16, 8&#125;。</span></span><br><span class="line"><span class="comment">/// Tensor Core的计算能力是硬件定死的，因此我们需要根据输入的数据类型来选择合适的指令形状。</span></span><br><span class="line"><span class="function"><span class="type">static</span> LogicalResult</span></span><br><span class="line"><span class="function"><span class="title">getInstructionShape</span><span class="params">(Operation *op, CodeGenPipeline pipeline,</span></span></span><br><span class="line"><span class="params"><span class="function">                    Type inputElementType,</span></span></span><br><span class="line"><span class="params"><span class="function">                    SmallVector&lt;<span class="type">int64_t</span>&gt; &amp;instructionShape)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">switch</span> (pipeline) &#123;</span><br><span class="line">  <span class="keyword">case</span> CodeGenPipeline::LLVMGPUMatmulSimt:</span><br><span class="line">    <span class="comment">// SIMT Pipeline / CUDA Cores</span></span><br><span class="line">    instructionShape = &#123;<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>&#125;;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 我们的pipeline是LLVMGPUMatmulTensorCore</span></span><br><span class="line">  <span class="keyword">case</span> CodeGenPipeline::LLVMGPUMatmulTensorCore:</span><br><span class="line">    <span class="comment">// Tensor Core Pipeline / WMMA API</span></span><br><span class="line">    <span class="comment">// 对于F16和BF16，tensorcore是16x16x16</span></span><br><span class="line">    <span class="keyword">if</span> (inputElementType.<span class="built_in">isF16</span>() || inputElementType.<span class="built_in">isBF16</span>()) &#123;</span><br><span class="line">      instructionShape = &#123;<span class="number">16</span>, <span class="number">16</span>, <span class="number">16</span>&#125;;</span><br><span class="line">    <span class="comment">// 对于F32，tensorcore是16x16x8</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (inputElementType.<span class="built_in">isF32</span>()) &#123;</span><br><span class="line">      instructionShape = &#123;<span class="number">16</span>, <span class="number">16</span>, <span class="number">8</span>&#125;;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> op-&gt;<span class="built_in">emitError</span>(</span><br><span class="line">          <span class="string">&quot;Expected f16, bf16 or f32 for Tensor Core (WMMA) pipeline&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> CodeGenPipeline::LLVMGPUMatmulTensorCoreMmaSync:</span><br><span class="line">    <span class="comment">// Tensor Core Pipeline / MMA.SYNC</span></span><br><span class="line">    <span class="keyword">if</span> (inputElementType.<span class="built_in">isF16</span>() || inputElementType.<span class="built_in">isBF16</span>()) &#123;</span><br><span class="line">      instructionShape = &#123;<span class="number">16</span>, <span class="number">8</span>, <span class="number">16</span>&#125;;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (inputElementType.<span class="built_in">isF32</span>()) &#123;</span><br><span class="line">      instructionShape = &#123;<span class="number">16</span>, <span class="number">8</span>, <span class="number">8</span>&#125;;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> op-&gt;<span class="built_in">emitError</span>(</span><br><span class="line">          <span class="string">&quot;Expected f16, bf16 or f32 for Tensor Core (MMA.SYNC) pipeline&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">default</span>:</span><br><span class="line">    <span class="keyword">return</span> op-&gt;<span class="built_in">emitError</span>(</span><br><span class="line">        <span class="string">&quot;Expected matmul SIMT, TensorCore(WMMA), or TensorCore(MMA.SYNC), &quot;</span></span><br><span class="line">        <span class="string">&quot;compilation pipeline&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">success</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>tensor core中的f16是[16,16,16] matmul矩阵运算。因此我们的尺寸正好匹配。这个warp tile步骤在源码中是<code>tileToInvocation()</code>：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">/// Patterns for thread level tiling.</span></span><br><span class="line"><span class="function"><span class="type">static</span> LogicalResult <span class="title">tileToInvocation</span><span class="params">(mlir::FunctionOpInterface funcOp,</span></span></span><br><span class="line"><span class="params"><span class="function">                                      SmallVectorImpl&lt;<span class="type">int64_t</span>&gt; &amp;workgroupSize)</span> </span>&#123;</span><br><span class="line">  linalg::TileSizeComputationFunction getInnerTileSizeFn =</span><br><span class="line">      [&amp;](OpBuilder &amp;builder, Operation *operation) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">calculateDistributedTileSize</span>(workgroupSize, builder, operation);    <span class="comment">// 计算tile size: [64/32, 2/1, 1/1] = [2, 2, 1]</span></span><br><span class="line">      &#125;;</span><br><span class="line">  <span class="keyword">auto</span> getThreadProcInfoFn =                                                            <span class="comment">// 显示计算生成thread id，不断向GPU的kernel表达靠近</span></span><br><span class="line">      [&amp;workgroupSize](OpBuilder &amp;builder, Location loc,</span><br><span class="line">                       ArrayRef&lt;Range&gt; parallelLoopRanges) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">getGPUThreadIdsAndCounts</span>(builder, loc, parallelLoopRanges.<span class="built_in">size</span>(),</span><br><span class="line">                                        workgroupSize);</span><br><span class="line">      &#125;;</span><br><span class="line">  linalg::LinalgLoopDistributionOptions invocationDistributionOptions;                             <span class="comment">// 设定thread的分布选项，设定procInfo</span></span><br><span class="line">  invocationDistributionOptions.procInfo = getThreadProcInfoFn;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> tilingOptions =</span><br><span class="line">      linalg::<span class="built_in">LinalgTilingOptions</span>()</span><br><span class="line">          .<span class="built_in">setLoopType</span>(linalg::LinalgTilingLoopType::Loops)</span><br><span class="line">          .<span class="built_in">setTileSizeComputationFunction</span>(getInnerTileSizeFn)</span><br><span class="line">          .<span class="built_in">setDistributionOptions</span>(invocationDistributionOptions);</span><br><span class="line"></span><br><span class="line">  MLIRContext *context = funcOp.<span class="built_in">getContext</span>();</span><br><span class="line">  <span class="function">LinalgTransformationFilter <span class="title">f</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      &#123;StringAttr::get(context, getWorkgroupKTiledMarker()),</span></span></span><br><span class="line"><span class="params"><span class="function">       StringAttr::get(context, getWorkgroupMemoryMarker())&#125;,</span></span></span><br><span class="line"><span class="params"><span class="function">      StringAttr::get(context, getVectorizeMarker()))</span></span>;</span><br><span class="line">  f.<span class="built_in">addFilter</span>([](Operation *op) &#123;</span><br><span class="line">     <span class="comment">// FFT doesn&#x27;t support second level of tiling yet.</span></span><br><span class="line">     <span class="keyword">return</span> <span class="built_in">success</span>(!<span class="built_in">isa</span>&lt;IREE::LinalgExt::FftOp&gt;(op));</span><br><span class="line">   &#125;).<span class="built_in">setMatchByDefault</span>();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">distributeLinalgOpsWithFilter</span>(funcOp, tilingOptions, f);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这一块搭配注释阅读源码已经很清晰了。相比前一个k维度tile，整体结构都是一样的（再一次感叹Google团队和MLIR社区的良好封装，大大简化冗余代码）。</p>
<p>这一步骤得到的代码变化如下：</p>
<p><img src="/images/image-20250514222617381.png" alt="image-20250514222617381"></p>
<p>最重要的点完成了分块，并分配好了每个线程完成的任务（通过thread_id_x和thread_id_y）：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">%<span class="number">5</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0] -&gt; (s0 * <span class="number">16</span>)&gt;()[%thread_id_y]  <span class="comment">// y方向起始行</span></span><br><span class="line">%<span class="number">6</span> = affine.apply <span class="built_in">affine_map</span>&lt;(d0) -&gt; ((d0 floordiv <span class="number">32</span>) * <span class="number">16</span>)&gt;(%thread_id_x) <span class="comment">// x方向起始列</span></span><br></pre></td></tr></table></figure>

<p>一个thread，通过（d0 floordiv 32）找到其属于的warp组，并通过*16找到其warp组负责的列。可以看到x方向总共可以分为两个warp，一个warp加载连续的16个列，符合coalesced memory优化。至于y，由于只有2个线程，难以构成warp，同时y方向也没法利用coalesced warp，直接每个线程处理连续16行即可。对这一部分具体细节不明白的，可以参考<code>calculateDistributedTileSize</code>源码或是深入理解<a href="https://siboehm.com/articles/22/CUDA-MMM">coalesced memory优化</a>。<code>calculateDistributedTileSize</code>计算每个thread或是warp对应的tile大小，具体函数如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">/// Return the tile size associated to one thread or warp based on the number of</span></span><br><span class="line"><span class="comment">/// element in the group.</span></span><br><span class="line"><span class="function"><span class="type">static</span> SmallVector&lt;Value&gt;</span></span><br><span class="line"><span class="function"><span class="title">calculateDistributedTileSize</span><span class="params">(ArrayRef&lt;<span class="type">int64_t</span>&gt; numElements, OpBuilder &amp;builder,</span></span></span><br><span class="line"><span class="params"><span class="function">                             Operation *operation)</span> </span>&#123;</span><br><span class="line">  SmallVector&lt;<span class="type">int64_t</span>&gt; blockTileSize = <span class="built_in">getTileSizes</span>(operation, <span class="number">0</span>);</span><br><span class="line">  SmallVector&lt;Value&gt; tileSizesVal;</span><br><span class="line">  <span class="comment">// Use partitionedLoop to know what loop needs to be distributed.</span></span><br><span class="line">  <span class="keyword">auto</span> interfaceOp = <span class="built_in">cast</span>&lt;PartitionableLoopsInterface&gt;(operation);</span><br><span class="line">  <span class="keyword">auto</span> partitionedLoops =</span><br><span class="line">      interfaceOp.<span class="built_in">getPartitionableLoops</span>(kNumMaxParallelDims);</span><br><span class="line">  <span class="keyword">if</span> (partitionedLoops.<span class="built_in">empty</span>()) &#123;</span><br><span class="line">    <span class="keyword">return</span> tileSizesVal;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">auto</span> zero = builder.<span class="built_in">create</span>&lt;arith::ConstantIndexOp&gt;(operation-&gt;<span class="built_in">getLoc</span>(), <span class="number">0</span>);</span><br><span class="line">  tileSizesVal.<span class="built_in">resize</span>(</span><br><span class="line">      <span class="built_in">cast</span>&lt;TilingInterface&gt;(operation).<span class="built_in">getLoopIteratorTypes</span>().<span class="built_in">size</span>(), zero);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// partitionedLoops contains the dimensions we want to distribute.</span></span><br><span class="line">  <span class="comment">// We are distributing them in order onto the different workgroup</span></span><br><span class="line">  <span class="comment">// dimensions.</span></span><br><span class="line">  <span class="function">SmallVector&lt;<span class="type">int64_t</span>&gt; <span class="title">distributedDim</span><span class="params">(numElements.begin(), numElements.end())</span></span>;</span><br><span class="line">  distributedDim.<span class="built_in">resize</span>(partitionedLoops.<span class="built_in">size</span>());</span><br><span class="line">  <span class="type">unsigned</span> idIdx = <span class="number">0</span>;</span><br><span class="line">  std::<span class="built_in">reverse</span>(distributedDim.<span class="built_in">begin</span>(), distributedDim.<span class="built_in">end</span>());</span><br><span class="line">  <span class="keyword">for</span> (<span class="type">unsigned</span> depth : partitionedLoops) &#123;</span><br><span class="line">    <span class="keyword">if</span> (depth &gt;= blockTileSize.<span class="built_in">size</span>())</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    tileSizesVal[depth] = builder.<span class="built_in">create</span>&lt;arith::ConstantIndexOp&gt;(</span><br><span class="line">        operation-&gt;<span class="built_in">getLoc</span>(),</span><br><span class="line">        llvm::<span class="built_in">divideCeil</span>(blockTileSize[depth], distributedDim[idIdx++]));</span><br><span class="line">    <span class="keyword">if</span> (idIdx == kNumMaxParallelDims)</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> tileSizesVal;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>这里有个注意的地方，每一处tile优化，IREE都提供了规则化善后：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="comment">// Apply canonicalization patterns.</span></span><br><span class="line">  RewritePatternSet threadTilingCanonicalizationPatterns =</span><br><span class="line">      linalg::<span class="built_in">getLinalgTilingCanonicalizationPatterns</span>(context);</span><br><span class="line">  <span class="built_in">populateAffineMinSCFCanonicalizationPattern</span>(</span><br><span class="line">      threadTilingCanonicalizationPatterns);</span><br><span class="line">  <span class="keyword">if</span> (<span class="built_in">failed</span>(<span class="built_in">applyPatternsAndFoldGreedily</span>(</span><br><span class="line">          funcOp, std::<span class="built_in">move</span>(threadTilingCanonicalizationPatterns)))) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">signalPassFailure</span>();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这段代码值得借鉴。</p>
</blockquote>
<p><strong>至此便是完整的<code>tileAndDistribute</code>流程。可以看到整个流程的逻辑其实是比较简单的，但在这里做了极其详细的分析，因为这个pass设计大量的tiling优化，其思想和具体实现是值得参考借鉴的。在前面的博客中已经详细讲过tiling优化，在各种异构架构（GPU&#x2F;NPU）中，都有多层级缓存架构，tiling大有用武之地。</strong></p>
<h4 id="GPUMultiBufferingPass"><a href="#GPUMultiBufferingPass" class="headerlink" title="GPUMultiBufferingPass"></a><font color = pink>GPUMultiBufferingPass</font></h4><p>在完成分块划分和面向GPU的并行负载分配工作后，IREE针对GPU的流水线特性做<code>GPUMultiBufferingPass</code>。这个pass代码量只有小一百行，但是要想真正弄懂这个pass，需要对GPU的流水线技术有一定了解。本博客只针对GPU软件流水线做简要介绍。在<a href="https://zhuanlan.zhihu.com/p/20884536063">IREE GPUMultiBufferingPass解读</a>博客中有这样一段话：</p>
<blockquote>
<p>通过广泛使用软件流水线技术，CUTLASS 可以最大程度地利用 GPU 的计算资源，并提高矩阵乘法的性能。软件流水线阶段包括：</p>
<ol>
<li>从全局内存加载数据</li>
<li>将数据拷贝至共享内存 &#x2F; 进行必要的预处理</li>
<li>执行计算</li>
<li>将计算结果写回共享内存或全局内存</li>
</ol>
<p>在软件流水线结构中，GPU 在使用共享内存的数据执行当前计算的同时，还应从全局内存中读取下一次计算所需数据。因此，从存储层次结构的角度看，在共享内存级别应使用多缓冲模式，保证在上游流水线阶段将数据写入共享内存的同时，下游流水线阶段可以从共享内存中加载数据到寄存器。换言之，循环中的不同迭代使用不同缓冲区，从而消除迭代间的数据依赖。多缓冲模式中应使用的缓冲数量取决于流水线深度（也称流水线阶段数量）。</p>
</blockquote>
<p>如下图所示，是GPU软件流水线的示意图：</p>
<p><img src="/images/image-20250517000503961.png" alt="image-20250517000503961"></p>
<p>可以看到，GPU的流水线和CPU多级流水线本质原理一样，但是由于GPU的分支逻辑简单，流水线设计相比CPU已经是大大简化了。</p>
<p>相信到这里，大家对于GPU软件流水的作用有了基础的认知。接下来解读<code>GPUMultiBufferingPass</code>，这个pass的代码比较简洁，这里选择直接贴出源码：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Copyright 2022 The IREE Authors</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Licensed under the Apache License v2.0 with LLVM Exceptions.</span></span><br><span class="line"><span class="comment">// See https://llvm.org/LICENSE.txt for license information.</span></span><br><span class="line"><span class="comment">// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;iree/compiler/Codegen/Common/GPU/Passes.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;iree/compiler/Codegen/Utils/GPUUtils.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/Dialect/Affine/IR/AffineOps.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/Dialect/MemRef/IR/MemRef.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/Dialect/MemRef/Transforms/Transforms.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/IR/Dominance.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/Interfaces/FunctionInterfaces.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> mlir::iree_compiler &#123;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> GEN_PASS_DEF_GPUMULTIBUFFERINGPASS</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;iree/compiler/Codegen/Common/GPU/Passes.h.inc&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// todo hal: 一个比较重要，且简单的pass</span></span><br><span class="line"><span class="comment">// 这个Pass是完成软件流水线的重要一环。</span></span><br><span class="line"><span class="comment">// 多缓冲模式中应使用的缓冲数量取决于流水线深度（也称流水线阶段数量）</span></span><br><span class="line"><span class="comment">// 其核心思想是通过引入多缓冲，使得循环迭代尽可能并行执行，从而为最终实现软件流水线创造条件。</span></span><br><span class="line"><span class="keyword">namespace</span> &#123;</span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">GPUMultiBufferingPass</span> <span class="keyword">final</span></span><br><span class="line">    : impl::GPUMultiBufferingPassBase&lt;GPUMultiBufferingPass&gt; &#123;</span><br><span class="line">  <span class="keyword">using</span> GPUMultiBufferingPassBase::GPUMultiBufferingPassBase;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">runOnOperation</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">    FunctionOpInterface funcOp = <span class="built_in">getOperation</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// First hoist all shared memory allocations to the entry block of the</span></span><br><span class="line">    <span class="comment">// function. We can see memref.alloc in loops after bufferizing scf.forall</span></span><br><span class="line">    <span class="comment">// with promoted shared memory usage inside.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 收集所有allocOp，并且是shared memory中开辟的</span></span><br><span class="line">    <span class="comment">// 提前到func头部</span></span><br><span class="line">    SmallVector&lt;memref::AllocOp&gt; allocs;</span><br><span class="line">    <span class="comment">// Collect all the alloc operations.</span></span><br><span class="line">    <span class="comment">// 收集所有在shared memory中的内存空间开辟</span></span><br><span class="line">    funcOp.<span class="built_in">walk</span>([&amp;](memref::AllocOp allocOp) &#123;</span><br><span class="line">      <span class="comment">// 如果是shared memory的分配，那么就将其移动到函数的entry block中</span></span><br><span class="line">      <span class="comment">// 即multi-buffering优化是只针对shared memory的分配</span></span><br><span class="line">      <span class="keyword">if</span> (<span class="built_in">hasSharedMemoryAddressSpace</span>(allocOp.<span class="built_in">getType</span>()))</span><br><span class="line">        allocs.<span class="built_in">push_back</span>(allocOp);</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">assert</span>(funcOp.<span class="built_in">getBlocks</span>().<span class="built_in">size</span>() == <span class="number">1</span>);</span><br><span class="line">    <span class="comment">// 将所有allocOp放在funcOp的最前面</span></span><br><span class="line">    <span class="keyword">for</span> (memref::AllocOp allocOp : allocs) &#123;</span><br><span class="line">      <span class="keyword">if</span> (allocOp-&gt;<span class="built_in">getParentOp</span>() != funcOp)</span><br><span class="line">        allocOp-&gt;<span class="built_in">moveBefore</span>(&amp;*funcOp.<span class="built_in">begin</span>()-&gt;<span class="built_in">begin</span>());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Then perform multibuffering transformations.</span></span><br><span class="line"></span><br><span class="line">    allocs.<span class="built_in">clear</span>();</span><br><span class="line">    <span class="comment">// Collect all the alloc operations.</span></span><br><span class="line">    funcOp.<span class="built_in">walk</span>([&amp;](memref::AllocOp allocOp) &#123;</span><br><span class="line">      <span class="comment">// Skip allocations not used in a loop.</span></span><br><span class="line">      <span class="comment">// 学习这里的写法：获取user list，以及判断user的所属operation</span></span><br><span class="line">      <span class="keyword">for</span> (Operation *user : allocOp-&gt;<span class="built_in">getUsers</span>()) &#123;</span><br><span class="line">        <span class="keyword">auto</span> loop = user-&gt;<span class="built_in">getParentOfType</span>&lt;scf::ForOp&gt;();</span><br><span class="line">        <span class="keyword">if</span> (!loop)</span><br><span class="line">          <span class="keyword">return</span> WalkResult::<span class="built_in">advance</span>();</span><br><span class="line">      &#125;</span><br><span class="line">      allocs.<span class="built_in">push_back</span>(allocOp);</span><br><span class="line">    </span><br><span class="line">      <span class="comment">// Interrupt: the walk will be interrupted and no more operations, regions or blocks will be visited.</span></span><br><span class="line">      <span class="comment">// Advance: the walk will continue.</span></span><br><span class="line">      <span class="comment">// Skip: the walk of the current operation, region or block and their nested elements that haven&#x27;t been visited already will be skipped and will continue with the next operation, region or block.</span></span><br><span class="line">      <span class="keyword">return</span> WalkResult::<span class="built_in">advance</span>();</span><br><span class="line">    &#125;);</span><br><span class="line">    <span class="comment">// Apply multi-buffering to all of them.</span></span><br><span class="line">    <span class="keyword">for</span> (memref::AllocOp alloc : allocs) &#123;</span><br><span class="line">      <span class="comment">// 这个pass的真正核心步骤，完成多缓冲的引入</span></span><br><span class="line">      <span class="comment">// 辅助gpu软件流水线的设计</span></span><br><span class="line">      <span class="comment">// 其核心是：</span></span><br><span class="line">      <span class="comment">// 1. alloc op的使用者必须在for loop里</span></span><br><span class="line">      <span class="comment">// 2. alloc op的使用者不能有loop-carried dependence</span></span><br><span class="line">      </span><br><span class="line">      <span class="keyword">if</span> (<span class="built_in">failed</span>(memref::<span class="built_in">multiBuffer</span>(alloc, numBuffers))) &#123;</span><br><span class="line">        <span class="comment">// Error out and stop if any buffer cannot be multi buffered, as future</span></span><br><span class="line">        <span class="comment">// software pipelining transformations will assume this happened.</span></span><br><span class="line">        alloc.<span class="built_in">emitOpError</span>(<span class="string">&quot;cannot be multi-buffered&quot;</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">signalPassFailure</span>();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">&#125; <span class="comment">// namespace</span></span><br><span class="line">&#125; <span class="comment">// namespace mlir::iree_compiler</span></span><br></pre></td></tr></table></figure>

<p>这个pass只有当GPU设定的流水线大于1的时候，才会启用，核心目的是通过multibuffer技术来overlap计算和数据传输的延迟开销。该pass的具体流程如下：</p>
<ul>
<li><p>收集所有shared memory的开辟操作（allocOp），并提前到入口函数处，旨在方便后续优化</p>
</li>
<li><p>判断每个开辟的空间，是否在后续的循环中有使用，并且不存在loop carried依赖。</p>
<blockquote>
<p><strong>Loop Carried依赖</strong> 是指循环中不同迭代之间的数据依赖关系，即后续迭代的执行依赖于前面迭代的结果。这种依赖限制了循环的并行化能力，因为迭代无法独立执行，必须按顺序进行。</p>
<p>例如：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt; N; i++) &#123;</span><br><span class="line">    A[i] = A[i<span class="number">-1</span>] + B[i]; <span class="comment">// 第i次迭代依赖第i-1次的结果</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></blockquote>
</li>
<li><p>调用memref dialect的方法：<code>multiBuffer()</code>完成多缓冲技术。</p>
</li>
</ul>
<p>大体读完pass后，我们来思考一下<code>multiBuffer()</code>技术是如何优化GPU流水线的。首先通过debug手段直观感受一下这个pass给IR带来的变化。在pass执行之前，我们的IR代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">func.func @<span class="built_in">matmul_accumulate_512x128xf16_times_128x512xf16_into_512x512xf16_for_LLVMGPUMatmulTensorCore_32_32_16_64_2_1_dispatch_0_matmul_512x512x128_f16</span>() attributes &#123;translation_info = #iree_codegen.translation_info&lt;pipeline = LLVMGPUMatmulTensorCore workgroup_size = [<span class="number">64</span>, <span class="number">2</span>, <span class="number">1</span>], &#123;pipeline_depth = <span class="number">3</span> : i64, store_stage = <span class="number">1</span> : i64&#125;&gt;&#125; &#123;</span><br><span class="line">  %c0 = arith.constant <span class="number">0</span> : index</span><br><span class="line">  %c128 = arith.constant <span class="number">128</span> : index</span><br><span class="line">  %c16 = arith.constant <span class="number">16</span> : index</span><br><span class="line">  %alloc = memref.<span class="built_in">alloc</span>() : memref&lt;<span class="number">16</span>x32xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">  %alloc_0 = memref.<span class="built_in">alloc</span>() : memref&lt;<span class="number">32</span>x16xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">  %alloc_1 = memref.<span class="built_in">alloc</span>() : memref&lt;<span class="number">32</span>x32xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">0</span> = hal.interface.binding.subspan <span class="built_in">layout</span>(&lt;bindings = [<span class="meta">#hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, Indirect&gt;</span>], flags = Indirect&gt;) binding(0) alignment(64) offset(%c0) flags(<span class="string">&quot;ReadOnly|Indirect&quot;</span>) : memref<span class="string">&lt;512x128xf16, #hal.descriptor_type&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  memref.assume_alignment %<span class="number">0</span>, <span class="number">64</span> : memref&lt;<span class="number">512</span>x128xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">1</span> = hal.interface.binding.subspan <span class="built_in">layout</span>(&lt;bindings = [<span class="meta">#hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, Indirect&gt;</span>], flags = Indirect&gt;) binding(1) alignment(64) offset(%c0) flags(<span class="string">&quot;ReadOnly|Indirect&quot;</span>) : memref<span class="string">&lt;128x512xf16, #hal.descriptor_type&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  memref.assume_alignment %<span class="number">1</span>, <span class="number">64</span> : memref&lt;<span class="number">128</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">2</span> = hal.interface.binding.subspan <span class="built_in">layout</span>(&lt;bindings = [<span class="meta">#hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, &quot;ReadOnly|Indirect&quot;&gt;</span>, #hal.pipeline.binding<span class="string">&lt;storage_buffer, Indirect&gt;</span>], flags = Indirect&gt;) binding(2) alignment(64) offset(%c0) flags(Indirect) : memref<span class="string">&lt;512x512xf16, #hal.descriptor_type&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  memref.assume_alignment %<span class="number">2</span>, <span class="number">64</span> : memref&lt;<span class="number">512</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %workgroup_id_x = hal.interface.workgroup.id[<span class="number">0</span>] : index</span><br><span class="line">  %workgroup_id_y = hal.interface.workgroup.id[<span class="number">1</span>] : index</span><br><span class="line">  %<span class="number">3</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0] -&gt; (s0 * <span class="number">32</span>)&gt;()[%workgroup_id_y]</span><br><span class="line">  %subview = memref.subview %<span class="number">0</span>[%<span class="number">3</span>, <span class="number">0</span>] [<span class="number">32</span>, <span class="number">128</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">512</span>x128xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x128xf16, strided&lt;[128, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %<span class="number">4</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0] -&gt; (s0 * <span class="number">32</span>)&gt;()[%workgroup_id_x]</span><br><span class="line">  %subview_2 = memref.subview %<span class="number">1</span>[<span class="number">0</span>, %<span class="number">4</span>] [<span class="number">128</span>, <span class="number">32</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">128</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;128x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  %subview_3 = memref.subview %<span class="number">2</span>[%<span class="number">3</span>, %<span class="number">4</span>] [<span class="number">32</span>, <span class="number">32</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">512</span>x512xf16, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  gpu.barrier</span><br><span class="line">  memref.copy %subview_3, %alloc_1 &#123;__internal_linalg_transform__ = <span class="string">&quot;copy_to_workgroup_memory&quot;</span>&#125; : memref&lt;<span class="number">32</span>x32xf16, strided&lt;[<span class="number">512</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x32xf16, #gpu.address_space&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">  gpu.barrier</span><br><span class="line">  scf.<span class="keyword">for</span> %arg0 = %c0 to %c128 step %c16 &#123;</span><br><span class="line">    %subview_4 = memref.subview %subview[<span class="number">0</span>, %arg0] [<span class="number">32</span>, <span class="number">16</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">32</span>x128xf16, strided&lt;[<span class="number">128</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x16xf16, strided&lt;[128, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">    %subview_5 = memref.subview %subview_2[%arg0, <span class="number">0</span>] [<span class="number">16</span>, <span class="number">32</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">128</span>x32xf16, strided&lt;[<span class="number">512</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;16x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">    gpu.barrier</span><br><span class="line">    memref.copy %subview_4, %alloc_0 &#123;__internal_linalg_transform__ = <span class="string">&quot;copy_to_workgroup_memory&quot;</span>&#125; : memref&lt;<span class="number">32</span>x16xf16, strided&lt;[<span class="number">128</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;32x16xf16, #gpu.address_space&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">    memref.copy %subview_5, %alloc &#123;__internal_linalg_transform__ = <span class="string">&quot;copy_to_workgroup_memory&quot;</span>&#125; : memref&lt;<span class="number">16</span>x32xf16, strided&lt;[<span class="number">512</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt; to memref<span class="string">&lt;16x32xf16, #gpu.address_space&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">    gpu.barrier</span><br><span class="line">    %thread_id_x = gpu.thread_id  x</span><br><span class="line">    %thread_id_y = gpu.thread_id  y</span><br><span class="line">    %<span class="number">5</span> = affine.apply <span class="built_in">affine_map</span>&lt;()[s0] -&gt; (s0 * <span class="number">16</span>)&gt;()[%thread_id_y]</span><br><span class="line">    %<span class="number">6</span> = affine.apply <span class="built_in">affine_map</span>&lt;(d0) -&gt; ((d0 floordiv <span class="number">32</span>) * <span class="number">16</span>)&gt;(%thread_id_x)</span><br><span class="line">    %subview_6 = memref.subview %alloc_0[%<span class="number">5</span>, <span class="number">0</span>] [<span class="number">16</span>, <span class="number">16</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">32</span>x16xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt; to memref<span class="string">&lt;16x16xf16, strided&lt;[16, 1], offset: ?&gt;</span>, #gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">    %subview_7 = memref.subview %alloc[<span class="number">0</span>, %<span class="number">6</span>] [<span class="number">16</span>, <span class="number">16</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">16</span>x32xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt; to memref<span class="string">&lt;16x16xf16, strided&lt;[32, 1], offset: ?&gt;</span>, #gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">    %subview_8 = memref.subview %alloc_1[%<span class="number">5</span>, %<span class="number">6</span>] [<span class="number">16</span>, <span class="number">16</span>] [<span class="number">1</span>, <span class="number">1</span>] : memref&lt;<span class="number">32</span>x32xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt; to memref<span class="string">&lt;16x16xf16, strided&lt;[32, 1], offset: ?&gt;</span>, #gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;</span></span><br><span class="line">    linalg.matmul &#123;__internal_linalg_transform__ = <span class="string">&quot;vectorize&quot;</span>, lowering_config = #iree_codegen.lowering_config&lt;tile_sizes = [[<span class="number">32</span>, <span class="number">32</span>, <span class="number">16</span>]]&gt;&#125; <span class="built_in">ins</span>(%subview_6, %subview_7 : memref&lt;<span class="number">16</span>x16xf16, strided&lt;[<span class="number">16</span>, <span class="number">1</span>], offset: ?&gt;, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;, memref<span class="string">&lt;16x16xf16, strided&lt;[32, 1], offset: ?&gt;</span>, #gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;) outs(%subview_8 : memref<span class="string">&lt;16x16xf16, strided&lt;[32, 1], offset: ?&gt;</span>, #gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt;)</span></span><br><span class="line">  &#125;</span><br><span class="line">  gpu.barrier</span><br><span class="line">  memref.copy %alloc_1, %subview_3 &#123;__internal_linalg_transform__ = <span class="string">&quot;copy_to_workgroup_memory&quot;</span>&#125; : memref&lt;<span class="number">32</span>x32xf16, <span class="meta">#gpu.address_space<span class="string">&lt;workgroup&gt;</span>&gt; to memref<span class="string">&lt;32x32xf16, strided&lt;[512, 1], offset: ?&gt;</span>, #hal.descriptor_type<span class="string">&lt;storage_buffer&gt;</span>&gt;</span></span><br><span class="line">  gpu.barrier</span><br><span class="line">  <span class="keyword">return</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>经过该pass之后，中间代码变动如下：</p>
<p><img src="/images/image-20250516213503860.png" alt="image-20250516213503860"></p>
<p>如上图所示将A和B矩阵buffer成三份。在<code>tileAndDistributePass</code>中，C矩阵的allo在循环外，并且没有在循环使用，所以不参与<code>MultiBufferPass</code>。而A和B矩阵，当<code>workgroup</code>比warp大，会针对warp做tiling，A和B矩阵的shared memory的alloc因此在tile循环中，可以参与<code>multiBuffer</code>优化。由于我们设置的 pipeline depth &#x3D; 3，因此 Matrix A ，B对应的 shared memory 会进行 factor &#x3D; 3 的 multi-buffering。</p>
<p><img src="/images/image-20250516214558122.png" alt="image-20250516214558122"></p>
<p>上图是内层k维度循环中的计算逻辑，也是MultiBuffer主要变换的代码逻辑。在矩阵乘法中，由于为了适配warp大小，做了warp分块（c0 to c128 step c16）。针对每一分块，先将work group mem拷贝入shared mem中，然后计算再拷贝出去。通过引入多缓冲机制，可以将每个循环的各个阶段overlap，以减少延迟，其可视化状态图如下：</p>
<p><img src="/images/image-20250516221417987.png" alt="image-20250516221417987"></p>
<p>可以看到每个单元同一时间都不空闲，因此IREE该pass的多缓冲机制是make sense的。接下来，我们深入一下整个pass的核心function：<code>memref::multiBuffer()</code>，学习该函数是如何完成缓冲任务的。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color = brown>参考资料</font></h2><ol>
<li><a href="http://arxiv.org/abs/2108.13191">论文：MLIR GPU代码生成工作</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/712857026">博客：IREE GPU后端解读</a></li>
</ol>
]]></content>
      <categories>
        <category>GPU</category>
        <category>编译</category>
      </categories>
      <tags>
        <tag>GPU</tag>
        <tag>编译</tag>
      </tags>
  </entry>
  <entry>
    <title>MLIR-basics-deepdive-part2</title>
    <url>/2025/04/24/MLIR-basics-deepdive-part2/</url>
    <content><![CDATA[<p><img src="/images/image-20250422105012239.png" alt="image-20250422105012239"></p>
<span id="more"></span>

<blockquote>
<p>本篇文章主要基于mlir的<a href="https://www.youtube.com/watch?v=7ofnlCFzlqg">公开talk</a>，深入理解一下mlir中的operation，attributes以及properties机制的底层实现。本文为此系列的第二篇，着重解读mlir的attirbutes特性。</p>
</blockquote>
<h2 id="Attribute初探"><a href="#Attribute初探" class="headerlink" title="Attribute初探"></a><font color = brown>Attribute初探</font></h2><p>首先，我们需要先简要理解一下Attribute在MLIR编译器中的作用。根据MLIR的官方文档资料，Attribute是在一个operation中用来表达常量数据的机制。MLIR中的每一operation都关联一个属性字典，其中包含属性名称和属性值的映射。</p>
<ul>
<li>属性按照所属方言划分，可以分为内嵌属性和方言属性。<ul>
<li>内嵌属性：mlir的内嵌方言提供的属性，比如我们常用的StringAttr，DenseArryAttr等。</li>
<li>方言属性：允许方言中定义自己的属性</li>
</ul>
</li>
<li>属性按照字典键是否又方言前缀，可以分为inherent attributes和discardable attributes。<ul>
<li>inherent属性：该属性由operation的语义信息来决定</li>
<li>discardable属性：属性自带语义，但要和operation相一致</li>
</ul>
</li>
</ul>
<p>如下图所示就是两种attribute的典型示例。<code>cmpi</code>操作的<code>谓词</code>属性和操作严格捆绑。</p>
<img src="/images/image-20250426205915933.png" alt="image-20250426205915933" style="zoom: 50%;" />

<h2 id="Attribute-底层原理"><a href="#Attribute-底层原理" class="headerlink" title="Attribute 底层原理"></a><font color = brown>Attribute 底层原理</font></h2><p><img src="/images/image-20250426210418663.png" alt="image-20250426210418663"></p>
<p>上图是编写mlir的时候，经常用到的创建新的attribute。该逻辑是，先构造构造一个vector，然后调用<code>get()</code> API获取一个属性的指针。这里的<code>get()</code> api的特点是，如果已有DenseI64ArrayAttr的内容是offsetsVec，则返回已有的属性，否则创建返回。</p>
<h3 id="Attribute的recap知识"><a href="#Attribute的recap知识" class="headerlink" title="Attribute的recap知识"></a><font color = green>Attribute的recap知识</font></h3><p>首先来总结一下attribute的一些特点：</p>
<img src="/images/image-20250426210700572.png" alt="image-20250426210700572" style="zoom:50%;" />

<p>其最核心的一个特点是属性是不可变的，因此其一系列底层机制都围绕不可变性展开。获取一个内容是offsetsVec的DenseI64ArrayAttr，有如下几步骤：</p>
<ul>
<li>在operation所在的MLIRContext中获取DenseI64Array的ParametricStorageUniquer（每个属性类型都由一个存储器管理）</li>
<li>在该storageuniqure中，通过hash的方式查找（key是&lt;Type, int64, llvm::ArrayRef<char>&gt;&gt;）。</li>
</ul>
<h3 id="Attribute的get-方法底层逻辑链条"><a href="#Attribute的get-方法底层逻辑链条" class="headerlink" title="Attribute的get()方法底层逻辑链条"></a><font color = green>Attribute的get()方法底层逻辑链条</font></h3><p>首先我们写梳理整个逻辑链条相关的数据结构：</p>
<h4 id="get方法返回什么"><a href="#get方法返回什么" class="headerlink" title="get方法返回什么"></a>get方法返回什么</h4><p><code>get()</code>方法返回的是DenseArrayAttrStorage指针，该指针包含实际存储内容和一个hash值。</p>
<ul>
<li>实际存储内容包含：存储数据类型，存储数据长度，数组内容。</li>
<li>KeyTy表示hash值，是type，数据长度和实际数组的一个元组。</li>
</ul>
<p><img src="/images/image-20250426221014974.png" alt="image-20250426221014974"></p>
<h4 id="通过属性类型：DenseI64ArrayAttr找寻到该类型的底层storage"><a href="#通过属性类型：DenseI64ArrayAttr找寻到该类型的底层storage" class="headerlink" title="通过属性类型：DenseI64ArrayAttr找寻到该类型的底层storage"></a>通过属性类型：DenseI64ArrayAttr找寻到该类型的底层storage</h4><p><img src="/images/image-20250426221323084.png" alt="image-20250426221323084"></p>
<p>如上图所示，一个context上下文中，每个属性都有对应的storage。</p>
<p><img src="/images/image-20250426221410708.png" alt="image-20250426221410708"></p>
<p>用一个densemap存储，每个attribute都有一个type id，通过type id检索到对应的ParametricStorageUniqer指针。</p>
<h4 id="从ParametricStorageUniqer中通过hash值创建-返回属性指针"><a href="#从ParametricStorageUniqer中通过hash值创建-返回属性指针" class="headerlink" title="从ParametricStorageUniqer中通过hash值创建&#x2F;返回属性指针"></a>从ParametricStorageUniqer中通过hash值创建&#x2F;返回属性指针</h4><p><img src="/images/image-20250426225725235.png" alt="image-20250426225725235"></p>
<p>如下是一个完整的get流程：</p>
<p><img src="/images/image-20250426225801721.png" alt="image-20250426225801721"></p>
<p>值得注意的是，底层的存储BumpPtrAllocator类似于内存池，只能开辟，然后空间释放是整个Allocator管理的所有存储都释放。</p>
<h2 id="Operation-Accessors"><a href="#Operation-Accessors" class="headerlink" title="Operation Accessors"></a><font color = brown>Operation Accessors</font></h2><p><img src="/images/image-20250426225942938.png" alt="image-20250426225942938"></p>
<p>上图中是一个operation常用的api，其中，Operand，Result和region都是可变的，而attr是不可变的。因此看似api都是get，set，但实际底层机制attr会复杂的多。</p>
<p>具体复杂性体现在如下slide中：</p>
<blockquote>
<p><img src="/images/image-20250426230139141.png" alt="image-20250426230139141"></p>
<p>由于不可变，因此set attr的时候，针对<strong>DictionaryAttr</strong>中添加一组name和attrvalue，底层也是十分复杂的（DictionaryAttr也是属性）。</p>
<ul>
<li>首先将已有的dict拷贝入数组</li>
<li>在数组中变化，添加新的value（inplace）</li>
<li>用get方法生成新的Dictionary</li>
</ul>
<p>一个简单的setAttr，底层流程分为如上三步，attr的immutable特性，导致其效率较低。</p>
</blockquote>
<blockquote>
<p><img src="/images/image-20250426230550008.png" alt="image-20250426230550008"></p>
<p>上述slide是一个很好地总结。</p>
<p>如下是一个检测理解的例子：</p>
<p><img src="/images/image-20250426230628810.png" alt="image-20250426230628810"></p>
<p><img src="/images/image-20250426230655927.png" alt="image-20250426230655927"></p>
<p>显而易见，这里存在大量的vector拷贝， 以及可能的存储空间浪费。</p>
</blockquote>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>mlir</category>
        <category>basics</category>
      </categories>
      <tags>
        <tag>mlir</tag>
        <tag>basics</tag>
      </tags>
  </entry>
  <entry>
    <title>MLIR basics: deepdive part1</title>
    <url>/2025/04/22/MLIR-basics-deepdive-part1/</url>
    <content><![CDATA[<p><img src="/images/image-20250422105012239.png" alt="image-20250422105012239"></p>
<span id="more"></span>

<blockquote>
<p>本篇文章主要基于mlir的<a href="https://www.youtube.com/watch?v=7ofnlCFzlqg">公开talk</a>，深入理解一下mlir中的operation，attributes以及properties机制的底层实现。本文先着重解读operation的底层实现机制。</p>
</blockquote>
<h2 id="MLIR-Operation-Intro"><a href="#MLIR-Operation-Intro" class="headerlink" title="MLIR Operation Intro"></a><font color = brown>MLIR Operation Intro</font></h2><p><img src="/images/image-20250422105310560.png" alt="image-20250422105310560"></p>
<p>上图来源<a href="https://www.youtube.com/watch?v=Y4SvqTtOIDk">mlir tutorial</a>。mlir operation是MLIR系统中最基本的可执行单元，其涵盖多种不同的抽象层级的计算（按照硬件相关性分类，可以分为硬件相关和硬件无关。抽象层级上，也有诸如vector operation和memref operation）。上图已经涵盖了operation op的每个细节，详情可参考<a href="https://mlir.llvm.org/doxygen/classmlir_1_1Operation.html#details">mlir operation docs</a>。</p>
<p>具体的使用细节，可以参考<a href="https://harmonyhu.com/2021/08/17/mlir/#mliroperation">HarmonyHu MLIR技术细节整理</a>。</p>
<p><code>mlir::Operation</code>是通用定义，包含通用的接口和属性；<code>MulOp</code>、<code>TransposeOp</code>、<code>ConstantOp</code>等等是特定定义，包含特定的属性。前者可以通过<code>llvm::dyn_cast</code>（动态）或<code>llvm::cast</code>（静态）转换成后者；后者通过<code>getOperation()</code>转换成前者。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">processConstantOp</span><span class="params">(mlir::Operation *operation)</span> </span>&#123;</span><br><span class="line">  ConstantOp op = llvm::<span class="built_in">dyn_cast</span>&lt;ConstantOp&gt;(operation);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// This operation is not an instance of `ConstantOp`.</span></span><br><span class="line">  <span class="keyword">if</span> (!op)</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Get the internal operation instance wrapped by the smart pointer.</span></span><br><span class="line">  mlir::Operation *internalOperation = op.<span class="built_in">getOperation</span>();</span><br><span class="line">  <span class="built_in">assert</span>(internalOperation == operation &amp;&amp;</span><br><span class="line">         <span class="string">&quot;these operation instances are the same&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>特定Op可以直接访问属性；<code>mlir::Operation</code>无法直接访问特定Op的属性，但可以类似如下间接访问，如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// getAttrOfType</span></span><br><span class="line">IntegerAttr opsetAttr = op-&gt;<span class="built_in">getAttrOfType</span>&lt;::mlir::Attribute&gt;(<span class="string">&quot;onnx_opset&quot;</span>).<span class="built_in">dyn_cast_or_null</span>&lt;IntegerAttr&gt;();</span><br><span class="line"><span class="keyword">if</span> (opsetAttr)</span><br><span class="line">  opset = opsetAttr.<span class="built_in">getValue</span>().<span class="built_in">getSExtValue</span>();</span><br><span class="line"><span class="comment">// getAttr</span></span><br><span class="line">mlir::Type targetType = op-&gt;<span class="built_in">getAttr</span>(<span class="string">&quot;to&quot;</span>).<span class="built_in">cast</span>&lt;::mlir::TypeAttr&gt;().<span class="built_in">getValue</span>();</span><br></pre></td></tr></table></figure>

<p>operation的具体使用方面，提供了如下接口：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">OperationName <span class="title">getName</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> name; &#125;</span><br><span class="line"><span class="comment">/// Remove this operation from its parent block and delete it.</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">erase</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">/// Remove the operation from its parent block, but don&#x27;t delete it.</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">remove</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">/// Returns the operation block that contains this operation.</span></span><br><span class="line"><span class="function">Block *<span class="title">getBlock</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> block; &#125;</span><br><span class="line"><span class="comment">/// Return the context this operation is associated with.</span></span><br><span class="line"><span class="function">MLIRContext *<span class="title">getContext</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">/// Returns the closest surrounding operation that contains this operation</span></span><br><span class="line"><span class="comment">/// or nullptr if this is a top-level operation.</span></span><br><span class="line"><span class="function">Operation *<span class="title">getParentOp</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> block ? block-&gt;<span class="built_in">getParentOp</span>() : <span class="literal">nullptr</span>; &#125;</span><br><span class="line"><span class="comment">/// Return the closest surrounding parent operation that is of type &#x27;OpTy&#x27;.</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> OpTy&gt; <span class="function">OpTy <span class="title">getParentOfType</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">dump</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="title">getNumOperands</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function">Value <span class="title">getOperand</span><span class="params">(<span class="type">unsigned</span> idx)</span> </span>&#123; <span class="keyword">return</span> <span class="built_in">getOpOperand</span>(idx).<span class="built_in">get</span>(); &#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">setOperand</span><span class="params">(<span class="type">unsigned</span> idx, Value value)</span></span>;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="title">getNumResults</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">/// Get the &#x27;idx&#x27;th result of this operation.</span></span><br><span class="line"><span class="function">OpResult <span class="title">getResult</span><span class="params">(<span class="type">unsigned</span> idx)</span> </span>&#123; <span class="keyword">return</span> <span class="built_in">OpResult</span>(<span class="keyword">this</span>, idx); &#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">replaceAllUsesWith</span><span class="params">(Operation *op)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">moveBefore</span><span class="params">(Operation *existingOp)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">moveBefore</span><span class="params">(Block *block, llvm::iplist&lt;Operation&gt;::iterator iterator)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">moveAfter</span><span class="params">(Operation *existingOp)</span></span>;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">moveAfter</span><span class="params">(Block *block, llvm::iplist&lt;Operation&gt;::iterator iterator)</span></span>;</span><br></pre></td></tr></table></figure>



<h2 id="MLIR-Operation-Internal"><a href="#MLIR-Operation-Internal" class="headerlink" title="MLIR Operation Internal"></a><font color = brown>MLIR Operation Internal</font></h2><p>前一章节主要介绍<code>mlir operation</code>以及其使用细节，而这一章则深入挖掘operation的底层实现存储。本章主要参考<a href="https://www.youtube.com/watch?v=7ofnlCFzlqg">mlir properties talk</a>。</p>
<p>首先我们先来看一下mlir的官方doc中，对于operation的底层细节的一些描述：</p>
<blockquote>
<p>An <a href="https://mlir.llvm.org/doxygen/classmlir_1_1Operation.html">Operation</a> defines zero or more SSA <code>Value</code> that we refer to as the <a href="https://mlir.llvm.org/doxygen/classmlir_1_1Operation.html">Operation</a> results. This array of <a href="https://mlir.llvm.org/doxygen/classmlir_1_1Value.html">Value</a> is actually stored in memory before the <a href="https://mlir.llvm.org/doxygen/classmlir_1_1Operation.html">Operation</a> itself in reverse order. That is for an <a href="https://mlir.llvm.org/doxygen/classmlir_1_1Operation.html">Operation</a> with 3 results we allocate the following memory layout:</p>
<p>[Result2, Result1, Result0, <a href="https://mlir.llvm.org/doxygen/classmlir_1_1Operation.html">Operation</a>] ^ this is where <code>Operation*</code> pointer points to.</p>
<p>A consequence of this is that this class must be heap allocated, which is handled by the various <code>create</code> methods. Each result contains:</p>
<ul>
<li>one pointer to the first use (see <code>OpOperand</code>)</li>
<li>the type of the SSA <a href="https://mlir.llvm.org/doxygen/classmlir_1_1Value.html">Value</a> this result defines.</li>
<li>the index for this result in the array. The results are defined as subclass of <code>ValueImpl</code>, and more precisely as the only two subclasses of <code>OpResultImpl</code>: <code>InlineOpResult</code> and <code>OutOfLineOpResult</code>. The former is used for the first 5 results and the latter for the subsequent ones. They differ in how they store their index: the first 5 results only need 3 bits and thus are packed with the <a href="https://mlir.llvm.org/doxygen/classmlir_1_1Type.html">Type</a> pointer, while the subsequent one have an extra <code>unsigned</code> value and thus need more space.</li>
</ul>
<p>An <a href="https://mlir.llvm.org/doxygen/classmlir_1_1Operation.html">Operation</a> also has zero or more operands: these are uses of SSA <a href="https://mlir.llvm.org/doxygen/classmlir_1_1Value.html">Value</a>, which can be the results of other operations or <a href="https://mlir.llvm.org/doxygen/classmlir_1_1Block.html">Block</a> arguments. Each of these uses is an instance of <code>OpOperand</code>. This optional array is initially tail allocated with the operation class itself, but can be dynamically moved out-of-line in a dynamic allocation as needed.</p>
<p>An <a href="https://mlir.llvm.org/doxygen/classmlir_1_1Operation.html">Operation</a> may contain optionally one or multiple Regions, stored in a tail allocated array. Each <code>Region</code> is a list of Blocks. Each <code>Block</code> is itself a list of Operations. This structure is effectively forming a tree.</p>
<p>Some operations like branches also refer to other <a href="https://mlir.llvm.org/doxygen/classmlir_1_1Block.html">Block</a>, in which case they would have an array of <code>BlockOperand</code>.</p>
<p>An <a href="https://mlir.llvm.org/doxygen/classmlir_1_1Operation.html">Operation</a> may contain optionally a “Properties” object: this is a pre-defined C++ object with a fixed size. This object is owned by the operation and deleted with the operation. It can be converted to an <a href="https://mlir.llvm.org/doxygen/classmlir_1_1Attribute.html">Attribute</a> on demand, or loaded from an <a href="https://mlir.llvm.org/doxygen/classmlir_1_1Attribute.html">Attribute</a>.</p>
<p>Finally an <a href="https://mlir.llvm.org/doxygen/classmlir_1_1Operation.html">Operation</a> also contain an optional <code>DictionaryAttr</code>, a <a href="https://mlir.llvm.org/doxygen/classmlir_1_1Location.html">Location</a>, and a pointer to its parent <a href="https://mlir.llvm.org/doxygen/classmlir_1_1Block.html">Block</a> (if any).</p>
</blockquote>
<p>总结一下，mlir operation的底层细节中，比较有趣的点：</p>
<ul>
<li>操作的结果是以逆序存储在operation之前，并且前5个result和之后可能的result存储大小不一样，这个机制类似llvm的llvm::SmallVector实现。</li>
<li>operand是尾部存储的方式存储在operation之后，旨在提供可扩展和可修改性（<strong>operand本质是对SSA value的引用，在mlir中经常会有修改引用的情况，比如replaceAllUsesWith()</strong>），同时也支持动态存储在别处。</li>
<li>一个operation会有多个region，也是尾部存储方式存储。</li>
</ul>
<p>针对这些有趣的点，我们后续主要挖掘operation类的具体实现，以及operation前存储和tailing存储等技术细节。</p>
<h3 id="Operation类的定义"><a href="#Operation类的定义" class="headerlink" title="Operation类的定义"></a><font color = green>Operation类的定义</font></h3><p><img src="/images/image-20250422130653462.png" alt="image-20250422130653462"></p>
<p>如图所示是<code>/include/mlir/ir/operation.h</code>文件，截取的是operation类的定义，继承自<code>ilist_node_with_parent</code>和<code>TrailingObjects</code>。</p>
<h4 id="ilist-node-with-parent"><a href="#ilist-node-with-parent" class="headerlink" title="ilist_node_with_parent"></a>ilist_node_with_parent</h4><p>参考<a href="https://llvm.org/doxygen/classllvm_1_1ilist__node__with__parent-members.html">llvm member list文档</a>，其定义如下：</p>
<p><img src="/images/image-20250422131035118.png" alt="image-20250422131035118"></p>
<p>是一个双向链表，允许operation获取其prev节点和next节点，分别有如下api调用：</p>
<p><img src="/images/image-20250422131140983.png" alt="image-20250422131140983"></p>
<blockquote>
<p>这个链表属于侵入式链表，和朴素的std::list有区别，具体参见<a href="https://www.boost.org/doc/libs/1_79_0/doc/html/intrusive/intrusive_vs_nontrusive.html">Boost侵入式链表文档</a>。这一块比较复杂，后续再学。</p>
</blockquote>
<h4 id="Trailing-Objects"><a href="#Trailing-Objects" class="headerlink" title="Trailing Objects"></a>Trailing Objects</h4><p>Trailing Objects的原理十分简单，如下是c++中的解释：</p>
<blockquote>
<p>是一个在 C++ 中的术语，指的是一个对象，它紧跟在一个类的末尾，并且其大小是动态的或不固定的。通常，尾部对象用于在类内部存储一些额外的数据，这些数据的大小在编译时是不确定的，可能根据运行时的条件或对象的使用来决定。</p>
</blockquote>
<p>在<code>llvm/include/support/TrailingObjects.h</code>头文件中，有对于这个类的详细定义。注释中给出一个如何定义trailing object类的例子。</p>
<p><code>类的定义</code></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 定义VarLenghObj，该对象有固定的int和可变的double数组</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">VarLengthObj</span> : <span class="keyword">private</span> TrailingObjects&lt;VarLengthObj, <span class="type">int</span>, <span class="type">double</span>&gt; &#123;</span><br><span class="line">  <span class="keyword">friend</span> TrailingObjects; <span class="comment">// 允许模板访问私有方法</span></span><br><span class="line">  <span class="type">unsigned</span> NumInts, NumDoubles;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="type">size_t</span> <span class="title">numTrailingObjects</span><span class="params">(OverloadToken&lt;<span class="type">int</span>&gt;)</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> NumInts; &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>注意这里有一个关键点，double是尾元素，因此其长度是可变的，所以没有numTrailingObjects方法实现。</p>
<p><code>内存分配</code></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">size_t</span> Size = VarLengthObj::<span class="built_in">totalSizeToAlloc</span>&lt;<span class="type">int</span>, <span class="type">double</span>&gt;(NumInts, NumDoubles);</span><br><span class="line"><span class="type">void</span> *Mem = <span class="keyword">operator</span> <span class="built_in">new</span>(Size);</span><br><span class="line"><span class="keyword">auto</span> *Obj = <span class="built_in">new</span> (Mem) <span class="built_in">VarLengthObj</span>();</span><br></pre></td></tr></table></figure>

<p><code>访问数据</code></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> *Ints = Obj-&gt;<span class="built_in">getTrailingObjects</span>&lt;<span class="type">int</span>&gt;();    <span class="comment">// 访问 int 数组</span></span><br><span class="line"><span class="type">double</span> *Doubles = Obj-&gt;<span class="built_in">getTrailingObjects</span>&lt;<span class="type">double</span>&gt;(); <span class="comment">// 访问 double 数组</span></span><br></pre></td></tr></table></figure>

<h3 id="Operation类的实现"><a href="#Operation类的实现" class="headerlink" title="Operation类的实现"></a><font color = green>Operation类的实现</font></h3><p><img src="/images/image-20250422133350287.png" alt="image-20250422133350287"></p>
<p>这张图解释了operation的实际存储，具体如下：</p>
<ul>
<li>前16bytes是前序节点和后继节点，注意前序节点是ilist_node_base类型加一个int，用来表示哨兵。</li>
<li>后续是block指针等信息，同时还有region，results等个数。</li>
</ul>
<p>上述的trailing objects中，只有OpOperand是编译时不确定可变的，其他均可通过operation这64bytes信息中的num值来推导实际存储位置，如下图所示：</p>
<p><img src="/images/image-20250422133953920.png" alt="image-20250422133953920"></p>
<p>该图展示了operation的getRegion()方法的底层实现原理。而唯一特殊的点就是OpOperand，是tail-allocated或是和operation存储分离的。该OpOperand的定位通过64bytes中的OperandStorage来辅助：</p>
<p><img src="/images/image-20250422134301210.png" alt="image-20250422134301210"></p>
<p>该OperandStorage中存储OpOperand指针，以及总共有多少operands。添加operands，这种模式也十分容易动态扩展。</p>
<p>解读完OpOperand，region等信息后，operation还剩一个关键信息：result。</p>
<p><img src="/images/image-20250422134727315.png" alt="image-20250422134727315"></p>
<p>result的特殊性是其存储在operation存储体的前面，没有放在trailing objects中。其设计思路参考SmallVector设计，分为OutOfLineOpResult和InlineOpResult，对于少量result情况（&lt;&#x3D;6）可以有效压缩空间。其主要思路是用3bits表示有限的index索引，而超过3bits范围，则使用int表示索引范围，使得前六个result占用16bytes，其余为24个bytes。</p>
<img src="/images/image-20250422134946924.png" alt="image-20250422134946924" style="zoom:67%;" />

<p>从operation中获取指定的result，其实现如下，逻辑是比较清晰简单的：</p>
<img src="/images/image-20250422135114847.png" alt="image-20250422135114847" style="zoom:67%;" />

<p>同时也支持从result获取operation这样的操作。逻辑和上图基本一致。</p>
<p>如下图是一张很好的operation结构总结图：</p>
<p><img src="/images/image-20250422141744357.png" alt="image-20250422141744357"></p>
<p>来源于<a href="https://llvm.org/devmtg/2024-04/slides/Keynote/Amini-Niu-HowSlowIsMLIR.pdf">How Slow is MLIR talk</a>。</p>
<h2 id="Debug实战"><a href="#Debug实战" class="headerlink" title="Debug实战"></a><font color = brown>Debug实战</font></h2><p>mlir的debug tips参考[debug博客](<a href="https://micropuma.github.io/2025/04/20/MLIR-debug-tips/">MLIR debug tips | Leon’s Blog</a>)。首先打断点打在如图所示：</p>
<p><img src="/images/image-20250422140535380.png" alt="image-20250422140535380"></p>
<p>我们重点关注func::FuncOp的实现。</p>
<p><img src="/images/image-20250422140614518.png" alt="image-20250422140614518"></p>
<p>可以看到，OpState是指向operation实体的指针，其余均是OpTrait萃取或是interface接口，不属于本章节内容。</p>
<p><img src="/images/image-20250422141259704.png" alt="image-20250422141259704"></p>
<p>可以看到，OpState是符合前几章讲解的底层实现。</p>
<p>至此，解读完mlir的operation的底层实现，下一章节会重点解读mlir的attributes机制的底层实现。</p>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>mlir</category>
        <category>basics</category>
      </categories>
      <tags>
        <tag>mlir</tag>
        <tag>basics</tag>
      </tags>
  </entry>
  <entry>
    <title>MLIR debug tips</title>
    <url>/2025/04/20/MLIR-debug-tips/</url>
    <content><![CDATA[<p><img src="/images/image-20250421225330748.png" alt="image-20250421225330748"></p>
<span id="more"></span>

<blockquote>
<p>本篇文章主要汇总debug mlir时候的一些tips，方便后续查找使用。</p>
</blockquote>
<h2 id="VS-Code-Debugger"><a href="#VS-Code-Debugger" class="headerlink" title="VS Code Debugger"></a><font color = brown>VS Code Debugger</font></h2><h3 id="有用的命令合集"><a href="#有用的命令合集" class="headerlink" title="有用的命令合集"></a><font color = green>有用的命令合集</font></h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-exec set scheduler-locking step    # 冻结其他线程</span><br></pre></td></tr></table></figure>



<h2 id="MLIR-Debug-Practice"><a href="#MLIR-Debug-Practice" class="headerlink" title="MLIR Debug Practice"></a><font color = brown>MLIR Debug Practice</font></h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Legalizing operation : &#x27;vector.transfer_read&#x27;(0x62c0434198a0) &#123;</span><br><span class="line"><span class="meta prompt_">  %</span><span class="language-bash">6 = <span class="string">&quot;vector.transfer_read&quot;</span>(%0, %arg3, %5) &#123;operand_segment_sizes = dense&lt;[1, 1, 1, 0]&gt; : vector&lt;4xi32&gt;, permutation_map = affine_map&lt;(d0) -&gt; (d0)&gt;&#125; : (memref&lt;32xf32&gt;, index, f32) -&gt; vector&lt;8xf32&gt;</span></span><br><span class="line"></span><br><span class="line">  * Fold &#123;</span><br><span class="line">  &#125; -&gt; FAILURE : unable to fold</span><br><span class="line"></span><br><span class="line">  * Pattern : &#x27;vector.transfer_read -&gt; ()&#x27; &#123;</span><br><span class="line">    ** Insert  : &#x27;arith.constant&#x27;(0x62c0433289e0)</span><br><span class="line">    ** Insert  : &#x27;arith.constant&#x27;(0x62c04332a260)</span><br><span class="line">    ** Insert  : &#x27;arith.subi&#x27;(0x62c0432f4b50)</span><br><span class="line">    ** Insert  : &#x27;vector.create_mask&#x27;(0x62c043329e30)</span><br><span class="line"></span><br><span class="line">    //===-------------------------------------------===//</span><br><span class="line">    Legalizing operation : &#x27;vector.transfer_read&#x27;(0x62c0434198a0) &#123;</span><br><span class="line">      // -----// IR Dump After AffineScalarReplacement //----- //</span><br><span class="line"><span class="meta prompt_">%</span><span class="language-bash">10 = <span class="string">&quot;vector.transfer_read&quot;</span>(%0, %arg3, %5, %9) &#123;in_bounds = [<span class="literal">true</span>], operand_segment_sizes = dense&lt;1&gt; : vector&lt;4xi32&gt;, permutation_map = affine_map&lt;(d0) -&gt; (d0)&gt;&#125; : (memref&lt;32xf32&gt;, index, f32, vector&lt;8xi1&gt;) -&gt; vector&lt;8xf32&gt;</span></span><br><span class="line"></span><br><span class="line">      * Fold &#123;</span><br><span class="line">      &#125; -&gt; FAILURE : unable to fold</span><br><span class="line"></span><br><span class="line">      * Pattern : &#x27;vector.transfer_read -&gt; ()&#x27; &#123;</span><br><span class="line">      &#125; -&gt; FAILURE : pattern was already applied</span><br><span class="line"></span><br><span class="line">      * Pattern : &#x27;vector.transfer_read -&gt; ()&#x27; &#123;</span><br><span class="line">        ** Insert  : &#x27;vector.splat&#x27;(0x62c04332a5b0)</span><br><span class="line">        ** Insert  : &#x27;vector.maskedload&#x27;(0x62c04329aad0)</span><br><span class="line">        ** Replace : &#x27;vector.transfer_read&#x27;(0x62c0434198a0)</span><br><span class="line"></span><br><span class="line">        //===-------------------------------------------===//</span><br><span class="line">        Legalizing operation : &#x27;vector.splat&#x27;(0x62c04332a5b0) &#123;</span><br><span class="line">          %10 = &quot;vector.splat&quot;(%5) : (f32) -&gt; vector&lt;8xf32&gt;</span><br><span class="line"></span><br><span class="line">          * Fold &#123;</span><br><span class="line">            ** Insert  : &#x27;arith.constant&#x27;(0x62c043420680)</span><br><span class="line">            ** Replace : &#x27;vector.splat&#x27;(0x62c04332a5b0)</span><br><span class="line"></span><br><span class="line">            //===-------------------------------------------===//</span><br><span class="line">            Legalizing operation : &#x27;arith.constant&#x27;(0x62c043420680) &#123;</span><br><span class="line">              %10 = &quot;arith.constant&quot;() &#123;value = dense&lt;0.000000e+00&gt; : vector&lt;8xf32&gt;&#125; : () -&gt; vector&lt;8xf32&gt;</span><br><span class="line"></span><br><span class="line">              * Fold &#123;</span><br><span class="line">              &#125; -&gt; FAILURE : unable to fold</span><br><span class="line">            &#125; -&gt; FAILURE : no matched legalization pattern</span><br><span class="line">            //===-------------------------------------------===//</span><br><span class="line">          &#125; -&gt; FAILURE : failed to legalize generated constant &#x27;arith.constant&#x27;</span><br><span class="line">        &#125; -&gt; FAILURE : no matched legalization pattern</span><br><span class="line">        //===-------------------------------------------===//</span><br><span class="line">      &#125; -&gt; FAILURE : failed to legalize generated operation &#x27;vector.splat&#x27;(0x000062C04332A5B0)</span><br><span class="line">    &#125; -&gt; FAILURE : pattern failed to match</span><br><span class="line">  &#125; -&gt; FAILURE : no matched legalization pattern</span><br><span class="line">  //===-------------------------------------------===//</span><br><span class="line">&#125; -&gt; FAILURE : failed to legalize operation updated in-place &#x27;vector.transfer_read&#x27;</span><br><span class="line">&#125; -&gt; FAILURE : pattern failed to match</span><br><span class="line"></span><br><span class="line">* Pattern : &#x27;vector.transfer_read -&gt; ()&#x27; &#123;</span><br><span class="line">&#125; -&gt; FAILURE : pattern failed to match</span><br><span class="line">&#125; -&gt; FAILURE : no matched legalization pattern</span><br><span class="line">//===-------------------------------------------===//</span><br></pre></td></tr></table></figure>





<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color = brown>参考资料</font></h2><ol>
<li><a href="https://vscode.js.cn/docs/debugtest/debugging">vscode debug文档</a></li>
</ol>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>mlir</category>
        <category>debug</category>
      </categories>
      <tags>
        <tag>mlir</tag>
        <tag>vscode debug</tag>
      </tags>
  </entry>
  <entry>
    <title>MLIR topic: Vectorization</title>
    <url>/2025/04/24/MLIR-topic-Vectorization/</url>
    <content><![CDATA[<p><img src="/images/image-20250509105758710.png" alt="image-20250509105758710"></p>
<span id="more"></span>

<blockquote>
<p>本篇博客重点阐述MLIR的自动向量化技术的原理，并结合实际项目（如IREE项目）探索实现细节。</p>
</blockquote>
<h2 id="MLIR-Vector-dialect引言"><a href="#MLIR-Vector-dialect引言" class="headerlink" title="MLIR Vector dialect引言"></a><font color = brown>MLIR Vector dialect引言</font></h2><blockquote>
<p>这一部分讲清楚亮点：</p>
<ul>
<li>vector dialect在整个pipeline的位置</li>
<li>vector dialect的逐层抽象下降逻辑</li>
</ul>
</blockquote>
<p>首先，我们需要明晰vector dialect在一个端到端编译器的lower过程中，所处的位置。</p>
<img src="F:\LeonBlog\source\images\image-20250507104738739.png" alt="image-20250507104738739" style="zoom:50%;" />

<p>由上图所示，vector dialect一般在affine和linalg dialect之后，在LLVM等底层dialect之前。此时编译器已经完成了高层的一些算子优化：多面体优化，tiling优化，循环融合优化，在vector层面则主要完成数据并行优化（主要针对SIMD架构做优化）。</p>
<p>在vector dialect的内部，也可以进行分层划分，主要逻辑是自底向上，从指令层级（LLVMIR）到硬件强相关操作（NVVM dialect）到虚拟硬件无关操作三个层级。具体的例子如下图所示：</p>
<img src="F:\LeonBlog\source\images\image-20250507104757817.png" alt="image-20250507104757817" style="zoom:50%;" />

<p>在高层的虚拟vector dialect中，和硬件无关，适合完成自动并行化等优化。然后通过MLIR的pattern rewrite机制逐层转换成硬件相关向量。以CUDA为例，MLIR社区提供NVVM dialect，可以抽象cuda的tensor core操作，编译器可以尝试将虚拟向量转为tensor core向量处理。最终将硬件相关vector转为LLVM做代码生成。</p>
<h2 id="设计思考"><a href="#设计思考" class="headerlink" title="设计思考"></a><font color = brown>设计思考</font></h2><h3 id="向量化粒度思考"><a href="#向量化粒度思考" class="headerlink" title="向量化粒度思考"></a><font color = green>向量化粒度思考</font></h3><p>首先，我们要思考在高层vector逐层转向LLVM IR的过程中，其vector的粒度由什么决定。整个vector系统的设计逻辑是要将高层vector逐层转换为可以在硬件上高效利用数据并行的指令，因此最小粒度由硬件决定。该最小粒度在硬件相关vector层级就会暴露出来：</p>
<p><img src="F:\LeonBlog\source\images\image-20250507104842507.png" alt="image-20250507104842507"></p>
<p>对于非最小粒度的倍数的vector，要么在高层转换成功，要么损失性能，或是在一些特定硬件会直接不合法而导致编译失败。</p>
<blockquote>
<p>这一部分其实也是vectorization优化的一个重难点，如何针对硬件进行合理的向量化优化。后续会围绕这一点重点分析已有的编译器的硬件相关向量化。</p>
</blockquote>
<h3 id="Automatic-Vectorization"><a href="#Automatic-Vectorization" class="headerlink" title="Automatic Vectorization"></a><font color = green>Automatic Vectorization</font></h3><p>不同于前面对于向量化降级粒度的思考，这一部分主要讨论如何将标量转换为向量表示。自动向量化的核心思想是，<strong>从一段代码中提取出结构化信息（数据并行性）并重组，本质是lowering的反面：raising操作</strong>。由于MLIR的多层级结构，因此自动向量化一般在linalg等高层ir就完成，而非affine层循环表示中完成，这样可以节省大量开销和优化复杂度。这一部分后续会结合代码详细解读。</p>
<h3 id="Lowering-to-LLVM-tradeoff"><a href="#Lowering-to-LLVM-tradeoff" class="headerlink" title="Lowering to LLVM tradeoff"></a><font color = green>Lowering to LLVM tradeoff</font></h3><p>在LLVM中，其ir只支持1-D vector，因此需要将高层mlir vector降级到1-D vecctor。该过程有如下选项：</p>
<p><img src="F:\LeonBlog\source\images\image-20250507104915961.png" alt="image-20250507104915961"></p>
<blockquote>
<p>一般选择最内维做flatten，方便simd优化，外维选择嵌套类型。</p>
</blockquote>
<p>明白我们有哪些选择后，下一步是理解LLVM的vector有什么约束条件，在原文中大概翻译如下：</p>
<blockquote>
<h3 id="LLVM对多维向量的限制"><a href="#LLVM对多维向量的限制" class="headerlink" title="LLVM对多维向量的限制"></a>LLVM对多维向量的限制</h3><ol>
<li><strong>原生仅支持1D向量</strong>：LLVM的向量操作（如<code>extractelement</code>、<code>insertelement</code>、<code>shufflevector</code>）直接支持动态索引，但仅适用于1D向量。</li>
<li><strong>聚合类型的静态性</strong>：多维向量需降维为<strong>聚合类型</strong>（如结构体或数组），但相关操作（<code>extractvalue</code>&#x2F;<code>insertvalue</code>）仅允许静态索引，动态索引需通过内存指针（<code>getelementptr</code>）间接实现，效率较低。</li>
</ol>
<hr>
<h3 id="MLIR中的两种实现方案"><a href="#MLIR中的两种实现方案" class="headerlink" title="MLIR中的两种实现方案"></a>MLIR中的两种实现方案</h3><h4 id="方案一：嵌套聚合（Nested-Aggregate）"><a href="#方案一：嵌套聚合（Nested-Aggregate）" class="headerlink" title="方案一：嵌套聚合（Nested Aggregate）"></a>方案一：嵌套聚合（Nested Aggregate）</h4><ul>
<li><p><strong>原理</strong>：将n维向量编码为多层嵌套的1D向量（如<code>vector&lt;4x8x16x32xf32&gt;</code>视为<code>[4 x [8 x [16 x &lt;32xf32&gt;]]]</code>）。</p>
<p>优点：</p>
<ul>
<li>保留维度层次，无需额外线性化逻辑。</li>
<li>静态索引操作（如提取子向量）天然适配硬件寄存器结构。</li>
</ul>
</li>
<li><p>缺点：</p>
<ul>
<li>动态索引非最内层维度时，需将数据写入内存再通过指针操作，性能损耗大。</li>
<li>无法直接使用LLVM针对1D向量的优化指令（如SIMD指令）。</li>
</ul>
</li>
</ul>
<h4 id="方案二：扁平化1D向量（Flattened-Vector）"><a href="#方案二：扁平化1D向量（Flattened-Vector）" class="headerlink" title="方案二：扁平化1D向量（Flattened Vector）"></a>方案二：扁平化1D向量（Flattened Vector）</h4><ul>
<li><strong>原理</strong>：将n维向量展平为单一大尺寸1D向量（如<code>vector&lt;4x8x16x32xf32&gt;</code>转为<code>vector&lt;4 * 8 * 16 * 32=16384xf32&gt;</code>）。</li>
<li>优点：<ul>
<li>支持全维度动态索引（<code>insertelement</code>&#x2F;<code>extractelement</code>）。</li>
<li>兼容LLVM的1D向量优化指令和硬件指令。</li>
</ul>
</li>
<li>缺点：<ul>
<li>引入复杂线性化&#x2F;反线性化逻辑，代码生成复杂度高。</li>
<li>掩盖实际硬件向量边界（如固定128位SIMD），可能导致非对齐访问或低效向量拆分。</li>
</ul>
</li>
</ul>
<hr>
<h3 id="核心权衡：抽象灵活性与硬件效率"><a href="#核心权衡：抽象灵活性与硬件效率" class="headerlink" title="核心权衡：抽象灵活性与硬件效率"></a>核心权衡：抽象灵活性与硬件效率</h3><ul>
<li><strong>嵌套聚合</strong>更贴近硬件物理结构（如多级寄存器），利于静态代码优化，但牺牲动态索引能力。</li>
<li><strong>扁平化向量</strong>提供编程灵活性，但需在代码生成阶段隐式处理维度映射，可能阻碍硬件针对性优化。</li>
</ul>
<h4 id="折中方案"><a href="#折中方案" class="headerlink" title="折中方案"></a>折中方案</h4><p>通过<strong>显式维度转换操作</strong>（如<code>vector.cast</code>）按需展平部分维度，例如将<code>vector&lt;4x8x16x32xf32&gt;</code>局部转为<code>vector&lt;4x4096xf32&gt;</code>，既保留外层静态索引优化空间，又允许内层使用动态索引或SIMD指令。</p>
</blockquote>
<p>上述关于extractvalue操作不支持动态索引这点，刚开始会感觉比较难以理解。其本质原因是现代处理器（CPU&#x2F;GPU）的<strong>寄存器文件（Register File）</strong> 是硬件直接操作的超高速存储单元，访问索引的时候存在物理设计约束：</p>
<ul>
<li><strong>固定寄存器数量</strong>：每个线程&#x2F;核心可用的寄存器数量是硬件固定的（如NVIDIA GPU每个线程最多255个寄存器）。</li>
<li><strong>静态寄存器分配</strong>：寄存器在编译时由编译器分配，操作寄存器的指令<strong>直接编码寄存器编号</strong>（如 <code>R0</code>, <code>R1</code>）。</li>
<li><strong>无动态索引支持</strong>：硬件指令无法通过运行时变量选择寄存器（如无法生成 <code>MOV R%idx, 42</code> 这样的指令）。</li>
</ul>
<p>由于上述三个物理设计约束，寄存器难以支持动态索引，也导致嵌套聚合的动态索引局限。将眼光从mlir编译放远来看，其实众多编程模型对于寄存器的动静态问题早有讨论：</p>
<ul>
<li><p>通过循环展开，化动态为静态。</p>
</li>
<li><p>split memory，内存可以动态索引，但是带来访存开销问题。</p>
</li>
</ul>
<h2 id="vector实际实现机制"><a href="#vector实际实现机制" class="headerlink" title="vector实际实现机制"></a><font color = brown>vector实际实现机制</font></h2><h3 id="Implication-on-codegen"><a href="#Implication-on-codegen" class="headerlink" title="Implication on codegen"></a><font color = green>Implication on codegen</font></h3><p>MLIR的n维向量类型在降级到LLVM时被表示为(n-1)维数组包裹的1维向量，这导致静态索引和动态索引的分层限制：仅最内层1维向量支持动态索引（通过<code>insertelement</code>&#x2F;<code>extractelement</code>），外层(n-1)维只能静态访问，否则需显式内存操作（<code>load</code>&#x2F;<code>store</code>）。代码生成需权衡循环展开与内存访问，动态索引需通过内存指针实现，而硬件寄存器分配和溢出问题则推迟到LLVM后端处理，MLIR层通过抽象成本模型指导优化选择。</p>
<h3 id="Implication-on-Lowering-to-Accelerators"><a href="#Implication-on-Lowering-to-Accelerators" class="headerlink" title="Implication on Lowering to Accelerators"></a><font color = green>Implication on Lowering to Accelerators</font></h3><p>针对支持高维向量的加速器，MLIR通过<code>vector.cast</code>将最内层维度展平为1维向量（如<code>vector&lt;Kxf32&gt;</code>），直接对接LLVM-IR或加速器特定指令。这一过程需在<strong>加速器专用方言中定制转换规则，例如处理不规则维度时的掩码或数据重组逻辑，而标准化的展平操作（如<code>K=K1*K2*…*Kn</code>）近乎无开销</strong>。目标是实现跨CPU&#x2F;GPU&#x2F;加速器的统一代码生成框架，通过模式重写和成本模型灵活适配硬件特性。</p>
<p><font color = red>这一条在各种机器学习编译器中都有大量代码实现</font></p>
<h3 id="Implication-on-calling-external-functions-that-operate-on-vectors"><a href="#Implication-on-calling-external-functions-that-operate-on-vectors" class="headerlink" title="Implication on calling external functions that operate on vectors"></a><font color = green>Implication on calling external functions that operate on vectors</font></h3><p>调用外部向量函数时，可能需要线性化多维向量以匹配ABI接口。MLIR需在调用边界隐式插入维度转换逻辑，确保数据布局与目标函数兼容。例如，将<code>vector&lt;4x4xf32&gt;</code>转为连续内存块或1D向量，避免因多维结构导致调用约定冲突。</p>
<h3 id="Relationship-to-LLVM-matrix-type-proposal"><a href="#Relationship-to-LLVM-matrix-type-proposal" class="headerlink" title="Relationship to LLVM matrix type proposal"></a><font color = green>Relationship to LLVM matrix type proposal</font></h3><p>LLVM矩阵提案（支持2D矩阵及专用指令）与MLIR多维向量设计存在互补性。MLIR更高维的通用向量抽象可降级为LLVM矩阵或原生1D向量，而MLIR的<code>vector.cast</code>机制为CPU&#x2F;加速器提供了统一接口。未来MLIR可能成为跨硬件向量化策略的枢纽层，LLVM矩阵则作为特定场景的子集实现。</p>
<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><font color = green>Conclusion</font></h3><p>**MLIR选择“嵌套聚合1D向量”作为核心抽象，因其明确区分内存与寄存器语义，同时通过<code>vector.cast</code>兼容硬件特定指令。**这种设计支持粗粒度向量操作（如跨多硬件向量的模式匹配），并通过显式维度转换平衡灵活性与性能，而非依赖LLVM隐式线性化。尽管LLVM矩阵提案适合硬件专用场景，MLIR的高维模型更适配跨平台代码生成需求，成为连接算法与硬件优化的桥梁。</p>
<h2 id="向量化流程"><a href="#向量化流程" class="headerlink" title="向量化流程"></a><font color = brown>向量化流程</font></h2><p>主要参考<a href="https://www.lei.chat/posts/mlir-vector-dialect-and-patterns/#handling-high-d-vectors">Lei chat blog</a>。下述内容大量摘自博客以及一些个人总结。</p>
<p><img src="F:\LeonBlog\source\images\image-20250507142752068.png" alt="image-20250507142752068"></p>
<p><img src="F:\LeonBlog\source\images\image-20250507142338159.png" alt="image-20250507142338159"></p>
<p>上图是vetorization的流程图。对于每一步，可以参考<a href="https://github.com/iree-org/iree/blob/a8e4c38c/compiler/src/iree/compiler/Codegen/SPIRV/SPIRVVectorize.cpp">Spirv的向量化处理源码</a>。</p>
<p><img src="F:\LeonBlog\source\images\image-20250507144005262.png" alt="image-20250507144005262"></p>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>mlir</category>
        <category>basics</category>
      </categories>
      <tags>
        <tag>mlir</tag>
        <tag>basics</tag>
      </tags>
  </entry>
  <entry>
    <title>Modern CPP: 条款1</title>
    <url>/2025/05/09/Modern-CPP-%E6%9D%A1%E6%AC%BE1/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>byteir初探：Compiler部分</title>
    <url>/2025/05/08/byteir%E5%88%9D%E6%8E%A2%EF%BC%9ACompiler%E9%83%A8%E5%88%86/</url>
    <content><![CDATA[<img src="/images/image-20250508125112166.png" alt="image-20250508125112166" style="zoom:50%;" />

<span id="more"></span>

<h2 id="Compiler部分概览"><a href="#Compiler部分概览" class="headerlink" title="Compiler部分概览"></a><font color = brown>Compiler部分概览</font></h2><p><img src="/images/image-20250508165752639.png" alt="image-20250508165752639"></p>
<p>ByteIR的compiler部分设计遵循标准的mlir层级转换流程，上图所示是前端，中间compiler和后端runtime的架构图。compiler的输入是MHLO dialect（包含部分ByteIR针对特定算子的扩展），输出是mlir（host code）和ptx（默认的cuda device后端中间形式）。最终输出给到runtime做调度和GPU上执行。如下图所示是compiler的编译优化流水线架构图：</p>
<pre class="mermaid">graph TD
    A[Start: hlo-graph-opt] --> B[hlo-fusion-opt]
    B --> C[linalg-tensor-opt]
    C --> D[byre-tensor-opt]
    D --> E[byteir-bufferize-opt]
    E --> F[linalg-memref-opt]
    F --> G[scf-opt]
    G --> H[gpu-opt]
    H --> I[inline + lccl-to-byre + gpu-launch-func-to-byre]
    I --> J[set-op-space & set-arg-space]
    J --> K[byre-opt]
    K --> L[nvvm-codegenfor device]
    K --> M[byre-hostfor host]
    M --> N[remove-module-tag]
    L --> O[translate_to_ptx]
    N --> P[write host MLIR / MLIRBC]</pre>



]]></content>
      <categories>
        <category>编译技术</category>
        <category>机器学习编译</category>
      </categories>
      <tags>
        <tag>机器学习编译器</tag>
        <tag>mlir</tag>
      </tags>
  </entry>
  <entry>
    <title>minitorch: 自动反向传播</title>
    <url>/2025/04/28/minitorch-%E8%87%AA%E5%8A%A8%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/</url>
    <content><![CDATA[<p><img src="/images/image-20250428233640749.png" alt="image-20250428233640749"></p>
<span id="more"></span>

<blockquote>
<p>在完成minitorch的task1部分的过程中，发现自己对于计算图以及自动反向求导仅仅停留在纸面公式上，难以转换为代码实现。本文章重点解读计算图的自动反向求导机制，从原理出发，最终分析minitorch中如何用代码实现该机制。</p>
</blockquote>
<h2 id="自动反向求导原理"><a href="#自动反向求导原理" class="headerlink" title="自动反向求导原理"></a><font color = brown>自动反向求导原理</font></h2><p>这一部分主要参考<a href="https://dlsyscourse.org/slides/4-automatic-differentiation.pdf">Auto Differentiation slide</a>。求导系统，可以分为前向求导和反向求导。</p>
<h3 id="前向求导-vs-反向求导"><a href="#前向求导-vs-反向求导" class="headerlink" title="前向求导 vs. 反向求导"></a><font color = green>前向求导 vs. 反向求导</font></h3><p>如下图所示是两种求导机制的对比：</p>
<p><img src="/images/image-20250429102807002.png" alt="image-20250429102807002"></p>
<p>可以总结为两点：</p>
<ul>
<li>传播顺序，一个是正向的拓扑序，一个是反向拓扑序。</li>
<li>计算的逻辑不一样，前向求导求的是中间结果关于叶子节点的偏导（<strong>符合数学计算逻辑</strong>），反向求导求的是结果节点关于中间结果的偏导（<strong>slide中称为伴随变量，表征某个中间结果对最终结果的偏导</strong>）。</li>
</ul>
<blockquote>
<p>仔细思考，反向求导相比前向求导最大的优势，是前向求导针对不同的变量，需要个走一个拓扑序完成求导过程，时间复杂度更高。并且，一般的计算图都是多输入少输出，因此反向求导相比前向求导计算量更小。</p>
</blockquote>
<p>在反向求导的过程中，以上图为例，v2节点由于同时是v4和v5的输入，因此在求v2节点的伴随变量的时候，需要做加法运算来讲v4和v5的伴随变量一块传播。具体的数学理论支撑见下图：</p>
<p><img src="/images/image-20250429103955103.png" alt="image-20250429103955103"></p>
<h3 id="反向求导计算公式-反向计算图"><a href="#反向求导计算公式-反向计算图" class="headerlink" title="反向求导计算公式 &amp; 反向计算图"></a><font color = green>反向求导计算公式 &amp; 反向计算图</font></h3><p>这一小节主要讲两件事：反向求导的计算公式，以及实际的机器学习系统中，如何通过构造计算图的方式加速反向求导的效率。这两点是紧密结合的，前者为理论支撑，后者为工程实现。</p>
<h4 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h4><p><img src="/images/image-20250429104341091.png" alt="image-20250429104341091"></p>
<p>这个伪代码整体逻辑比较清晰，再次不过多赘述。</p>
<h4 id="反向计算图"><a href="#反向计算图" class="headerlink" title="反向计算图"></a>反向计算图</h4><p>有了上述公式，我们需要思考实际工程中，怎样实现能够尽量高效。一种想法是结合partial disjoint手动计算，而目前常用的机器学习系统都是在正向传播的过程中（遍历正向图），同时构建一个反向传播图。</p>
<p><img src="/images/image-20250429104616503.png" alt="image-20250429104616503"></p>
<p>上图摘自<a href="https://dlsyscourse.org/">Deep Learning System</a>课程的slide，在课程中提供了一个完整示例，揭示如何在运行反向计算算法的同时，构建反向计算图。强烈建议读者参考学习该课程的反向求导章节加深理解。</p>
<p><img src="/images/image-20250430140134356.png" alt="image-20250430140134356"></p>
<p>上图摘自<a href="https://archwalker.github.io/blog/2019/05/27/pytorch-internals.html">Pytorch Internal 博客</a>，下面摘抄一些博客中的话辅助理解：</p>
<blockquote>
<p>请花一点时间学习上面这张图。有一些东西需要展开来讲；下面列出了哪些东西值得关注：</p>
<ol>
<li>首先请忽略掉那些红色和蓝色的代码。PyTorch实现了<a href="https://en.wikipedia.org/wiki/Automatic_differentiation#Reverse_accumulation">reverse-mode automatic differentiation</a> (反向模式自动微分)，意味着我们通过反向遍历计算图的方式计算出梯度。注意看变量名：我们在红色代码区域的最下面计算了loss；然后，在蓝色代码区域首先我们计算了grad_loss。loss 由 next_h2计算而来，因此我们计算grad_next_h2。严格来讲，这些以grad_开头的变量其实并不是gradients；他们实际上是Jacobian矩阵左乘了一个向量，但是在PyTorch中我们就叫它们grad，大部分人都能理解其中的差异。</li>
<li>即使代码结构相同，代码的行为也是不同的：前向(forwards)的每一行被一个微分计算代替，表示对这个前向操作的求导。例如，<code>tanh</code>操作符变成了<code>tanh_backward</code>操作符(如上图最左边的绿线所关联的两行所示)。前向和后向计算的输入和输出颠倒过来：如果前向操作生成了<code>next_h2</code>，那么后向操作取<code>grad_next_h2</code>作为输入。</li>
</ol>
</blockquote>
<p>至此，我们来总结一下反向传播和自动求导反向传播（构建反向图）两种方式得优缺点。</p>
<p><img src="/images/image-20250429104853521.png" alt="image-20250429104853521"></p>
<p><img src="/images/image-20250429104946307.png" alt="image-20250429104946307"></p>
<p>朴素的反向传播时间复杂度高（不同变量均需要遍历），但是图存储量小。而构建反向图图存储量会变大（反向节点：Partial Disjoint节点和Disjoint节点），但是只用跑一遍反向图即可完成所有计算，并且如果添加计算逻辑，可扩展性也更强。目前的主流机器学习框架：PyTorch，TensorFlow和JAX均使用反向图，朴素的反向传播可见于早期的Caffe系统中。</p>
<h2 id="MiniTorch工程实现"><a href="#MiniTorch工程实现" class="headerlink" title="MiniTorch工程实现"></a><font color = brown>MiniTorch工程实现</font></h2><p>这一部分主要是记录完成<a href="https://minitorch.github.io/module1/module1/">minitorch assign1</a>过程中的学习心得。该实验仅仅是将Minitorch的反向传播系统最核心的组件挖空留给学生进行编写，如何完成这个实验可以参考<a href="https://dezeming.top/wp-content/uploads/2022/02/MiniTorch-%E5%AD%A6%E4%B9%A0%E5%85%A8%E6%94%BB%E7%95%A5.pdf">minitorch 学习攻略</a>。本章节重点解读minitorch的反向传播系统的架构设计。</p>
<p>本章节将按照如下逻辑展开：</p>
<ul>
<li>类之间的关系和类的详细解读</li>
<li>Pytorch内部机制</li>
</ul>
<h3 id="类的解读"><a href="#类的解读" class="headerlink" title="类的解读"></a><font color = green>类的解读</font></h3><p>在初阅读minitorch的底层代码，会迷失在眼花缭乱的各种类实现中。我们要去把握的，是自动反向求导过程中，最核心的功能是什么？在构造反向计算图中，我们需要知道一个值（output）是如何由输入（inputs）计算得到的，即需要跟踪函数内部计算，而这也是目前python系统无法支持的事情。因此本质上，minitorch的类系统设计，就是完成如下三件事：</p>
<ul>
<li>用代理类（Variables）替换所有python值</li>
<li>把所有数学运算符替换为代理运算符（Functions）</li>
<li>把Variables功能增强以记住过去应用于它们的Functions</li>
</ul>
<blockquote>
<p>在assign1中，只是完成标量的反向求导系统，后续assign会扩展。因此本章节主要面向Scalar讲述。</p>
</blockquote>
<p><img src="/images/image-20250430165328520.png" alt="image-20250430165328520"></p>
<p>如上图所示，是整个minitorch的从前向计算到反向传播的一个完整的流程图。Scalar是代理类替换python值，ScalarFn替换数学运算法，包含forward和backward方法，辅助Scalar类的上下文记录环节。ScalarHistory类则是每个Scalar用来记录上下文的辅助类。整个流程是比较清晰的，结合流程图和源码可以方便理解全貌。</p>
<h3 id="Pytorch的自动微分设计"><a href="#Pytorch的自动微分设计" class="headerlink" title="Pytorch的自动微分设计"></a><font color = green>Pytorch的自动微分设计</font></h3><p>由于Pytorch系统源码极其复杂，这里仅仅参考<a href="https://archwalker.github.io/blog/2019/05/27/pytorch-internals.html">Pytorch Internal的talk</a>，概括性分享自动微分设计，比对minitorch的具体实现学习。后续可能会开一篇文章着重讲解PyTorch系统，包括PyTorch2.0提出的一些新特性（TorchDynamo等）。</p>
<img src="/images/image-20250430140602705.png" alt="image-20250430140602705" style="zoom:50%;" />

<p>当我们在tensor上调用各种operations的时候（forward前向过程），一些元数据(metadata)也需要被记录下来。让我们调整一下tensor数据结构的示意图：现在不仅仅单单一个tensor指向storage，我们会有一个封装着这个tensor和更多信息(自动微分元信息(AutogradeMeta))的变量(variable)。这个变量所包含的信息是用户调用<code>loss.backward()</code>执行自动微分所必备的。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color = brown>参考资料</font></h2><ol>
<li><a href="https://dezeming.top/wp-content/uploads/2022/02/MiniTorch-%E5%AD%A6%E4%B9%A0%E5%85%A8%E6%94%BB%E7%95%A5.pdf">minitorch学习攻略</a></li>
<li><a href="https://dlsyscourse.org/slides/4-automatic-differentiation.pdf">Deep Learning System chapter5 slide</a></li>
</ol>
]]></content>
      <categories>
        <category>pytorch</category>
        <category>机器学习系统</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习系统</tag>
      </tags>
  </entry>
  <entry>
    <title>tpu-mlir初探</title>
    <url>/2025/04/08/tpu-mlir%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<p><img src="/images/image-20250411150531193.png" alt="image-20250411150531193"></p>
<span id="more"></span>

<blockquote>
<p>TPU-MLIR是算能开发的一套端到端编译框架，支持将onnx模型编译到多款tpu上运行。该项目基于mlir，写法标准，是极佳的mlir学习项目。从中可以学到mlir的各种语法，多层级ir的设计，以及如何完成runtime的接入。</p>
</blockquote>
<h2 id="项目构建"><a href="#项目构建" class="headerlink" title="项目构建"></a><font color = brown>项目构建</font></h2><p>参考<a href="https://github.com/sophgo/tpu-mlir">tpu-mlir github</a>，整个项目的构建比较简单：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">docker pull sophgo/tpuc_dev:latest     # 获取tpu-mlir镜像</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">myname1234 is just an example, you can <span class="built_in">set</span> your own name</span></span><br><span class="line">docker run --privileged --name myname1234 -v $PWD:/workspace -it sophgo/tpuc_dev:latest    # 构建容器</span><br><span class="line">cd tpu-mlir</span><br><span class="line">source ./envsetup.sh</span><br><span class="line">./build.sh</span><br></pre></td></tr></table></figure>



<h2 id="项目架构以及技术点解读"><a href="#项目架构以及技术点解读" class="headerlink" title="项目架构以及技术点解读"></a><font color = brown>项目架构以及技术点解读</font></h2><h3 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a><font color  = green>整体架构</font></h3><p>这一部分主要参考<a href="http://arxiv.org/abs/2210.15016">TPU-MLIR论文</a>。</p>
<p><img src="/images/image-20250411151435735.png" alt="image-20250411151435735"></p>
<p>上图中的框架，对应代码是<code>tpu-mlir/regression/run_model.py</code>。该顶层模块中调用<code>model_transfer.py</code>和<code>model_deploy.py</code>， 分别完成模型 –&gt; top ir –&gt; tpu ir 和tpu ir到<a href="https://doc.sophgo.com/docs/3.0.0/docs_latest_release/nntc/html/usage/bmodel.html">bmodel部署</a>。如果想要得到int8的模型，则需要调用<code>run_calibration.py</code>。具体流程如下图所示：</p>
<p><img src="/images/image-20250412125832691.png" alt="image-20250412125832691"></p>
<p>参考<a href="https://tpumlir.org/docs/developer_manual/04_over_design.html">TPU-MLIR 官方文档</a>理解更细节的mlir转换图。</p>
<h3 id="架构亮点"><a href="#架构亮点" class="headerlink" title="架构亮点"></a><font color = green>架构亮点</font></h3><blockquote>
<p>TPU-MLIR项目的价值一是提供mlir学习资料，整个项目的架构和实现比较简洁，二是提供tpu设计思路，此部分值得我们重点关注</p>
</blockquote>
<ul>
<li><p><strong>ONNX converter</strong>：用python编写的前端工具，可以用来参考如何将高级语言快速转换为mlir ir。</p>
</li>
<li><p><strong>多层级ir</strong>：TPU-MLIR设计了Top dialect和Tpu dialect，是比较经典的ir设计（硬件无关ir，统一不同模型框架语义，以及硬件相关ir，负责提供硬件抽象），诸如GLOW编译器也是这种ir设计。可以用来参考如何使用mlir系统设计ir抽象。</p>
</li>
<li><p><strong>优化pass</strong>：这一部分是TPU-MLIR的一个重点，主要有如下方面。</p>
<ul>
<li>动态shape，TPU-MLIR是支持动态shape的inference的，可以做参考</li>
<li>高层算子融合pass</li>
<li>TPU相关pass，这一部分是后续重点关注pass。包括：<ul>
<li>TPU inference，主要借助于onnx 的runtime，后续会专门介绍</li>
<li>weight-reorder，op-reorder等tpu专属优化</li>
<li>layer group优化，类似局部缓存融合优化（<a href="https://dl.acm.org/doi/10.1145/3503222.3507723">Astitch论文</a>），后续专门介绍</li>
</ul>
</li>
<li>量化，TPU-MLIR对于量化操作做了诸多考量，包含带<strong>calibration的量化</strong>，对称量化和非对称量化等。、</li>
</ul>
</li>
<li><p>从TOP dialect到TPU dialect，各种conversion步骤都做inference操作。</p>
<p><img src="/images/image-20250423144517050.png" alt="image-20250423144517050"></p>
<p>摘自TPU-MLIR 的论文，解释为什么要做inference操作。</p>
</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color = brown>参考资料</font></h2><ol>
<li><a href="https://github.com/micropuma/tpu-mlir">TPU-MLIR github</a></li>
<li><a href="http://arxiv.org/abs/2210.15016">TPU-MLIR论文</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/613328745">TPU-MLIR博客</a></li>
</ol>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>机器学习编译</category>
        <category>TPU技术</category>
        <category>量化技术</category>
      </categories>
      <tags>
        <tag>机器学习编译器</tag>
        <tag>mlir</tag>
        <tag>异构计算系统</tag>
      </tags>
  </entry>
  <entry>
    <title>tpu-mlir源码解读</title>
    <url>/2025/04/11/tpu-mlir%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<p><img src="/images/image-20250411150531193.png" alt="image-20250411150531193"></p>
<span id="more"></span>

<blockquote>
<p>本博客主要学习TPU-MLIR的代码结构，mlir编程技术等。</p>
</blockquote>
<h2 id="TPU-MLIR测试流程"><a href="#TPU-MLIR测试流程" class="headerlink" title="TPU-MLIR测试流程"></a><font color = brown>TPU-MLIR测试流程</font></h2><p>开始对TPU-MLIR项目的详细解读之前，先编写好测试case。我们选择对于Yolov5s做测试，具体的模型和训练测试数据可以参考<code>regression/</code>文件夹。需要如下文件：</p>
<ul>
<li>yolov5s.onnx</li>
<li>COCO2017数据集</li>
<li>image图片集</li>
</ul>
<p>具体脚本如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mkdir model_yolov5s &amp;&amp; cd model_yolov5s</span><br><span class="line">cp $&#123;REGRESSION_PATH&#125;/model/yolov5s.onnx .</span><br><span class="line">cp -rf $&#123;REGRESSION_PATH&#125;/dataset/COCO2017 .</span><br><span class="line">cp -rf $&#123;REGRESSION_PATH&#125;/image .</span><br><span class="line">mkdir workspace &amp;&amp; cd workspace</span><br></pre></td></tr></table></figure>

<p>然后调用TPU-MLIR的python driver程序，完成整个流程。由于目前先不关注量化部分，所以先只用<code>model_transform.py</code>和<code>model_deploy.py</code>两个python文件。具体脚本如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">model_transform.py \</span><br><span class="line">    --model_name yolov5s \</span><br><span class="line">    --model_def ./yolov5s.onnx \</span><br><span class="line">    --input_shapes [[1,3,640,640]] \</span><br><span class="line">    --mean 0.0,0.0,0.0 \</span><br><span class="line">    --scale 0.0039216,0.0039216,0.0039216 \</span><br><span class="line">    --keep_aspect_ratio \</span><br><span class="line">    --pixel_format rgb \</span><br><span class="line">    --output_names 350,498,646 \</span><br><span class="line">    --test_input ./image/dog.jpg \</span><br><span class="line">    --test_result yolov5s_top_outputs.npz \</span><br><span class="line">    --mlir yolov5s.mlir</span><br><span class="line"></span><br><span class="line">model_deploy.py \</span><br><span class="line">  --mlir yolov5s.mlir \</span><br><span class="line">  --quantize F16 \</span><br><span class="line">  --processor bm1684x \</span><br><span class="line">  --test_input yolov5s_in_f32.npz \</span><br><span class="line">  --test_reference yolov5s_top_outputs.npz \</span><br><span class="line">  --model yolov5s_1684x_f16.bmodel</span><br></pre></td></tr></table></figure>

<p>最终生成的npu可执行是bmodle模型，是Sophon AI平台专用的高效推理模型文件，包含特定npu的硬件指令集。上述流程得到的中间表示，权重文件，最终的bmodel文件如下：</p>
<img src="/images/image-20250417183143802.png" alt="image-20250417183143802" style="zoom:50%;" />

<blockquote>
<ul>
<li><p>onnx到mlir系统：纯python，无第三方依赖的一套前端parser:rocket:</p>
</li>
<li><p>图优化：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tpuc-opt yolov5s_origin.mlir --shape-infer --canonicalize --extra-optimize -o yolov5s.mlir</span><br></pre></td></tr></table></figure>

<ul>
<li>shape infer pass</li>
<li>canonicalize pass：对于top dialect的每个op都有对应的标准化，详细代码见：<code>lib/Dialect/Top/Canonicalize</code>。</li>
<li>extra  optimize：目前还不清楚这个pass做了什么</li>
</ul>
</li>
<li><p>对于输入数据，做处理：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">model_runner.py --input yolov5s_in_f32.npz --model ./yolov5s.onnx --output yolov5s_ref_outputs.npz</span><br></pre></td></tr></table></figure>
</li>
<li><p>deploy阶段（后续会详细介绍）</p>
</li>
</ul>
</blockquote>
<h2 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a><font color = brown>代码结构</font></h2><p>TPU-MLIR项目的一大特点，就是代码结构非常规整。</p>
<ul>
<li><p>tools中是各种驱动工具</p>
</li>
<li><p>python中是python binding的驱动工具</p>
</li>
<li><p>include文件结构如下：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">├── CMakeLists.txt</span><br><span class="line">├── tpu_mlir</span><br><span class="line">│   ├── Backend</span><br><span class="line">│   ├── Builder</span><br><span class="line">│   │   ├── BM168x</span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   ├── Conversion</span><br><span class="line">│   │   ├── TopToLinalg</span><br><span class="line">│   │   ├── TopToTosa</span><br><span class="line">│   │   └── TopToTpu</span><br><span class="line">│   ├── Dialect</span><br><span class="line">│   │   ├── CMakeLists.txt</span><br><span class="line">│   │   ├── Top</span><br><span class="line">│   │   │   ├── CMakeLists.txt</span><br><span class="line">│   │   │   ├── IR</span><br><span class="line">│   │   │   │   ├── CMakeLists.txt</span><br><span class="line">│   │   │   │   ├── TopOps.h</span><br><span class="line">│   │   │   │   └── TopOps.td</span><br><span class="line">│   │   │   └── Transforms</span><br><span class="line">│   │   │       ├── CMakeLists.txt</span><br><span class="line">│   │   │       ├── Passes.h</span><br><span class="line">│   │   │       └── Passes.td</span><br><span class="line">│   │   └── Tpu</span><br><span class="line">│   │       ├── CMakeLists.txt</span><br><span class="line">│   │       ├── IR</span><br><span class="line">│   │       │   ├── CMakeLists.txt</span><br><span class="line">│   │       │   ├── TpuOps.h</span><br><span class="line">│   │       │   └── TpuOps.td</span><br><span class="line">│   │       └── Transforms</span><br><span class="line">│   │           ├── CoreParallel</span><br><span class="line">│   │           │   └── CoreParallel.hpp</span><br><span class="line">│   │           ├── DevParallel</span><br><span class="line">│   │           │   ├── Distribute.h</span><br><span class="line">│   │           │   └── DistributeUtils.h</span><br><span class="line">│   │           ├── LayerGroup</span><br><span class="line">│   │           ├── Passes.h</span><br><span class="line">│   │           ├── Passes.td</span><br><span class="line">│   │           └── TruncOp</span><br><span class="line">│   │               └── TruncOp.h</span><br><span class="line">│   ├── InitAll.h</span><br><span class="line">│   ├── Interfaces</span><br><span class="line">│   ├── Support</span><br><span class="line">│   └── Traits</span><br><span class="line">│       ├── Traits.h</span><br><span class="line">│       └── Traits.td</span><br><span class="line">└── tpu_mlir-c</span><br><span class="line">    ├── Dialects.h</span><br><span class="line">    └── RegisterEverything.h</span><br></pre></td></tr></table></figure>

<p>这个文件结构十分值得解读。</p>
<ul>
<li>Conversion是dailect之间的转换函数</li>
<li>Backend和builder是辅助tpu 方言代码生成npu bmodel</li>
<li>Dialect中包含top顶层dialect和tpu dialect<ul>
<li>ir是方言的定义</li>
<li>transforms是ir内部的优化pass，tpu dialect有比较多的优化pass</li>
</ul>
</li>
<li>Interface中包含operation使用的接口定义，同理traits的作用</li>
<li>Support的作用是代码生成过程中的api支持，后续会详细解读</li>
</ul>
</li>
<li><p>lib文件结构类似，特殊的在dialect文件夹下多了Canonicalize文件夹，<strong>该文件作用是针对每个operation完成一些图层面优化</strong>。</p>
</li>
</ul>
<blockquote>
<p>:key:一个十分好的总结TPU-MLIR工程目录结构和编译流程图，红色部分为前端，绿色部分为TOP层以及与之有关功能，蓝色部分为TPU层以及与之有关的功能</p>
</blockquote>
<p><img src="/images/image-20250418175103004.png" alt="image-20250418175103004"></p>
<h2 id="计算图IR设计"><a href="#计算图IR设计" class="headerlink" title="计算图IR设计"></a><font color = brown>计算图IR设计</font></h2><h2 id="Top-dialect设计"><a href="#Top-dialect设计" class="headerlink" title="Top dialect设计"></a><font color = green>Top dialect设计</font></h2><p>这一部分是很好的机器学习静态图表示的资料，因为<code>Top dialect</code>完整的涵盖了机器学习的大部分算子操作，做了相当完善的抽象。这里的解读以<code>Top_conv</code>算子来解读。</p>
<h4 id="Interface设计"><a href="#Interface设计" class="headerlink" title="Interface设计"></a>Interface设计</h4><h4 id="Canoicalization设计"><a href="#Canoicalization设计" class="headerlink" title="Canoicalization设计"></a>Canoicalization设计</h4><h2 id="Pass优化设计"><a href="#Pass优化设计" class="headerlink" title="Pass优化设计"></a><font color = brown>Pass优化设计</font></h2><h2 id="后端设计"><a href="#后端设计" class="headerlink" title="后端设计"></a><font color  = brown>后端设计</font></h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">|---include/tpu_mlir</span><br><span class="line">	|---Dialect</span><br><span class="line">		|---TPU/IR/TpuOps.td：各算子定义，codegen函数定义在traits中</span><br><span class="line">	|---Interfaces</span><br><span class="line">		|---*GenInterface.td：各种codegen模式的声明与实现</span><br><span class="line">|---lib</span><br><span class="line">	|---Backend</span><br><span class="line">		|---Arch.cpp：架构抽象基类，包含后端函数（供codegen调用）与硬件参数</span><br><span class="line">		|---BM168x：168x系列的架构类实现</span><br><span class="line">		|---CV18xx：cv18xx系列的架构类实现</span><br><span class="line">	|---Builder：bmodel定义</span><br><span class="line">	|---Dialect</span><br><span class="line">		|---Tpu</span><br><span class="line">			|---Interfaces：各op codegen函数的重载实现</span><br><span class="line">			|---Transforms</span><br><span class="line">				|---BM168x/Codegen.cpp：bm168系列codegen pass实现</span><br><span class="line">	|---Interfaces：</span><br><span class="line">		|---*GenInterface.cpp：各种codegen模式的补充实现（大部分逻辑实现在了td文件中）</span><br></pre></td></tr></table></figure>

<p>上图是TPU-MLIR的后端代码结构，主要有如下几个重点：</p>
<ul>
<li>TPU dialect用来抽象tpu硬件指令集（这里和AIE dialect等抽象层级还不一样，这里的抽象是对于所有TPU的一个粗粒度抽象，后续还要通过backend映射到对应tpu。<strong>因此这个TPU dialect的设计十分有参考意义</strong>）。</li>
<li>Backend针对每个TPU架构都有对应的指令集api。</li>
<li>Interface里面主要是各个TPU operation的codegen逻辑。<ul>
<li>codegen分为localgen和global gen，对应的是argument是在global mem中还是显示移动到local mem中。</li>
<li><strong>所有算子均要实现global mem</strong>，部分算子考虑layer grouping策略，所以要实现local  mem。</li>
</ul>
</li>
</ul>
<blockquote>
<p>TPU-MLIR后端的另一个重点是，编译器后端如何和runtime系统协同做算子映射和调度的。</p>
</blockquote>
<h3 id="TPU-MLIR-backend框架"><a href="#TPU-MLIR-backend框架" class="headerlink" title="TPU-MLIR backend框架"></a><font color = green>TPU-MLIR backend框架</font></h3><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color = brown>参考资料</font></h2><ol>
<li><a href="https://zhuanlan.zhihu.com/p/615180103">TPU-MLIR博客</a></li>
<li><a href="https://mlir.llvm.org/docs/Interfaces/">MLIR interface官方doc</a></li>
<li><a href="https://tpumlir.org/en/">TPU-MLIR 官方文档</a></li>
<li><a href="https://tpumlir.org/en/2023/04/04/how-to-add-a-new-operator-for-tpu-mlir.html">Add an op for TPU-MLIR – blog</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/718042043">TPU-MLIR blog 1</a></li>
</ol>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>机器学习编译</category>
        <category>TPU技术</category>
        <category>量化技术</category>
      </categories>
      <tags>
        <tag>机器学习编译器</tag>
        <tag>mlir</tag>
        <tag>异构计算系统</tag>
      </tags>
  </entry>
  <entry>
    <title>计算密集算子融合</title>
    <url>/2025/03/31/%E8%AE%A1%E7%AE%97%E5%AF%86%E9%9B%86%E7%AE%97%E5%AD%90%E8%9E%8D%E5%90%88/</url>
    <content><![CDATA[<p><img src="/images/image-20250331211738664.png" alt="image-20250331211738664"></p>
<span id="more"></span>

<blockquote>
<p>机器学习中算子可以分为<strong>计算密集算子</strong>和<strong>访存密集算子</strong>，之前的博客已经讲解了<strong>访存密集算子融合技术</strong>，本文重点解读计算密集算子融合，即针对gemm等算子的计算优化。本文主要结合BladeDISC和Rammer等文章加以解读</p>
</blockquote>
<h2 id="BladeDISC的计算密集算子融合"><a href="#BladeDISC的计算密集算子融合" class="headerlink" title="BladeDISC的计算密集算子融合"></a><font color = brown>BladeDISC的计算密集算子融合</font></h2><h3 id="计算密集算子融合pipeline-overview"><a href="#计算密集算子融合pipeline-overview" class="headerlink" title="计算密集算子融合pipeline overview"></a><font color = green>计算密集算子融合pipeline overview</font></h3><p>在<code>BladeDISC</code>项目的pipeline中，涉及计算密集算子融合有三处：</p>
<h4 id="针对简单的mhlo-dot算子做融合"><a href="#针对简单的mhlo-dot算子做融合" class="headerlink" title="针对简单的mhlo_dot算子做融合"></a>针对简单的mhlo_dot算子做融合</h4><p><img src="/images/image-20250403152318767.png" alt="image-20250403152318767"></p>
<p>具体流程图如下：</p>
<pre class="mermaid">flowchart TD
    A[开始: DiscDotMergePass] --> B[执行共享操作数合并]
    B --> C[执行批量合并]
    C --> D{合并成功?}
    D -->|是| E[结束]
    D -->|否| F[标记失败]

    subgraph 共享操作数合并流程
        B1[遍历所有基本块] --> B2[初始化ShareOperandMap]
        B2 --> B3[遍历block中的DotGeneralOp]
        B3 --> B4[提取共享操作数和维度信息]
        B4 --> B5[构建聚类映射]
        B5 --> B6[检测循环依赖]
        B6 --> B7[尝试合并聚类]
        B7 --> B8[应用合并操作]
        B8 --> B9[清理原始操作]
    end

    subgraph 批量合并流程
        C1[形状分析] --> C2[构建MergingShapeMap]
        C2 --> C3[检测可合并聚类]
        C3 --> C4[维度扩展操作]
        C4 --> C5[创建批量concat]
        C5 --> C6[生成批量dot]
        C6 --> C7[切片替换原始操作]
        C7 --> C8[清理原始操作]
    end

    B -->|核心操作| B1
    C -->|核心操作| C1
    B9 -->|清理后| C
    C8 --> D</pre>

<h4 id="混合精度优化"><a href="#混合精度优化" class="headerlink" title="混合精度优化"></a>混合精度优化</h4><p><img src="/images/image-20250402205232495.png" alt="image-20250402205232495"></p>
<h4 id="针对GEMM做layout优化"><a href="#针对GEMM做layout优化" class="headerlink" title="针对GEMM做layout优化"></a>针对GEMM做layout优化</h4><p><img src="/images/image-20250402205308725.png" alt="image-20250402205308725"></p>
<p>具体流程图如下：</p>
<pre class="mermaid">graph TD
    A[原始卷积操作 mhlo::ConvolutionOp] --> B[转换为 DynamicConvOp]
    B --> C[提取卷积参数]
    C --> D[推断预期布局]
    D --> E{当前布局符合预期?}
    E -->|是| F[保持原布局]
    E -->|否| G[插入转置操作调整布局]
    G --> H[输入布局调整]
    G --> I[滤波器布局调整]
    G --> J[输出布局调整]
    H --> K[更新输入布局]
    I --> L[更新滤波器布局]
    J --> M[更新输出布局]
    K --> N[更新卷积属性]
    L --> N
    M --> N
    N --> O[优化后的卷积操作]
    
    P[量化卷积操作 QuantizedDynamicConvOp] --> C
    Q[GPU/CUDA环境] --> D
    R[CPU环境] --> D
    
    style A fill:#f9d,stroke:#333
    style P fill:#f9d,stroke:#333
    style B fill:#bbf,stroke:#333
    style C fill:#cfc,stroke:#333
    style D fill:#cfc,stroke:#333
    style E fill:#ffd,stroke:#333
    style G fill:#fcc,stroke:#333
    style N fill:#cfc,stroke:#333
    style O fill:#9f9,stroke:#333</pre>

]]></content>
      <categories>
        <category>编译技术</category>
        <category>机器学习编译</category>
        <category>计算优化</category>
      </categories>
      <tags>
        <tag>机器学习编译器</tag>
        <tag>mlir</tag>
        <tag>算子融合技术</tag>
      </tags>
  </entry>
  <entry>
    <title>访存密集算子融合</title>
    <url>/2025/04/01/%E8%AE%BF%E5%AD%98%E5%AF%86%E9%9B%86%E7%AE%97%E5%AD%90%E8%9E%8D%E5%90%88/</url>
    <content><![CDATA[<p><img src="/images/image-20250331214150737.png" alt="image-20250331214150737"></p>
<span id="more"></span>

<blockquote>
<p>这是算子优化系列的第一篇，主要聚焦于存储密集型算子的融合优化。探讨这一主题的动机源于 BladeDISC 编译器，这个由阿里云开发的编译器专注于解决动态 shape 问题，并在大模型业务中实现了显著的优化。BladeDISC 的一大亮点是复用了 astitch 论文中提出的多元访存算子融合方法，因此，本文将对这一问题展开深入讨论，从两方面来分析：XLA传统融合和BladeDISC使用的stich融合。</p>
</blockquote>
<h2 id="XLA代码解析"><a href="#XLA代码解析" class="headerlink" title="XLA代码解析"></a><font color = brown>XLA代码解析</font></h2><p>在阿里的bladedisc的ppt中，可以看到如下总结：</p>
<p><img src="/images/image-20250325195503234.png" alt="image-20250325195503234"></p>
<p>先考虑从<code>mlir-hlo</code>项目入手，理解xla alike的fusion是如何做的。具体代码在<a href="https://github.com/tensorflow/mlir-hlo/blob/1857b1eac21ef5b30b088ccc79ef2fa0e3161621/lib/Dialect/mhlo/transforms/mhlo_fusion.cc#L549">mhlo_fusion.cc</a>中。上述图片其实已经总结了XLA可以做的两种kernel fusion方式：<code>KLoop</code>和<code>KInput</code>。可以看出，<code>XLA</code>还是重点关注的算子pattern是比较简单的。</p>
<h3 id="Source-Code源码分析"><a href="#Source-Code源码分析" class="headerlink" title="Source Code源码分析"></a><font color = green>Source Code源码分析</font></h3><blockquote>
<p>核心原理是通过<strong>形状约束分析</strong>和图论中的<strong>边收缩算法</strong>，动态识别可融合的操作组，并生成高效的融合计划。</p>
<p><font color = red>形状约束是阿里团队提出的动态shape约束。</font></p>
</blockquote>
<p>该代码是典型的两阶段code，第一阶段分析出fusion plan，第二阶段应用fusion plan做相应的fusion：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">runOnOperation</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">    FuncOp func = <span class="built_in">getOperation</span>();</span><br><span class="line">    <span class="keyword">if</span> (!<span class="built_in">IsTargetFunc</span>(func)) &#123;</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// process each block and do fusion within a block.</span></span><br><span class="line">    <span class="keyword">for</span> (Block&amp; block : func) &#123;</span><br><span class="line">      <span class="comment">// 收集一个block内部的所有operation list</span></span><br><span class="line">      SmallVector&lt;Operation*, <span class="number">4</span>&gt; op_list;</span><br><span class="line">      <span class="keyword">for</span> (Operation&amp; op : block) &#123;</span><br><span class="line">        op_list.<span class="built_in">push_back</span>(&amp;op);</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="function">FusionPlanner <span class="title">planner</span><span class="params">(op_list)</span></span>;</span><br><span class="line">      llvm::Optional&lt;FusionPlan&gt; plan = planner.<span class="built_in">Run</span>();</span><br><span class="line">      <span class="keyword">if</span> (!plan) &#123;</span><br><span class="line">        <span class="built_in">emitError</span>(func.<span class="built_in">getLoc</span>(), <span class="string">&quot;can&#x27;t find a fusion plan&quot;</span>);</span><br><span class="line">        <span class="built_in">signalPassFailure</span>();</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (!<span class="built_in">ApplyFusionPlan</span>(*plan)) &#123;</span><br><span class="line">        <span class="built_in">emitError</span>(func.<span class="built_in">getLoc</span>(), <span class="string">&quot;apply fusion plan failed&quot;</span>);</span><br><span class="line">        <span class="built_in">signalPassFailure</span>();</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>上述代码段时整个pass的入口，也是整体流程控制。</p>
<ul>
<li><code>FusionPlanner planner(op_list)</code>针对一个block的oplist，初始化一个fusion plan。</li>
<li><code>planner.Run()</code>根据planner构建的图，做子图划分，生成潜在的fusion pattern集和。</li>
<li><code>ApplyFusionPlan</code>为第二阶段，将fusion pattern list用于代码改写，完成最终的fusion操作。</li>
</ul>
<p>接下来分别展开这三个函数。</p>
<h4 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h4><p>下面先来介绍一些辅助函数：</p>
<ol>
<li><p>首先，先明确<code>XLA Fusion</code>的几个<strong>基本数据结构概念</strong>：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> FusionPattern = std::vector&lt;Operation*&gt;;</span><br><span class="line"><span class="keyword">using</span> FusionPlan = std::vector&lt;FusionPattern&gt;;</span><br></pre></td></tr></table></figure>

<p>FusionPattern是可融合的operation集和。FusionPlan是FusionPattern集和。</p>
</li>
<li><p><code>XLA</code>针对memory-intensive的算子，主要考虑如下两个算子：<code>ReduceOp</code>和<code>element-wiseOp</code>。分别有如下可能的融合方案：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// reduceOp主要判断，其operand的define point的op是否shape相同</span></span><br><span class="line"><span class="comment">// 如果相同，reduceOp考虑的模式是operand融合</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">IsFusibleWithOperand</span><span class="params">(Operation* op)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 相比IsFusibleWithConsumer，支持reduce op</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">IsMhlo</span>(op) &amp;&amp;</span><br><span class="line">         (op-&gt;<span class="built_in">hasTrait</span>&lt;::mlir::OpTrait::Elementwise&gt;() || <span class="built_in">isa</span>&lt;ReduceOp&gt;(op));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// element-wise操作或是常量操作，可以考虑是否可以和consumer融合</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">IsFusibleWithConsumer</span><span class="params">(Operation* op)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 必须是MHLO操作，并且是elementwise操作，或是常量操作</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">IsMhlo</span>(op) &amp;&amp; (op-&gt;<span class="built_in">hasTrait</span>&lt;::mlir::OpTrait::Elementwise&gt;() ||</span><br><span class="line">                        <span class="built_in">matchPattern</span>(op, <span class="built_in">m_Constant</span>()));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 全局的isFusible判断</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">IsFusible</span><span class="params">(Operation* op)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 只有常量操作或是能和consumer融合的操作才是可融合的，或是能和operand融合的操作</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">matchPattern</span>(op, <span class="built_in">m_Constant</span>()) || <span class="built_in">IsFusibleWithConsumer</span>(op) ||</span><br><span class="line">         <span class="built_in">IsFusibleWithOperand</span>(op);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>针对<code>FusionPattern</code>和其他block的交互：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">SmallVector&lt;Value, 4&gt; <span class="title">GetInputsOfFusionPattern</span><span class="params">(<span class="type">const</span> FusionPattern&amp; pattern)</span> </span>&#123;</span><br><span class="line">  SmallVector&lt;Value, <span class="number">4</span>&gt; inputs;</span><br><span class="line">  DenseSet&lt;Value&gt; input_set;</span><br><span class="line">  DenseSet&lt;Operation*&gt; op_set;</span><br><span class="line">  <span class="comment">// 收集一个fusion pattern里的所有operation</span></span><br><span class="line">  <span class="keyword">for</span> (Operation* op : pattern) &#123;</span><br><span class="line">    <span class="type">bool</span> inserted = op_set.<span class="built_in">insert</span>(op).second;</span><br><span class="line">    (<span class="type">void</span>)inserted;</span><br><span class="line">    <span class="built_in">assert</span>(inserted &amp;&amp; <span class="string">&quot;FusionPattern contains duplicate operations&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (Operation* op : pattern) &#123;</span><br><span class="line">    <span class="keyword">for</span> (Value operand : op-&gt;<span class="built_in">getOperands</span>()) &#123;</span><br><span class="line">      Operation* operand_op = operand.<span class="built_in">getDefiningOp</span>();</span><br><span class="line">      <span class="comment">// 如果defining op在pattern里，则跳过</span></span><br><span class="line">      <span class="comment">// 否则加入到inputs中，表示是该潜在的fusion pattern的输入</span></span><br><span class="line">      <span class="keyword">if</span> (op_set.<span class="built_in">find</span>(operand_op) != op_set.<span class="built_in">end</span>()) &#123;</span><br><span class="line">        <span class="comment">// skip if defining op is in the pattern</span></span><br><span class="line">        <span class="keyword">continue</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (input_set.<span class="built_in">insert</span>(operand).second) &#123;</span><br><span class="line">        inputs.<span class="built_in">push_back</span>(operand);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> inputs;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 收集融合操作中所有被外部使用的输出，作为融合后的输出</span></span><br><span class="line"><span class="function">SmallVector&lt;Value, 4&gt; <span class="title">GetOutputsOfFusionPattern</span><span class="params">(<span class="type">const</span> FusionPattern&amp; pattern)</span> </span>&#123;</span><br><span class="line">  SmallVector&lt;Value, <span class="number">4</span>&gt; outputs;</span><br><span class="line">  DenseSet&lt;Operation*&gt; op_set;</span><br><span class="line">  <span class="keyword">for</span> (Operation* op : pattern) &#123;</span><br><span class="line">    <span class="comment">// 检查是否有重复的operation</span></span><br><span class="line">    <span class="type">bool</span> inserted = op_set.<span class="built_in">insert</span>(op).second;</span><br><span class="line">    (<span class="type">void</span>)inserted;</span><br><span class="line">    <span class="built_in">assert</span>(inserted &amp;&amp; <span class="string">&quot;FusionPattern contains duplicate operations&quot;</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 尝试做融合的operation</span></span><br><span class="line">  <span class="keyword">for</span> (Operation* op : pattern) &#123;</span><br><span class="line">    <span class="keyword">for</span> (Value result : op-&gt;<span class="built_in">getResults</span>()) &#123;</span><br><span class="line">      <span class="comment">// 判断该operation是否result有被外部使用</span></span><br><span class="line">      <span class="type">bool</span> has_external_user = llvm::<span class="built_in">any_of</span>(</span><br><span class="line">          result.<span class="built_in">getUses</span>(),</span><br><span class="line">          [&amp;](OpOperand&amp; use) &#123; <span class="keyword">return</span> !op_set.<span class="built_in">count</span>(use.<span class="built_in">getOwner</span>()); &#125;);</span><br><span class="line">      <span class="comment">// 显示收集被外部使用的output</span></span><br><span class="line">      <span class="keyword">if</span> (has_external_user) &#123;</span><br><span class="line">        outputs.<span class="built_in">push_back</span>(result);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> outputs;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述两个函数支持获取FusionPattern的外部输入和输出。</p>
</li>
<li><p>合并两个<code>FusionPattern</code>，这个函数比较重要，其实就是发现可以合并融合的operation list，扩大单个fusion region：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">FusionPattern <span class="title">MergeFusionPattern</span><span class="params">(<span class="type">const</span> FusionPattern&amp; lhs,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 <span class="type">const</span> FusionPattern&amp; rhs)</span> </span>&#123;</span><br><span class="line">  <span class="function">FusionPattern <span class="title">pattern</span><span class="params">(lhs)</span></span>;</span><br><span class="line">  pattern.<span class="built_in">insert</span>(pattern.<span class="built_in">end</span>(), rhs.<span class="built_in">begin</span>(), rhs.<span class="built_in">end</span>());</span><br><span class="line">  <span class="keyword">return</span> pattern;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>并查集用于做shape推导，<font color = red>注意，XLA的fusion中，shape 推导是比较简单的</font>:</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ShapeConstraintAnalysis</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">ShapeConstraintAnalysis</span><span class="params">(<span class="type">const</span> SmallVectorImpl&lt;Operation*&gt;&amp; op_list)</span> </span>&#123;</span><br><span class="line">    <span class="built_in">PropagateEquality</span>(op_list);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Returns true is `lhs` and `rhs` are supposed to have same shape.</span></span><br><span class="line">  <span class="function"><span class="type">bool</span> <span class="title">HasSameShape</span><span class="params">(Value lhs, Value rhs)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 单纯判断两者在unionfind中的位置是否相同</span></span><br><span class="line">    <span class="keyword">return</span> impl_.<span class="built_in">isEquivalent</span>(<span class="built_in">ValueWrapper</span>(lhs), <span class="built_in">ValueWrapper</span>(rhs));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="comment">// shape equality propagation based on the shape constrains of</span></span><br><span class="line">  <span class="comment">// elementwise ops.</span></span><br><span class="line">  <span class="comment">// 针对elementwise操作，做shape相等传播</span></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">PropagateEquality</span><span class="params">(<span class="type">const</span> SmallVectorImpl&lt;Operation*&gt;&amp; op_list)</span> </span>&#123;</span><br><span class="line">    <span class="type">bool</span> converged = <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">      converged = <span class="literal">true</span>;</span><br><span class="line">      <span class="comment">// 显示对两个value做unionfind</span></span><br><span class="line">      <span class="keyword">auto</span> update = [&amp;](Value lhs, Value rhs) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!impl_.<span class="built_in">isEquivalent</span>(<span class="built_in">ValueWrapper</span>(lhs), <span class="built_in">ValueWrapper</span>(rhs))) &#123;</span><br><span class="line">          <span class="comment">// 有更改，说明还没有完全收敛</span></span><br><span class="line">          converged = <span class="literal">false</span>;</span><br><span class="line">          impl_.<span class="built_in">unionSets</span>(<span class="built_in">ValueWrapper</span>(lhs), <span class="built_in">ValueWrapper</span>(rhs));</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;;</span><br><span class="line">      <span class="keyword">for</span> (Operation* op : op_list) &#123;</span><br><span class="line">        <span class="comment">// 只对有InferShapeEqualityOpInterface trait的operation做shape相等传播</span></span><br><span class="line">        <span class="keyword">auto</span> op_fusibility = <span class="built_in">dyn_cast</span>&lt;InferShapeEqualityOpInterface&gt;(op);</span><br><span class="line">        <span class="keyword">if</span> (!op_fusibility) <span class="keyword">continue</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">int</span> numInput = op-&gt;<span class="built_in">getNumOperands</span>();</span><br><span class="line">        <span class="type">int</span> numOutput = op-&gt;<span class="built_in">getNumResults</span>();</span><br><span class="line">        <span class="comment">// shape equality propagation between inputs.</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> input1 = <span class="number">0</span>; input1 &lt; numInput; ++input1)</span><br><span class="line">          <span class="keyword">for</span> (<span class="type">int</span> input2 = input1 + <span class="number">1</span>; input2 &lt; numInput; ++input2)</span><br><span class="line">            <span class="comment">// 通过op_fusibility.inferInputsShapeEquality函数判断两个input是否shape相等</span></span><br><span class="line">            <span class="keyword">if</span> (op_fusibility.<span class="built_in">inferInputsShapeEquality</span>(input1, input2))</span><br><span class="line">              <span class="built_in">update</span>(op-&gt;<span class="built_in">getOperand</span>(input1), op-&gt;<span class="built_in">getOperand</span>(input2));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// shape equality propagation between outputs.</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> output1 = <span class="number">0</span>; output1 &lt; numOutput; ++output1)</span><br><span class="line">          <span class="keyword">for</span> (<span class="type">int</span> output2 = output1 + <span class="number">1</span>; output2 &lt; numOutput; ++output2)</span><br><span class="line">            <span class="comment">// 同理，判断两个output是否shape相等</span></span><br><span class="line">            <span class="keyword">if</span> (op_fusibility.<span class="built_in">inferOutputsShapeEquality</span>(output1, output2))</span><br><span class="line">              <span class="built_in">update</span>(op-&gt;<span class="built_in">getResult</span>(output1), op-&gt;<span class="built_in">getResult</span>(output2));</span><br><span class="line"></span><br><span class="line">        <span class="comment">// shape equality propagation between input and output.</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> input = <span class="number">0</span>; input &lt; numInput; ++input)</span><br><span class="line">          <span class="keyword">for</span> (<span class="type">int</span> output = <span class="number">0</span>; output &lt; numOutput; ++output)</span><br><span class="line">            <span class="comment">// 最关键的步骤，判断input和output是否shape相等</span></span><br><span class="line">            <span class="keyword">if</span> (op_fusibility.<span class="built_in">inferInputOutputShapeEquality</span>(input, output))</span><br><span class="line">              <span class="comment">// 如果相等，则调用lambda函数，将两者做unionfind</span></span><br><span class="line">              <span class="built_in">update</span>(op-&gt;<span class="built_in">getOperand</span>(input), op-&gt;<span class="built_in">getResult</span>(output));</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">while</span> (!converged);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// a UnionFind set</span></span><br><span class="line">  <span class="comment">// 使用LLVM提供的内置UF集和</span></span><br><span class="line">  EquivalenceClasses&lt;ValueWrapper&gt; impl_;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="planner的初始化"><a href="#planner的初始化" class="headerlink" title="planner的初始化"></a>planner的初始化</h4><p>FusionPlanner的初始化逻辑是比较简单的：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">FusionPlanner</span> &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">FusionPlanner</span><span class="params">(<span class="type">const</span> SmallVectorImpl&lt;Operation*&gt;&amp; op_list)</span></span></span><br><span class="line"><span class="function">      : op_list_(op_list),</span></span><br><span class="line"><span class="function">        shape_analysis_(op_list),</span></span><br><span class="line"><span class="function">        cycle_detector_(op_list.size()) &#123;</span></span><br><span class="line">    <span class="comment">// 构建初始的cluster图</span></span><br><span class="line">    <span class="built_in">BuildNodeMap</span>();</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>主要干了如下事情：</p>
<ol>
<li>初始化op_list_，用于后续的fusion</li>
<li>初始化shape_analysis_工具，用于知道fusion的shape compatible analysis</li>
<li>初始化cycle detector，<code>XLA</code>的fusion中，一个条件是不能引入loop</li>
</ol>
<p>最后构建一个nodemap，构建一个cluster图，用于后续子图生成，op遍历。</p>
<p>成员变量如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">const</span> SmallVectorImpl&lt;Operation*&gt;&amp; op_list_;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Shape equality checker</span></span><br><span class="line">ShapeConstraintAnalysis shape_analysis_;</span><br><span class="line"></span><br><span class="line"><span class="comment">// op -&gt; node_id</span></span><br><span class="line">std::unordered_map&lt;Operation*, <span class="type">int</span>&gt; op_to_node_id_;</span><br><span class="line"></span><br><span class="line"><span class="comment">// make sure not introduce cycle after fusion</span></span><br><span class="line">GraphCycles cycle_detector_;</span><br><span class="line">std::vector&lt;std::unique_ptr&lt;Cluster&gt;&gt; cluster_storage_;</span><br><span class="line"></span><br><span class="line"><span class="comment">// a UnionFind set. Each set represents a (partial) fused pattern</span></span><br><span class="line"><span class="comment">// and has a leader as representation.</span></span><br><span class="line">EquivalenceClasses&lt;<span class="type">int32_t</span>&gt; leader_for_node_;</span><br></pre></td></tr></table></figure>

<p>构建图的逻辑如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">BuildNodeMap</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 当前初始构建图，每个operation为一个node，为一个cluster的head，并leader_for_node_的并查集存储</span></span><br><span class="line">    <span class="type">int</span> num_nodes = op_list_.<span class="built_in">size</span>();</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> node_id = <span class="number">0</span>; node_id &lt; num_nodes; ++node_id) &#123;</span><br><span class="line">      <span class="comment">// 针对当前operation，构建一个cluster</span></span><br><span class="line">      <span class="comment">// 并设定当前op为cluster的头leader</span></span><br><span class="line">      Operation* op = op_list_[node_id];</span><br><span class="line">      <span class="built_in">MakeCluster</span>(node_id);</span><br><span class="line">      op_to_node_id_[op] = node_id;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// learder_for_node_是一个UnionFind set</span></span><br><span class="line">      leader_for_node_.<span class="built_in">insert</span>(node_id);</span><br><span class="line">      <span class="keyword">for</span> (Value operand : op-&gt;<span class="built_in">getOperands</span>()) &#123;</span><br><span class="line">        Operation* operand_op = operand.<span class="built_in">getDefiningOp</span>();</span><br><span class="line">        <span class="keyword">if</span> (operand_op == <span class="literal">nullptr</span>) &#123;</span><br><span class="line">          <span class="comment">// skip block argument</span></span><br><span class="line">          <span class="keyword">continue</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 检查operand的def point是否属于别的融合组</span></span><br><span class="line">        <span class="comment">// 显示构建依赖关系</span></span><br><span class="line">        <span class="keyword">auto</span> iter = op_to_node_id_.<span class="built_in">find</span>(operand_op);</span><br><span class="line">        <span class="built_in">assert</span>(iter != op_to_node_id_.<span class="built_in">end</span>());</span><br><span class="line">        cycle_detector_.<span class="built_in">InsertEdge</span>(iter-&gt;second, node_id);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>遍历每一个op，在unionfind中，每一个op为一个cluster，后续fusion操作才会融合cluster。</li>
<li>初始化op_to_node_id_等的映射方式，unionfind中存储的是该op的node_id，即在当前fusionplan中的次序号。</li>
<li>operand的define op和当前op，在cycle_detector中显示插入依赖链条。</li>
</ul>
<p>上述整体逻辑是十分清楚的。</p>
<p>在FusionPlanner中，有一个私有类是<code>Cluster</code>：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Represent a (partial) fused pattern</span></span><br><span class="line">  <span class="comment">// 根据注释，这个cluster类并不是完整的融合模式，而是一个融合模式的一部分</span></span><br><span class="line">  <span class="comment">// 这是unionfind的思想，每个cluster都是一个融合模式的一部分，最终通过unionfind合并</span></span><br><span class="line">  <span class="keyword">class</span> <span class="title class_">Cluster</span> &#123;</span><br><span class="line">   <span class="keyword">public</span>:</span><br><span class="line">    <span class="built_in">Cluster</span>(<span class="type">int</span> node_id, FusionPlanner* planner) : <span class="built_in">node_id_</span>(node_id) &#123;</span><br><span class="line">      <span class="type">const</span> SmallVectorImpl&lt;Operation*&gt;&amp; op_list = planner-&gt;<span class="built_in">op_list</span>();</span><br><span class="line">      pattern_.<span class="built_in">push_back</span>(op_list[node_id]);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Merges `other` into this cluster, and clears `other`.</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">Merge</span><span class="params">(Cluster* other)</span> </span>&#123;</span><br><span class="line">      pattern_.<span class="built_in">insert</span>(pattern_.<span class="built_in">end</span>(), other-&gt;pattern_.<span class="built_in">begin</span>(),</span><br><span class="line">                      other-&gt;pattern_.<span class="built_in">end</span>());</span><br><span class="line">      other-&gt;pattern_.<span class="built_in">clear</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The number of nodes in this cluster.</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">cluster_size</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> pattern_.<span class="built_in">size</span>(); &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The ID of the cluster as represented in `cycle_detector_`.</span></span><br><span class="line">    <span class="function"><span class="type">int</span> <span class="title">cycles_graph_node_id</span><span class="params">()</span> <span class="type">const</span> </span>&#123; <span class="keyword">return</span> node_id_; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Sets the ID of the cluster as represented in `cycle_detector_`.</span></span><br><span class="line">    <span class="function"><span class="type">void</span> <span class="title">set_cycles_graph_node_id</span><span class="params">(<span class="type">int</span> cycles_graph_node_id)</span> </span>&#123;</span><br><span class="line">      node_id_ = cycles_graph_node_id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Currently the fused pattern this cluster holds.</span></span><br><span class="line">    <span class="function"><span class="type">const</span> FusionPattern&amp; <span class="title">fused_pattern</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> pattern_; &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">// ID of the representative node of this cluster.</span></span><br><span class="line">    <span class="type">int</span> node_id_;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// the fused pattern this cluster holds.</span></span><br><span class="line">    FusionPattern pattern_;</span><br><span class="line">  &#125;;</span><br></pre></td></tr></table></figure>

<p>上述cluster最主要的功能，就是可以对于cluster中的operation，显示记录他们可以应用的<code>FusionPattern</code>。同时还支持merge操作，将cluster之间进行fusion。该原理是将另一个cluster的pattern拷贝入当前cluster，并清空另一个cluster的pattern。</p>
<p><img src="/images/2.png" alt="2"></p>
<h4 id="planner运行"><a href="#planner运行" class="headerlink" title="planner运行"></a>planner运行</h4><p>核心部分，主要作用是构建cluster融合，并给对应cluster赋予fusion pattern。</p>
<p>主函数如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Returns a fusion plan if success, otherwise none.</span></span><br><span class="line">  <span class="comment">// 返回一个fusion plan，如果没有找到则返回none</span></span><br><span class="line">  <span class="function">llvm::Optional&lt;FusionPlan&gt; <span class="title">Run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Greedily search connected fusible pattern, and ops belonging to</span></span><br><span class="line">    <span class="comment">// a same fusion pattern are grouped into a cluster.</span></span><br><span class="line">    <span class="comment">// 每一个op有一个可融合的pattern，找寻compatible的pattern并合并对应的operation</span></span><br><span class="line">    <span class="built_in">RunEdgeContractionLoop</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// After doing edge contraction, each unique cluster having size</span></span><br><span class="line">    <span class="comment">// more than one represents a potential fusion pattern.</span></span><br><span class="line">    <span class="comment">// We collect all these clusters and construct a fusion plan.</span></span><br><span class="line">    <span class="comment">// union find中，每一个有大于1的size的cluster都是一个融合模式</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// Note that the ops in a fusion pattern are in topological ordering.</span></span><br><span class="line">    <span class="comment">// fusion pattern中的operation是按照拓扑排序的</span></span><br><span class="line">    <span class="comment">// 得到一个fusion plan集合后，后续apply这些fusion plan即可</span></span><br><span class="line">    FusionPlan plan;</span><br><span class="line">    DenseMap&lt;<span class="type">int</span>, <span class="type">int</span>&gt; pattern_ids;</span><br><span class="line">    <span class="keyword">for</span> (Operation* op : op_list_) &#123;</span><br><span class="line">      <span class="comment">// 获取该op所属的cluster</span></span><br><span class="line">      Cluster* cluster = <span class="built_in">GetClusterForNode</span>(op);</span><br><span class="line">      <span class="type">int</span> node_id = cluster-&gt;<span class="built_in">cycles_graph_node_id</span>();</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 不可融合或是融合模式的size小于等于1，跳过</span></span><br><span class="line">      <span class="keyword">if</span> (!<span class="built_in">IsFusible</span>(op_list_[node_id]) ||</span><br><span class="line">          <span class="built_in">EffectiveSize</span>(<span class="built_in">GetClusterForNode</span>(op)-&gt;<span class="built_in">fused_pattern</span>()) &lt;= <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">continue</span>;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (!pattern_ids.<span class="built_in">count</span>(node_id)) &#123;</span><br><span class="line">        <span class="type">int</span> pattern_id = pattern_ids.<span class="built_in">size</span>();</span><br><span class="line">        pattern_ids[node_id] = pattern_id;</span><br><span class="line">        plan.<span class="built_in">emplace_back</span>();</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 特定的plan后添加operation</span></span><br><span class="line">      plan[pattern_ids[node_id]].<span class="built_in">push_back</span>(op);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> plan;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<blockquote>
<p>这里需要理解一个重要的点：每一个cluster都有一个list，该list存储所有的fusion Pattern，一个cluster是否可以做融合，其实就是看发现了多少个fusion Pattern。</p>
<ul>
<li>如果fusion pattern &gt; 1，则其<strong>内部可以融合</strong>。</li>
<li>如果cluster之间有兼容的fusion pattern，则<strong>intra cluster 融合</strong>是ok的。</li>
</ul>
<p><font color =red>识别出潜在的fusion后，构造fusion plan：一个fusion pattern的vector存储结构。</font></p>
</blockquote>
<p>上述code可以分为两大阶段：<code>RunEdgeContractionLoop()</code>和融合pattern收集。</p>
<p><img src="/images/1.png" alt="1"></p>
<p>上述是一个完整的流程图。可以看出，run()函数最重要的function是<code>RunEdgeContractionLoop()</code>。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Greedily fuse connected node.</span></span><br><span class="line"><span class="comment">// 贪心算法，融合可融合的node</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">RunEdgeContractionLoop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">using</span> std::placeholders::_1;</span><br><span class="line">    <span class="keyword">using</span> std::placeholders::_2;</span><br><span class="line">    <span class="comment">// 理解std::bind操作</span></span><br><span class="line">    <span class="comment">// 给TryToContractEdge函数绑定了两个参数，第一个参数是this指针，第二个参数是_1和_2，是占位符</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">ForEachEdgeInPostOrder</span>(</span><br><span class="line">        std::<span class="built_in">bind</span>(&amp;FusionPlanner::TryToContractEdge, <span class="keyword">this</span>, _1, _2));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>对于graph做后序遍历，并<strong>贪心引用边收缩算法</strong>。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 按照后序遍历的顺序，对每一个edge执行fn函数</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> FnTy&gt;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">ForEachEdgeInPostOrder</span><span class="params">(FnTy fn)</span> </span>&#123;</span><br><span class="line"><span class="type">bool</span> changed = <span class="literal">false</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int32_t</span> node : cycle_detector_.<span class="built_in">AllNodesInPostOrder</span>()) &#123;</span><br><span class="line">  Cluster* cluster_from = <span class="built_in">GetClusterForCyclesGraphNode</span>(node);</span><br><span class="line">  <span class="comment">// Make a copy of the set of successors because we may modify the graph in</span></span><br><span class="line">  <span class="comment">// TryToContractEdge.</span></span><br><span class="line">  <span class="comment">// 可能在TryToContractEdge函数中修改后续的node，所以这里需要拷贝一份</span></span><br><span class="line">  std::vector&lt;<span class="type">int32_t</span>&gt; successors_copy =</span><br><span class="line">      cycle_detector_.<span class="built_in">SuccessorsCopy</span>(cluster_from-&gt;<span class="built_in">cycles_graph_node_id</span>());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 对该节点的后续分别尝试做融合</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="type">int</span> to : successors_copy) &#123;</span><br><span class="line">    Cluster* cluster_to = <span class="built_in">GetClusterForCyclesGraphNode</span>(to);</span><br><span class="line">    <span class="comment">// 这里传入的fn是TryToContractEdge函数，传入cluster_from和cluster_to两个参数，通过std::bind绑定。</span></span><br><span class="line">    <span class="type">bool</span> contracted_edge = <span class="built_in">fn</span>(cluster_from, cluster_to);</span><br><span class="line">    changed |= contracted_edge;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> changed;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>该逻辑是，后续访问每个node，并尝试对后续的每个op尝试引用fn函数，即<code>TryToContractEdge</code>函数。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// This function check if fusing `from` with `to` is valid and if so perform</span></span><br><span class="line"><span class="comment">// the merge. The validity is based on the operations in the clusters and</span></span><br><span class="line"><span class="comment">// the compatibility of the shapes of the outputs of the would-be fused</span></span><br><span class="line"><span class="comment">// clusters.</span></span><br><span class="line"><span class="comment">// Returns true is the merge was performed.</span></span><br><span class="line"><span class="comment">// 尝试合并两个cluster，如果合并成功则返回true</span></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">TryToContractEdge</span><span class="params">(Cluster* from, Cluster* to)</span> </span>&#123;</span><br><span class="line"><span class="type">int</span> node_to = to-&gt;<span class="built_in">cycles_graph_node_id</span>();</span><br><span class="line"><span class="type">int</span> node_from = from-&gt;<span class="built_in">cycles_graph_node_id</span>();</span><br><span class="line"></span><br><span class="line"><span class="comment">// Both node_to and node_from should be fusible</span></span><br><span class="line"><span class="keyword">if</span> (!<span class="built_in">IsFusible</span>(op_list_[node_to]) || !<span class="built_in">IsFusible</span>(op_list_[node_from])) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 判断node from是否可以和consumer融合</span></span><br><span class="line"><span class="comment">// 即该node是否是const或是elementwise</span></span><br><span class="line"><span class="keyword">if</span> (!<span class="built_in">IsFusibleWithConsumer</span>(op_list_[node_from])) &#123;</span><br><span class="line">  <span class="comment">// This op cannot be fused with its consumers.</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 判断node to是否可以和operand融合</span></span><br><span class="line"><span class="comment">// 即该node是否是element wise或是const或是reduce</span></span><br><span class="line"><span class="keyword">if</span> (!<span class="built_in">IsFusibleWithOperand</span>(op_list_[node_to])) &#123;</span><br><span class="line">  <span class="comment">// This op cannot be fused with its operands.</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Output shapes of a fusion pattern should be compatible as described in</span></span><br><span class="line"><span class="comment">// the document of this class.</span></span><br><span class="line"><span class="comment">// 判断两个cluster的输出是否shape相同</span></span><br><span class="line">SmallVector&lt;Value, <span class="number">4</span>&gt; results = <span class="built_in">GetResultsOfFusedPattern</span>(from, to);</span><br><span class="line"></span><br><span class="line">Value ref = <span class="built_in">InferEffectiveWorkloadShape</span>(results[<span class="number">0</span>]);</span><br><span class="line"><span class="keyword">if</span> (!llvm::<span class="built_in">all_of</span>(results, [&amp;](Value result) &#123;</span><br><span class="line">      Value val = <span class="built_in">InferEffectiveWorkloadShape</span>(result);</span><br><span class="line">      <span class="keyword">return</span> shape_analysis_.<span class="built_in">HasSameShape</span>(ref, val);</span><br><span class="line">    &#125;)) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 实际的fusion操作，将cluster做fusion</span></span><br><span class="line"><span class="keyword">return</span> <span class="built_in">MergeClusters</span>(from, to);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>注意，如果op是reduceOp，则其只能是to，不能是from，所以<code>reduceOp</code>一定在sub graph的end point。</p>
<ul>
<li><p>判断from和to是否可fuse：isfusable()</p>
</li>
<li><p><code>GetResultsOfFusedPattern</code>：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// returns the outputs if two cluster were merged</span></span><br><span class="line"><span class="function">SmallVector&lt;Value, 4&gt; <span class="title">GetResultsOfFusedPattern</span><span class="params">(Cluster* from, Cluster* to)</span> </span>&#123;</span><br><span class="line"><span class="comment">// 将两个cluster的fused_pattern合并</span></span><br><span class="line">FusionPattern fused_pattern =</span><br><span class="line">    <span class="built_in">MergeFusionPattern</span>(from-&gt;<span class="built_in">fused_pattern</span>(), to-&gt;<span class="built_in">fused_pattern</span>());</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">GetOutputsOfFusionPattern</span>(fused_pattern);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>尝试强行融合两个cluster的pattern，并获取一个fusion厚的output。</p>
</li>
<li><p>显示做shape 判断，来判断前一步的fusion是否是合法的：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">Value ref = <span class="built_in">InferEffectiveWorkloadShape</span>(results[<span class="number">0</span>]);</span><br><span class="line"><span class="keyword">if</span> (!llvm::<span class="built_in">all_of</span>(results, [&amp;](Value result) &#123;</span><br><span class="line">      Value val = <span class="built_in">InferEffectiveWorkloadShape</span>(result);</span><br><span class="line">      <span class="keyword">return</span> shape_analysis_.<span class="built_in">HasSameShape</span>(ref, val);</span><br><span class="line">    &#125;)) &#123;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个inferworkload逻辑如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 根据value的类型，判断workload的shape的推导方式</span></span><br><span class="line"><span class="comment">// 主要针对reduce op做特殊化处理</span></span><br><span class="line"><span class="function">Value <span class="title">InferEffectiveWorkloadShape</span><span class="params">(Value v)</span> </span>&#123;</span><br><span class="line">  Operation* op = v.<span class="built_in">getDefiningOp</span>();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 如果是reduce op，则返回operand的shape</span></span><br><span class="line">  <span class="comment">// 否则，v本身用于推导shape</span></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">isa_and_nonnull</span>&lt;ReduceOp&gt;(op) ? op-&gt;<span class="built_in">getOperand</span>(<span class="number">0</span>) : v;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>取融合后第一个输出的“有效工作负载形状”作为参考。</p>
</li>
<li><p>对普通操作，有效形状即输出本身的形状；对 <code>ReduceOp</code>，有效形状是操作数的形状（因为融合需基于其输入数据的形状）。</p>
</li>
<li><p>对融合后的所有输出结果，逐一检查它们的有效形状是否与参考形状一致。</p>
</li>
<li><p>若存在任意一个输出形状不匹配，返回 <code>false</code>，拒绝合并。</p>
</li>
</ul>
<p>这背后的原理如下:</p>
<blockquote>
<p><strong>kLoop 融合</strong>：要求所有输出形状一致，以放入同一并行循环。</p>
<p><strong>kInput 融合</strong>：</p>
<p>允许包含 <code>ReduceOp</code>，但其有效形状需与其他输出的有效形状一致（即 <code>ReduceOp</code> 的输入形状需与其他输出形状一致）。例如，若融合模式包含一个reduceOp和elementwiseOp，做如下操作：</p>
<ol>
<li><p><code>ReduceOp</code> 的有效形状是其输入（操作数）的形状。</p>
</li>
<li><p><code>ElementWise</code> 的有效形状是其输出的形状。</p>
</li>
<li><p>两者必须相同才能融合。</p>
</li>
</ol>
</blockquote>
</li>
</ul>
<p><img src="/images/3.png" alt="3"></p>
<h4 id="ApplyFusionPlan"><a href="#ApplyFusionPlan" class="headerlink" title="ApplyFusionPlan"></a>ApplyFusionPlan</h4><p>源码如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">bool</span> <span class="title">ApplyFusionPlan</span><span class="params">(<span class="type">const</span> FusionPlan&amp; plan)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 遍历每个融合模式</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">const</span> FusionPattern&amp; pattern : plan) &#123;</span><br><span class="line">        <span class="comment">// 在pattern最后一个操作位置创建Builder</span></span><br><span class="line">        <span class="function">OpBuilder <span class="title">b</span><span class="params">(pattern.back())</span></span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 收集所有操作的位置信息</span></span><br><span class="line">        SmallVector&lt;Location, <span class="number">4</span>&gt; locations;</span><br><span class="line">        locations.<span class="built_in">reserve</span>(pattern.<span class="built_in">size</span>());</span><br><span class="line">        <span class="keyword">for</span> (Operation* op : pattern) &#123;</span><br><span class="line">            locations.<span class="built_in">push_back</span>(op-&gt;<span class="built_in">getLoc</span>());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 创建融合位置标识（用于调试）</span></span><br><span class="line">        Location fused_loc = FusedLoc::<span class="built_in">get</span>(pattern.<span class="built_in">back</span>()-&gt;<span class="built_in">getContext</span>(), locations);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 获取外部输入和输出</span></span><br><span class="line">        SmallVector&lt;Value, <span class="number">4</span>&gt; inputs = <span class="built_in">GetInputsOfFusionPattern</span>(pattern);</span><br><span class="line">        SmallVector&lt;Value, <span class="number">4</span>&gt; outputs = <span class="built_in">GetOutputsOfFusionPattern</span>(pattern);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 构建输出类型列表</span></span><br><span class="line">        SmallVector&lt;Type, <span class="number">4</span>&gt; output_types;</span><br><span class="line">        output_types.<span class="built_in">reserve</span>(outputs.<span class="built_in">size</span>());</span><br><span class="line">        <span class="keyword">for</span> (Value v : outputs) &#123;</span><br><span class="line">            output_types.<span class="built_in">push_back</span>(v.<span class="built_in">getType</span>());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* 消费者调整阶段 */</span></span><br><span class="line">        <span class="comment">// 记录融合操作集合</span></span><br><span class="line">        <span class="function">DenseSet&lt;Operation*&gt; <span class="title">fused_set</span><span class="params">(pattern.begin(), pattern.end())</span></span>;</span><br><span class="line">        DenseSet&lt;Operation*&gt; consumers_set;  <span class="comment">// 已处理的消费者</span></span><br><span class="line">        SmallVector&lt;Operation*, <span class="number">4</span>&gt; consumers_vec; <span class="comment">// 待移动的消费者</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 确定原始代码范围：第一个融合操作到最后一个的迭代器范围</span></span><br><span class="line">        <span class="keyword">auto</span> first_iter = pattern.<span class="built_in">front</span>()-&gt;<span class="built_in">getIterator</span>();</span><br><span class="line">        <span class="keyword">auto</span> last_iter = pattern.<span class="built_in">back</span>()-&gt;<span class="built_in">getIterator</span>();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 扫描区间内的所有操作</span></span><br><span class="line">        <span class="keyword">for</span> (Operation&amp; cur_op : llvm::<span class="built_in">make_range</span>(first_iter, last_iter)) &#123;</span><br><span class="line">            <span class="keyword">if</span> (!fused_set.<span class="built_in">contains</span>(&amp;cur_op)) &#123; <span class="comment">// 非融合操作</span></span><br><span class="line">                <span class="comment">// 检查是否是消费者：操作数来自融合集或已标记的消费者</span></span><br><span class="line">                <span class="type">bool</span> is_consumer = llvm::<span class="built_in">any_of</span>(cur_op.<span class="built_in">getOperands</span>(), </span><br><span class="line">                    [&amp;](Value v) &#123;</span><br><span class="line">                        Operation* def_op = v.<span class="built_in">getDefiningOp</span>();</span><br><span class="line">                        <span class="keyword">return</span> fused_set.<span class="built_in">contains</span>(def_op) || consumers_set.<span class="built_in">contains</span>(def_op);</span><br><span class="line">                    &#125;);</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> (is_consumer) &#123;</span><br><span class="line">                    consumers_set.<span class="built_in">insert</span>(&amp;cur_op); <span class="comment">// 标记为已处理</span></span><br><span class="line">                    consumers_vec.<span class="built_in">push_back</span>(&amp;cur_op); <span class="comment">// 加入移动队列</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 逆序移动消费者到融合点之后（防止顺序移动导致迭代器失效）</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span>* op : llvm::<span class="built_in">reverse</span>(consumers_vec)) &#123;</span><br><span class="line">            op-&gt;<span class="built_in">moveAfter</span>(pattern.<span class="built_in">back</span>()); <span class="comment">// 重定位到融合操作末尾</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* 创建融合操作 */</span></span><br><span class="line">        FusionOp fusion = b.<span class="built_in">create</span>&lt;mhlo::FusionOp&gt;(fused_loc, output_types, inputs);</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 构建融合计算区域</span></span><br><span class="line">        Region&amp; region = fusion.<span class="built_in">fused_computation</span>();</span><br><span class="line">        region.<span class="built_in">push_back</span>(<span class="keyword">new</span> Block); <span class="comment">// 创建基本块</span></span><br><span class="line">        Block&amp; block = region.<span class="built_in">front</span>();</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 将原始操作移入融合区域</span></span><br><span class="line">        <span class="keyword">for</span> (Operation* op : pattern) &#123;</span><br><span class="line">            op-&gt;<span class="built_in">moveBefore</span>(&amp;block, block.<span class="built_in">end</span>()); <span class="comment">// 保持原有顺序</span></span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 在区域末尾插入ReturnOp</span></span><br><span class="line">        b.<span class="built_in">setInsertionPoint</span>(&amp;block, block.<span class="built_in">end</span>());</span><br><span class="line">        b.<span class="built_in">create</span>&lt;mhlo::ReturnOp&gt;(fused_loc, outputs);</span><br><span class="line"></span><br><span class="line">        <span class="comment">/* 结果替换阶段 */</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">auto</span> [output, fusion_result] : llvm::<span class="built_in">zip</span>(outputs, fusion.<span class="built_in">getResults</span>())) &#123;</span><br><span class="line">            <span class="comment">// 替换所有外部使用点</span></span><br><span class="line">            <span class="keyword">for</span> (OpOperand&amp; use : llvm::<span class="built_in">make_early_inc_range</span>(output.<span class="built_in">getUses</span>())) &#123;</span><br><span class="line">                <span class="keyword">if</span> (use.<span class="built_in">getOwner</span>()-&gt;<span class="built_in">getBlock</span>() != &amp;block) &#123; <span class="comment">// 外部使用</span></span><br><span class="line">                    use.<span class="built_in">set</span>(fusion_result); <span class="comment">// 替换为融合结果</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如下是<code>ApplyFusionPlan</code>的流程图</p>
<p><img src="/images/4.png" alt="4"></p>
<p>其中比较核心的是识别消费者操作，该consumer要求是直接或间接依赖fusion里的operation，但本身并不是fusion operation，并且其location在fusionOp的范围内，<strong>因此需要显示地移动</strong>。</p>
<p><img src="/images/4-1743436524621-10.png" alt="4"></p>
<h2 id="BladeDISC-source-code分析"><a href="#BladeDISC-source-code分析" class="headerlink" title="BladeDISC source code分析"></a><font color = brown>BladeDISC source code分析</font></h2><p><code>BladeDISC</code>的kernel 融合主要参考<code>AStitch</code>和<a href="http://arxiv.org/abs/2009.10924">titch fusion</a>。</p>
<img src="/images/image-20250326111430445.png" alt="4" style="zoom:67%;" />

<p>上述很好地阐述了应用XLA和fusion stitich技术的差异。<font color = red>XLA编译器无法对middle reduce操作做fusion</font>，fusion stitch划分四类memory intensive op融合方式，起到有效扩展作用：</p>
<img src="/images/image-20250326114004653.png" style="zoom:67%;" />

<h3 id="Source-code解读"><a href="#Source-code解读" class="headerlink" title="Source code解读"></a><font color = green>Source code解读</font></h3><p>一个总的框架：</p>
<img src="/images/image-20250327221745561.png" style="zoom:67%;" />

<p>上述详情参考<a href="http://arxiv.org/abs/2009.10924">BladeDISC slide</a>。</p>
<h4 id="节点类型划分"><a href="#节点类型划分" class="headerlink" title="节点类型划分"></a>节点类型划分</h4><p>stich的fusion pattern中，相比xla，其重点是将node划分为不同的类：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Represents a list of lmhlo ops that are going to be fused.</span></span><br><span class="line"><span class="comment">// Concepts for a fusion pattern:</span></span><br><span class="line"><span class="comment">//   - Root op: the op whose output is the fusion-pattern&#x27;s output.</span></span><br><span class="line"><span class="comment">//     Sub-root op可以理解为stich fusion的缝合点，为用shared-memory缝合的op bound</span></span><br><span class="line"><span class="comment">//   - Sub-root op: the op whose output is to be maintained on shared-memory for</span></span><br><span class="line"><span class="comment">//     kStitch fusion. Currently, we only support row-reduction to be a sub-root</span></span><br><span class="line"><span class="comment">//     op.</span></span><br><span class="line"><span class="comment">//   - Regular xroot op: either a root op or a sub-root op, for whose operands</span></span><br><span class="line"><span class="comment">//     we successfully build tile information during kStitch fusion-pattern init</span></span><br><span class="line"><span class="comment">//     phase.</span></span><br><span class="line"><span class="comment">//   - Irregular xroot op: an root op for whose operands we fail to build tile</span></span><br><span class="line"><span class="comment">//     information durint kStitch fusion-pattern init phase.</span></span><br><span class="line"><span class="comment">//   - Skeleton op: the op who will be used to build the loop skeleton when</span></span><br><span class="line"><span class="comment">//     lowering a kStitch fusion to parallel loops. Currently, sub-root ops, and</span></span><br><span class="line"><span class="comment">//     regular xroot ops who generate external only results, are skeleton ops.</span></span><br><span class="line"><span class="comment">//     Other xroot ops are lowered with input-inline fusion phase.</span></span><br><span class="line"><span class="comment">//   Note: for an regular xroot op which is not an skeleton op, the output data</span></span><br><span class="line"><span class="comment">//     to be written should be coverred by its corresponding skeleton op.</span></span><br><span class="line"><span class="comment">//     Otherwise, this xroot are regared as irregular.</span></span><br></pre></td></tr></table></figure>

<p>上述注释中详细解读了node的分类：</p>
<ul>
<li>Root op：<code>fusion-pattern</code>的output node，即fusion的边界。</li>
<li>Sub-root op：通过shared-memory fuse的op。</li>
<li>xroot op：在astich论文中，每个op的thread感知分配信息，都是通过分析sub-root或是root，然后反向传播到整个fusion区域。xroot op是分析出thread信息的root或sub-root</li>
<li>Irregular xroot：没有分析出thread信息的sub root或是root op。</li>
<li>Skeleton op：负责构建<strong>动态shape的并行循环骨架</strong>，是GPU kernel代码生成的模板基础。通过选择具有典型计算特征的子根操作（如行归约）作为骨架，能够自动推导出循环维度、分块策略等关键参数。主要负责<strong>codegen</strong>部分。</li>
</ul>
<h4 id="Shape-analysis"><a href="#Shape-analysis" class="headerlink" title="Shape analysis"></a>Shape analysis</h4><h4 id="GPU-Stitch策略"><a href="#GPU-Stitch策略" class="headerlink" title="GPU Stitch策略"></a>GPU Stitch策略</h4><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 重点关注如何将stitch技术用在gpu上</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StitchGpuFusionStrategy</span> : <span class="keyword">public</span> FusionStrategy &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">StitchGpuFusionStrategy</span>(<span class="type">const</span> FusionOptions&amp; options)</span><br><span class="line">      : <span class="built_in">FusionStrategy</span>(options) &#123;&#125;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="type">bool</span> <span class="title">isFusible</span><span class="params">(Operation* op)</span> <span class="keyword">override</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="type">bool</span> <span class="title">tryFuse</span><span class="params">(ShapeAnalysis&amp; shapeAnalysis, FusionPattern&amp; lhs,</span></span></span><br><span class="line"><span class="params"><span class="function">                       FusionPattern&amp; rhs, FusionPattern&amp; target)</span> <span class="keyword">override</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> <span class="type">bool</span> <span class="title">initFusionPattern</span><span class="params">(ShapeAnalysis&amp; shapeAnalysis,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 FusionPattern&amp; fusion_pattern)</span> <span class="keyword">override</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> StringRef <span class="title">getName</span><span class="params">()</span> <span class="keyword">override</span> </span>&#123; <span class="keyword">return</span> <span class="string">&quot;StitchGpuFusionStrategy&quot;</span>; &#125;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="function"><span class="keyword">virtual</span> Value <span class="title">getEffectiveShape</span><span class="params">(FusionPattern&amp; target, Value value)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">bool</span> <span class="title">tileCoverInfoPropagateO2I</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      ShapeAnalysis&amp; shapeAnalysis, DenseMap&lt;Value, TileInfo&gt;&amp; tile_plan,</span></span></span><br><span class="line"><span class="params"><span class="function">      Operation* op, SmallVector&lt;std::pair&lt;Value, TileInfo&gt;, <span class="number">4</span>&gt;&amp; in_info,</span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="type">bool</span>&amp; cover)</span></span>;</span><br><span class="line">  <span class="function"><span class="type">bool</span> <span class="title">findFusionPatternTypeAndSubroot</span><span class="params">(ShapeAnalysis&amp; shapeAnalysis,</span></span></span><br><span class="line"><span class="params"><span class="function">                                       FusionPattern&amp; fusion_pattern)</span></span>;</span><br><span class="line">  <span class="function"><span class="type">bool</span> <span class="title">tileXroots</span><span class="params">(ShapeAnalysis&amp; shapeAnalysis, FusionPattern&amp; fusion_pattern)</span></span>;</span><br><span class="line">  <span class="function"><span class="type">bool</span> <span class="title">backtraceTileAndCover</span><span class="params">(ShapeAnalysis&amp; shapeAnalysis,</span></span></span><br><span class="line"><span class="params"><span class="function">                             FusionPattern&amp; fusion_pattern, Value value)</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>整体的流程框架如下：</p>
<p><img src="/images/6.png" alt="6"></p>
<p>上述流程图很好的表达了整个流程框架：</p>
<ul>
<li><p><code>initFusionPatter</code>负责初始化fusion的option信息</p>
</li>
<li><p><code>findFusionPatternTypeAndSubroot</code>负责引用fusion，找寻subroot等特殊节点，其整体逻辑如下：</p>
<p><img src="/images/7.png" alt="7"></p>
</li>
<li><p><code>tileXroots</code>针对sub root做线程分配算法</p>
<p><img src="/images/8.png" alt="8"></p>
</li>
<li><p><code>backtraceTileAndCover</code>在一个fusion中，从不同sub root出发，做反向thread分配推导</p>
<p><img src="/images/9.png" alt="9"></p>
</li>
</ul>
<p>对应论文中的图片：</p>
<p><img src="/images/image-20250327184033247.png" alt="image-20250327184033247"></p>
<p>上述source code和论文中的对应描述参考论文中的chapter4.3的steps。</p>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>机器学习编译</category>
        <category>访存优化</category>
      </categories>
      <tags>
        <tag>机器学习编译器</tag>
        <tag>mlir</tag>
        <tag>算子融合技术</tag>
      </tags>
  </entry>
  <entry>
    <title>初探minitorch</title>
    <url>/2025/04/27/%E5%88%9D%E6%8E%A2minitorch/</url>
    <content><![CDATA[<p><img src="/images/image-20250427110245462.png" alt="image-20250427110245462"></p>
<span id="more"></span>

<blockquote>
<p>本篇文章主要介绍minitorch项目的module0所需要的基础工具以及一些python知识。</p>
</blockquote>
<h2 id="Functional-Python"><a href="#Functional-Python" class="headerlink" title="Functional Python"></a><font color = brown>Functional Python</font></h2><h3 id="为什么用Functional-Python"><a href="#为什么用Functional-Python" class="headerlink" title="为什么用Functional Python"></a><font color = green>为什么用Functional Python</font></h3><p>重点参考<a href="https://minitorch.github.io/module0/functional/">Functional tutorial</a>文档。首先我们需要弄清楚为什么使用Functional python。MiniTorch 对外的接口（API）模仿 PyTorch 的标准用法，降低用户学习成本。而在内部，库的核心逻辑采用<strong>函数式编程</strong>风格，即通过纯函数（无副作用）和组合函数的方式构建计算逻辑。关于函数式编程，可以参考<a href="https://liaoxuefeng.com/books/python/functional/index.html">廖雪峰python博客</a>（后续会专门开一个博客内容学习讲解函数式编程，函数是编程是一个很重要且复杂的topic）。函数式编程有如下两个优点：</p>
<ol>
<li>更易测试</li>
<li>更易优化</li>
</ol>
<p>函数式编程允许如下两件事情：</p>
<ol>
<li>将一个函数作为参数传入另一个函数</li>
<li>将一个函数作为返回值返回</li>
</ol>
<h3 id="一个简单的例子"><a href="#一个简单的例子" class="headerlink" title="一个简单的例子"></a><font color = green>一个简单的例子</font></h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 支持type hints</span></span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Callable</span>, Iterable</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个函数，返回一个函数，该函数将传入的filter function参数应用到参数数组上</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_filter</span>(<span class="params"><span class="built_in">filter</span>: <span class="type">Callable</span>[[<span class="built_in">float</span>], <span class="built_in">bool</span>]</span>) -&gt; <span class="type">Callable</span>[[Iterable[<span class="built_in">float</span>]], Iterable[<span class="built_in">float</span>]]:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">func</span>(<span class="params">ls: Iterable[<span class="built_in">float</span>]</span>):</span><br><span class="line">        res = []</span><br><span class="line">        <span class="keyword">for</span> a <span class="keyword">in</span> ls:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">filter</span>(a):</span><br><span class="line">                res.append(a)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    <span class="keyword">return</span> func</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">more_than_4</span>(<span class="params">a: <span class="built_in">float</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">    <span class="keyword">return</span> a &gt; <span class="number">4</span></span><br><span class="line"></span><br><span class="line">array = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line">filter_fn = apply_filter(more_than_4) </span><br><span class="line"><span class="built_in">print</span>(filter_fn(array))</span><br></pre></td></tr></table></figure>

<p>通过上述例子，来辅助我们的理解。上述例子需要完成一个任务：从一个数组中，挑选出数值大于4的数字并重新组合返回。如果使用传统写法，则针对不同的筛选条件，都要遍历并筛选，之间会有大量逻辑重复。通过使用函数式编程范式，将filter封装成一个函数并传参，可减少重复代码，且有更强的可扩展性。</p>
<h2 id="MiniTorch项目的辅助工具"><a href="#MiniTorch项目的辅助工具" class="headerlink" title="MiniTorch项目的辅助工具"></a><font color = brown>MiniTorch项目的辅助工具</font></h2><p>MiniTorch项目学习起来一个最大的优点，就是其对于测试，coding等规范性均做了详细的要求和工具辅助。<code>black</code>和<code>flake</code>等代码规范性检测工具，<code>PyTest</code>测试框架以及<code>github CI</code>版本管理和测试流程，以上均是本章讲解的重点。</p>
<h3 id="PyTest框架"><a href="#PyTest框架" class="headerlink" title="PyTest框架"></a><font color = green>PyTest框架</font></h3><h2 id="References"><a href="#References" class="headerlink" title="References"></a><font color = brown>References</font></h2><ol>
<li><a href="https://minitorch.github.io/">MiniTorch官网</a></li>
<li><a href="https://dezeming.top/wp-content/uploads/2022/02/MiniTorch-%E5%AD%A6%E4%B9%A0%E5%85%A8%E6%94%BB%E7%95%A5.pdf">MiniTorch学习参考攻略</a></li>
</ol>
]]></content>
      <categories>
        <category>pytorch</category>
        <category>机器学习系统</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习系统</tag>
      </tags>
  </entry>
  <entry>
    <title>Transform Dialect tutorial1</title>
    <url>/2025/04/08/Transform-Dialect-tutorial1/</url>
    <content><![CDATA[<p><img src="/images/image-20250408184457571.png" alt="image-20250408184457571"></p>
<span id="more"></span>

<blockquote>
<p>Transform dialect是mlir基础设施实现的调度dsl。通过在同一个mlir文件中，使用标准ir定义计算任务（payload ir），利用transform ir定义调度方式（schedule ir），最后借助于transform-interpreter注册和使用整个pass。</p>
</blockquote>
<h2 id="Motivation-of-Transform-Dialect"><a href="#Motivation-of-Transform-Dialect" class="headerlink" title="Motivation of Transform Dialect"></a><font color = brown>Motivation of Transform Dialect</font></h2><p>总结许多成熟编译器，比如Halide，TVM，TC等，可以发现如下规律：</p>
<ol>
<li><strong>调度表示</strong>（Schedule Representation）：以结构化数据描述优化流程的元信息集合</li>
<li><strong>声明式规范</strong>（Declarative Specification）：通过定义预期目标状态而非具体操作步骤进行配置</li>
<li><strong>多版本化</strong>（Multi-Versioning）：针对不同硬件&#x2F;场景生成多个优化方案分支</li>
<li><strong>运行时调度</strong>（Runtime Dispatch）：通过动态决策机制选择最优版本执行</li>
<li><strong>垂直时序控制</strong>（Vertical Sequencing）：在单一功能域内进行深度优化组合</li>
</ol>
<p>MLIR基础设施中，也希望提供用户类似TVM的计算调度分离的能力，从而方便用户对于一个计算任务自定义调度优化策略。在MLIR中，一切都是<code>op</code>操作，调度操作也是<code>op</code>，封装在transform这个dialect中，这便是transform dialect的由来。</p>
<blockquote>
<p>后续教程主要来源于<a href="https://mlir.llvm.org/docs/Tutorials/transform/">transform tutorial</a></p>
</blockquote>
<h2 id="Chapter1：利用transform-op组建pipeline"><a href="#Chapter1：利用transform-op组建pipeline" class="headerlink" title="Chapter1：利用transform op组建pipeline"></a><font color = brown>Chapter1：利用transform op组建pipeline</font></h2><p>transform dialect内置丰富的schedule op，计算调度分离机制使得用户需要定义payload ir以及schedule ir。在transform的官方tutorial中给的例子，是将一个矩阵乘-逐项加-relu操作，利用transform op做调度优化。</p>
<ul>
<li><p>payload ir（详细定义计算任务）：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Original function to optimize.</span></span><br><span class="line">func.func @<span class="built_in">fc_relu</span>(%lhs: tensor&lt;<span class="number">512</span>x512xf32&gt;, %rhs: tensor&lt;<span class="number">512</span>x512xf32&gt;,</span><br><span class="line">                   %bias: tensor&lt;<span class="number">512</span>x512xf32&gt;, %output: tensor&lt;<span class="number">512</span>x512xf32&gt;)</span><br><span class="line">                   -&gt; tensor&lt;<span class="number">512</span>x512xf32&gt; &#123;</span><br><span class="line">  <span class="comment">// Matrix-matrix multiplication.</span></span><br><span class="line">  %matmul = linalg.matmul <span class="built_in">ins</span>(%lhs, %rhs: tensor&lt;<span class="number">512</span>x512xf32&gt;, tensor&lt;<span class="number">512</span>x512xf32&gt;)</span><br><span class="line">                          <span class="built_in">outs</span>(%output: tensor&lt;<span class="number">512</span>x512xf32&gt;) -&gt; tensor&lt;<span class="number">512</span>x512xf32&gt;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Elementwise addition.</span></span><br><span class="line">  %biased = linalg.elemwise_binary &#123; fun = <span class="meta">#linalg.binary_fn<span class="string">&lt;add&gt;</span> &#125;</span></span><br><span class="line">    <span class="built_in">ins</span>(%matmul, %bias : tensor&lt;<span class="number">512</span>x512xf32&gt;, tensor&lt;<span class="number">512</span>x512xf32&gt;)</span><br><span class="line">    <span class="built_in">outs</span>(%output : tensor&lt;<span class="number">512</span>x512xf32&gt;) -&gt; tensor&lt;<span class="number">512</span>x512xf32&gt;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Elementwise max with 0 (ReLU).</span></span><br><span class="line">  %c0f = arith.constant <span class="number">0.0</span> : f32</span><br><span class="line">  %relued = linalg.elemwise_binary &#123; fun = <span class="meta">#linalg.binary_fn<span class="string">&lt;max_signed&gt;</span> &#125;</span></span><br><span class="line">    <span class="built_in">ins</span>(%biased, %c0f : tensor&lt;<span class="number">512</span>x512xf32&gt;, f32)</span><br><span class="line">    <span class="built_in">outs</span>(%output : tensor&lt;<span class="number">512</span>x512xf32&gt;) -&gt; tensor&lt;<span class="number">512</span>x512xf32&gt;</span><br><span class="line">  func.<span class="keyword">return</span> %relued : tensor&lt;<span class="number">512</span>x512xf32&gt;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>payload ir的定义就使用linalg等标准mlir ir定义即可。</p>
</li>
<li><p>schedule ir（调度原语）</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">module</span> attributes &#123;transform.with_named_sequence&#125; &#123;  </span><br><span class="line">  transform.named_sequence @__transform_main(</span><br><span class="line">      %arg0: !transform.any_op,</span><br><span class="line">      %arg1: !transform.op&lt;<span class="string">&quot;linalg.matmul&quot;</span>&gt;,</span><br><span class="line">      %arg2: !transform.op&lt;<span class="string">&quot;linalg.elemwise_binary&quot;</span>&gt;) &#123;</span><br><span class="line">    <span class="comment">// Since the %arg2 handle is associated with both elementwise operations,</span></span><br><span class="line">    <span class="comment">// we need to split it into two handles so we can target only the second</span></span><br><span class="line">    <span class="comment">// elementwise operation.</span></span><br><span class="line">    %add, %max = transform.split_handle %arg2 : (!transform.op&lt;<span class="string">&quot;linalg.elemwise_binary&quot;</span>&gt;)</span><br><span class="line">        -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// The actual tiling transformation takes tile sizes as attributes. It produces a</span></span><br><span class="line">    <span class="comment">// handle to the loop generated during tiling.</span></span><br><span class="line">    %tiled, %loop = transform.structured.tile_using_forall %max tile_sizes [<span class="number">8</span>, <span class="number">32</span>]</span><br><span class="line">        : (!transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// We can now fuse the other operations into the loop. Here, we fuse</span></span><br><span class="line">    <span class="comment">// operations one-by-one. This requires the operation that is being fused</span></span><br><span class="line">    <span class="comment">// to define the value used within the loop, so the order of such fusions</span></span><br><span class="line">    <span class="comment">// is important. We could also use &quot;transform.merge_handles&quot; to obtain</span></span><br><span class="line">    <span class="comment">// a single handle to all operations and give it to `fuse_into_containing_op`</span></span><br><span class="line">    <span class="comment">// that would take care of the ordering in this case</span></span><br><span class="line">    %add_fused, %loop2 = transform.structured.fuse_into_containing_op %add into %loop</span><br><span class="line">        : (!transform.any_op, !transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">    %matmul_fused, %loop3 = transform.structured.fuse_into_containing_op %arg1 into %loop2</span><br><span class="line">        : (!transform.op&lt;<span class="string">&quot;linalg.matmul&quot;</span>&gt;, !transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Tile again to get the desired size. Note that this time this tiles the</span></span><br><span class="line">    <span class="comment">// &quot;add&quot; operation and fuses matmul into the loop, but doesn&#x27;t affect the</span></span><br><span class="line">    <span class="comment">// &quot;max&quot; operation. This illustrates the precise targeting with the transform</span></span><br><span class="line">    <span class="comment">// dialect. Otherwise, it is difficult to differentiate &quot;add&quot; and &quot;max&quot;, both</span></span><br><span class="line">    <span class="comment">// of which having the same kind.</span></span><br><span class="line">    %tiled_second, %loop_second = transform.structured.tile_using_forall %add_fused tile_sizes [<span class="number">4</span>, <span class="number">4</span>]</span><br><span class="line">        : (!transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">    %matmul_fused_2, %loop_second_2 =</span><br><span class="line">        transform.structured.fuse_into_containing_op %matmul_fused into %loop_second</span><br><span class="line">        : (!transform.any_op, !transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// // Since outlining is currently only implemented for region-holding operations</span></span><br><span class="line">    <span class="comment">// // such as loops, use tiling to size 1 to materialize the outer loop that is</span></span><br><span class="line">    <span class="comment">// // going to be outlined.</span></span><br><span class="line">    <span class="comment">// %_0, %loop_third = transform.structured.tile_using_forall %tiled_second tile_sizes [1]</span></span><br><span class="line">    <span class="comment">//     : (!transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span></span><br><span class="line">    <span class="comment">// %_1, %outline_target = transform.structured.fuse_into_containing_op %matmul_fused_2 into %loop_third</span></span><br><span class="line">    <span class="comment">//     : (!transform.any_op, !transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span></span><br><span class="line">    <span class="comment">// %func, %call = transform.loop.outline %outline_target &#123;func_name = &quot;outlined&quot;&#125;</span></span><br><span class="line">    <span class="comment">//     : (!transform.any_op) -&gt; (!transform.any_op, !transform.op&lt;&quot;func.call&quot;&gt;)</span></span><br><span class="line">  </span><br><span class="line">    transform.yield</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>完整代码片段如下。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// RUN: mlir-opt %s \</span></span><br><span class="line"><span class="comment">// RUN:   --pass-pipeline=&quot;builtin.module(transform-interpreter&#123; \</span></span><br><span class="line"><span class="comment">// RUN:        debug-bind-trailing-args=linalg.matmul,linalg.elemwise_binary&#125;,\</span></span><br><span class="line"><span class="comment">// RUN:        canonicalize,cse,symbol-dce)&quot; |\</span></span><br><span class="line"><span class="comment">// RUN: FileCheck %s</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// ****************************** IMPORTANT NOTE ******************************</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// If you are changing this file, you may also need to change</span></span><br><span class="line"><span class="comment">// mlir/docs/Tutorials/Transform accordingly.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// ****************************************************************************</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Original function to optimize.</span></span><br><span class="line">func.func @<span class="built_in">fc_relu</span>(%lhs: tensor&lt;<span class="number">512</span>x512xf32&gt;, %rhs: tensor&lt;<span class="number">512</span>x512xf32&gt;,</span><br><span class="line">                   %bias: tensor&lt;<span class="number">512</span>x512xf32&gt;, %output: tensor&lt;<span class="number">512</span>x512xf32&gt;)</span><br><span class="line">                   -&gt; tensor&lt;<span class="number">512</span>x512xf32&gt; &#123;</span><br><span class="line">  <span class="comment">// Matrix-matrix multiplication.</span></span><br><span class="line">  %matmul = linalg.matmul <span class="built_in">ins</span>(%lhs, %rhs: tensor&lt;<span class="number">512</span>x512xf32&gt;, tensor&lt;<span class="number">512</span>x512xf32&gt;)</span><br><span class="line">                          <span class="built_in">outs</span>(%output: tensor&lt;<span class="number">512</span>x512xf32&gt;) -&gt; tensor&lt;<span class="number">512</span>x512xf32&gt;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Elementwise addition.</span></span><br><span class="line">  %biased = linalg.elemwise_binary &#123; fun = <span class="meta">#linalg.binary_fn<span class="string">&lt;add&gt;</span> &#125;</span></span><br><span class="line">    <span class="built_in">ins</span>(%matmul, %bias : tensor&lt;<span class="number">512</span>x512xf32&gt;, tensor&lt;<span class="number">512</span>x512xf32&gt;)</span><br><span class="line">    <span class="built_in">outs</span>(%output : tensor&lt;<span class="number">512</span>x512xf32&gt;) -&gt; tensor&lt;<span class="number">512</span>x512xf32&gt;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Elementwise max with 0 (ReLU).</span></span><br><span class="line">  %c0f = arith.constant <span class="number">0.0</span> : f32</span><br><span class="line">  %relued = linalg.elemwise_binary &#123; fun = <span class="meta">#linalg.binary_fn<span class="string">&lt;max_signed&gt;</span> &#125;</span></span><br><span class="line">    <span class="built_in">ins</span>(%biased, %c0f : tensor&lt;<span class="number">512</span>x512xf32&gt;, f32)</span><br><span class="line">    <span class="built_in">outs</span>(%output : tensor&lt;<span class="number">512</span>x512xf32&gt;) -&gt; tensor&lt;<span class="number">512</span>x512xf32&gt;</span><br><span class="line">  func.<span class="keyword">return</span> %relued : tensor&lt;<span class="number">512</span>x512xf32&gt;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// CHECK: func @outlined</span></span><br><span class="line"><span class="comment">// CHECK:   linalg.matmul</span></span><br><span class="line"><span class="comment">// CHECK:   linalg.elemwise_binary &#123;fun = #linalg.binary_fn&lt;add&gt;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// CHECK-LABEL: func @fc_relu</span></span><br><span class="line"><span class="comment">// CHECK: scf.forall</span></span><br><span class="line"><span class="comment">// CHECK:   scf.forall</span></span><br><span class="line"><span class="comment">// CHECK:     %[[SLICE4:.+]] = tensor.extract_slice</span></span><br><span class="line"><span class="comment">// CHECK:     %[[SLICE5:.+]] = tensor.extract_slice</span></span><br><span class="line"><span class="comment">// CHECK:     %[[SLICE6:.+]] = tensor.extract_slice</span></span><br><span class="line"><span class="comment">// CHECK:     %[[SLICE7:.+]] = tensor.extract_slice</span></span><br><span class="line"><span class="comment">// CHECK:     %[[SLICE8:.+]] = tensor.extract_slice</span></span><br><span class="line"><span class="comment">// CHECK:     func.call @outlined(%[[SLICE4]], %[[SLICE5]], %[[SLICE6]], %[[SLICE7]], %[[SLICE8]])</span></span><br><span class="line"><span class="comment">// CHECK-NOT: linalg.matmul</span></span><br><span class="line"><span class="comment">// CHECK-NOT: linalg.elemwise_binary</span></span><br><span class="line"><span class="comment">// CHECK:     scf.forall.in_parallel</span></span><br><span class="line"><span class="comment">// CHECK:   linalg.elemwise_binary &#123;fun = #linalg.binary_fn&lt;max_signed&gt;&#125;</span></span><br><span class="line"><span class="comment">// CHECK:   scf.forall.in_parallel</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Declaration of the &quot;microkernel&quot; function that we will be targeting.</span></span><br><span class="line">func.func <span class="keyword">private</span> @<span class="built_in">microkernel</span>(</span><br><span class="line">    %lhs: tensor&lt;<span class="number">4</span>x512xf32&gt;,</span><br><span class="line">    %rhs: tensor&lt;<span class="number">512</span>x4xf32&gt;,</span><br><span class="line">    %bias: tensor&lt;<span class="number">4</span>x4xf32&gt;,</span><br><span class="line">    %init: tensor&lt;<span class="number">4</span>x4xf32&gt;,</span><br><span class="line">    %output: tensor&lt;<span class="number">4</span>x4xf32&gt;) -&gt; tensor&lt;<span class="number">4</span>x4xf32&gt;</span><br><span class="line"></span><br><span class="line"><span class="keyword">module</span> attributes &#123;transform.with_named_sequence&#125; &#123;  </span><br><span class="line">  transform.named_sequence @__transform_main(</span><br><span class="line">      %arg0: !transform.any_op,</span><br><span class="line">      %arg1: !transform.op&lt;<span class="string">&quot;linalg.matmul&quot;</span>&gt;,</span><br><span class="line">      %arg2: !transform.op&lt;<span class="string">&quot;linalg.elemwise_binary&quot;</span>&gt;) &#123;</span><br><span class="line">    <span class="comment">// Since the %arg2 handle is associated with both elementwise operations,</span></span><br><span class="line">    <span class="comment">// we need to split it into two handles so we can target only the second</span></span><br><span class="line">    <span class="comment">// elementwise operation.</span></span><br><span class="line">    %add, %max = transform.split_handle %arg2 : (!transform.op&lt;<span class="string">&quot;linalg.elemwise_binary&quot;</span>&gt;)</span><br><span class="line">        -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// The actual tiling transformation takes tile sizes as attributes. It produces a</span></span><br><span class="line">    <span class="comment">// handle to the loop generated during tiling.</span></span><br><span class="line">    %tiled, %loop = transform.structured.tile_using_forall %max tile_sizes [<span class="number">8</span>, <span class="number">32</span>]</span><br><span class="line">        : (!transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// We can now fuse the other operations into the loop. Here, we fuse</span></span><br><span class="line">    <span class="comment">// operations one-by-one. This requires the operation that is being fused</span></span><br><span class="line">    <span class="comment">// to define the value used within the loop, so the order of such fusions</span></span><br><span class="line">    <span class="comment">// is important. We could also use &quot;transform.merge_handles&quot; to obtain</span></span><br><span class="line">    <span class="comment">// a single handle to all operations and give it to `fuse_into_containing_op`</span></span><br><span class="line">    <span class="comment">// that would take care of the ordering in this case</span></span><br><span class="line">    %add_fused, %loop2 = transform.structured.fuse_into_containing_op %add into %loop</span><br><span class="line">        : (!transform.any_op, !transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">    %matmul_fused, %loop3 = transform.structured.fuse_into_containing_op %arg1 into %loop2</span><br><span class="line">        : (!transform.op&lt;<span class="string">&quot;linalg.matmul&quot;</span>&gt;, !transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// Tile again to get the desired size. Note that this time this tiles the</span></span><br><span class="line">    <span class="comment">// &quot;add&quot; operation and fuses matmul into the loop, but doesn&#x27;t affect the</span></span><br><span class="line">    <span class="comment">// &quot;max&quot; operation. This illustrates the precise targeting with the transform</span></span><br><span class="line">    <span class="comment">// dialect. Otherwise, it is difficult to differentiate &quot;add&quot; and &quot;max&quot;, both</span></span><br><span class="line">    <span class="comment">// of which having the same kind.</span></span><br><span class="line">    %tiled_second, %loop_second = transform.structured.tile_using_forall %add_fused tile_sizes [<span class="number">4</span>, <span class="number">4</span>]</span><br><span class="line">        : (!transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">    %matmul_fused_2, %loop_second_2 =</span><br><span class="line">        transform.structured.fuse_into_containing_op %matmul_fused into %loop_second</span><br><span class="line">        : (!transform.any_op, !transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// // Since outlining is currently only implemented for region-holding operations</span></span><br><span class="line">    <span class="comment">// // such as loops, use tiling to size 1 to materialize the outer loop that is</span></span><br><span class="line">    <span class="comment">// // going to be outlined.</span></span><br><span class="line">    <span class="comment">// %_0, %loop_third = transform.structured.tile_using_forall %tiled_second tile_sizes [1]</span></span><br><span class="line">    <span class="comment">//     : (!transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span></span><br><span class="line">    <span class="comment">// %_1, %outline_target = transform.structured.fuse_into_containing_op %matmul_fused_2 into %loop_third</span></span><br><span class="line">    <span class="comment">//     : (!transform.any_op, !transform.any_op) -&gt; (!transform.any_op, !transform.any_op)</span></span><br><span class="line">    <span class="comment">// %func, %call = transform.loop.outline %outline_target &#123;func_name = &quot;outlined&quot;&#125;</span></span><br><span class="line">    <span class="comment">//     : (!transform.any_op) -&gt; (!transform.any_op, !transform.op&lt;&quot;func.call&quot;&gt;)</span></span><br><span class="line">  </span><br><span class="line">    transform.yield</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Chapter2：自定义一个简单的transform-operation"><a href="#Chapter2：自定义一个简单的transform-operation" class="headerlink" title="Chapter2：自定义一个简单的transform operation"></a><font color = brown>Chapter2：自定义一个简单的transform operation</font></h2><h3 id="目的"><a href="#目的" class="headerlink" title="目的"></a><font color = green>目的</font></h3><p>很多时候，transform dialect中的operation无法满足我们调度优化需求。比如在chapter1中，我们最后tiling的小块可以替换成手写的算子microkernel，这个替换操作在transform中没有直接的op。可以实现自定义operation（<code>transform.my.change_call_target</code>），使用方法如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Rewrite the call target.</span></span><br><span class="line">transform.my.change_call_target %call, <span class="string">&quot;microkernel&quot;</span> : !transform.any_op</span><br></pre></td></tr></table></figure>

<p>即将%call这个handle（一个operation）转变成microkernel调用。</p>
<blockquote>
<p>在实际编译开发中，microkernel相当于微内核算子，一般为手写，编译器可以自动发现可以替换的op做替换，以进一步提高性能。</p>
</blockquote>
<h3 id="项目结构"><a href="#项目结构" class="headerlink" title="项目结构"></a><font color = green>项目结构</font></h3><p>transform extension的写法，和mlir的一个standalone项目的结构是类似的。项目结构可以参考<a href="https://github.com/llvm/llvm-project/tree/main/mlir/examples/standalone">mlir standalone项目框架</a>。</p>
<p>如下是chapter2的项目结构：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">├── build.sh</span><br><span class="line">├── CMakeLists.txt</span><br><span class="line">├── include</span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   ├── MyExtension.h</span><br><span class="line">│   └── MyExtension.td</span><br><span class="line">├── lib</span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   └── MyExtension.cpp</span><br><span class="line">├── test</span><br><span class="line">│   ├── invalid.mlir</span><br><span class="line">│   ├── sequence.mlir</span><br><span class="line">│   └── test.sh</span><br><span class="line">└── transform-opt</span><br><span class="line">    ├── CMakeLists.txt</span><br><span class="line">    └── transform-opt.cpp</span><br></pre></td></tr></table></figure>

<p>为transform dialect定义一个operation，主要需要思考如下几个点：</p>
<ul>
<li>利用ods系统定义一个operation，自动生成.h和.cpp文件。</li>
<li>该operation需要重载几个interface。<ul>
<li>TransformOpInterface是transform dialect的op必须实现的interface，主要实现<code>apply</code>方法。</li>
<li>MemoryEffectsOpInterface是内存side effect定义interface，需要实现<code>getEffects</code>方法。</li>
</ul>
</li>
<li>注册operation。</li>
</ul>
<h3 id="代码框架讲解"><a href="#代码框架讲解" class="headerlink" title="代码框架讲解"></a><font color = green>代码框架讲解</font></h3><p>后续按照这个顺序来罗列代码，代码注释十分全面了。</p>
<h4 id="定义operation"><a href="#定义operation" class="headerlink" title="定义operation"></a>定义operation</h4><p><code>/include/MyExtension.td</code></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//===-- MyExtension.td - Transform dialect tutorial --------*- tablegen -*-===//</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.</span></span><br><span class="line"><span class="comment">// See https://llvm.org/LICENSE.txt for license information.</span></span><br><span class="line"><span class="comment">// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//===----------------------------------------------------------------------===//</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// This file defines Transform dialect extension operations used in the</span></span><br><span class="line"><span class="comment">// Chapter 2 of the Transform dialect tutorial.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//===----------------------------------------------------------------------===//</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifndef</span> MY_EXTENSION</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MY_EXTENSION</span></span><br><span class="line"></span><br><span class="line">include <span class="string">&quot;mlir/Dialect/Transform/IR/TransformDialect.td&quot;</span></span><br><span class="line">include <span class="string">&quot;mlir/Dialect/Transform/Interfaces/TransformInterfaces.td&quot;</span></span><br><span class="line">include <span class="string">&quot;mlir/IR/OpBase.td&quot;</span></span><br><span class="line">include <span class="string">&quot;mlir/Interfaces/SideEffectInterfaces.td&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Define the new operation. By convention, prefix its name with the name of the dialect </span></span><br><span class="line"><span class="comment">// extension, &quot;my.&quot;. The full operation name will be further prefixed with &quot;transform.&quot;.</span></span><br><span class="line">def ChangeCallTargetOp : Op&lt;Transform_Dialect, <span class="string">&quot;my.change_call_target&quot;</span>,</span><br><span class="line">    <span class="comment">// Indicate that the operation implements the required TransformOpInterface and</span></span><br><span class="line">    <span class="comment">// MemoryEffectsOpInterface.</span></span><br><span class="line">    [DeclareOpInterfaceMethods&lt;TransformOpInterface&gt;,</span><br><span class="line">     DeclareOpInterfaceMethods&lt;MemoryEffectsOpInterface&gt;]&gt; &#123;</span><br><span class="line">  <span class="comment">// Provide a brief and a full description. It is recommended that the latter describes </span></span><br><span class="line">  <span class="comment">// the effects on the operands and how the operation processes various failure modes.</span></span><br><span class="line">  let summary = <span class="string">&quot;Changes the callee of a call operation to the specified one&quot;</span>;</span><br><span class="line">  let description = [&#123;</span><br><span class="line">    For each `func.call` payload operation associated with the handle, changes its </span><br><span class="line">    callee to be the symbol whose name is provided as an attribute to <span class="keyword">this</span> operation.</span><br><span class="line"></span><br><span class="line">    Generates a silenceable failure <span class="keyword">if</span> the operand is associated with payload operations </span><br><span class="line">    that are <span class="keyword">not</span> `func.call`.</span><br><span class="line">    Only reads the operand.</span><br><span class="line">  &#125;];</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The arguments include the handle to the payload operations and the attribute that </span></span><br><span class="line">  <span class="comment">// specifies the new callee. The handle must implement TransformHandleTypeInterface.   </span></span><br><span class="line">  <span class="comment">// We use a string attribute as the symbol may not exist in the transform IR so the </span></span><br><span class="line">  <span class="comment">// verification may fail. </span></span><br><span class="line">  let arguments = (ins</span><br><span class="line">    TransformHandleTypeInterface:$call,</span><br><span class="line">    StrAttr:$new_target);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The results are empty as the transformation does not produce any new payload.</span></span><br><span class="line">  let results = (outs);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Provide nice syntax.</span></span><br><span class="line">  let assemblyFormat = <span class="string">&quot;$call `,` $new_target attr-dict `:` type($call)&quot;</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">endif</span> <span class="comment">// MY_EXTENSION</span></span></span><br></pre></td></tr></table></figure>

<p>上述td定义和mlir op定义是一样的，需要注意如下几点：</p>
<ul>
<li><p>定义interface：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">[DeclareOpInterfaceMethods&lt;TransformOpInterface&gt;,</span><br><span class="line"> DeclareOpInterfaceMethods&lt;MemoryEffectsOpInterface&gt;]</span><br></pre></td></tr></table></figure>
</li>
<li><p>operation的定义：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// The arguments include the handle to the payload operations and the attribute that </span></span><br><span class="line"><span class="comment">// specifies the new callee. The handle must implement TransformHandleTypeInterface.   </span></span><br><span class="line"><span class="comment">// We use a string attribute as the symbol may not exist in the transform IR so the </span></span><br><span class="line"><span class="comment">// verification may fail. </span></span><br><span class="line">let arguments = (ins</span><br><span class="line">  TransformHandleTypeInterface:$call,</span><br><span class="line">  StrAttr:$new_target);</span><br></pre></td></tr></table></figure>

<p>重点是argument中需要传入一个handle，该handle必须实现TransformHandleTypeInterface，即这个handle可以通过transform op来操纵。</p>
</li>
</ul>
<p><code>MyExtension.h</code></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//===-- MyExtension.h - Transform dialect tutorial --------------*- c++ -*-===//</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.</span></span><br><span class="line"><span class="comment">// See https://llvm.org/LICENSE.txt for license information.</span></span><br><span class="line"><span class="comment">// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//===----------------------------------------------------------------------===//</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// This file defines Transform dialect extension operations used in the</span></span><br><span class="line"><span class="comment">// Chapter 2 of the Transform dialect tutorial.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//===----------------------------------------------------------------------===//</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/Bytecode/BytecodeOpInterface.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/Dialect/Transform/IR/TransformDialect.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/Dialect/Transform/Interfaces/TransformInterfaces.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> GET_OP_CLASSES</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;MyExtension.h.inc&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Registers our Transform dialect extension.</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">registerMyExtension</span><span class="params">(::mlir::DialectRegistry &amp;registry)</span></span>;</span><br></pre></td></tr></table></figure>

<p>这个头文件和mlir基础操作一样，利用宏定义获取.h.inc中的op定义，并定义一个extension的注册函数。</p>
<p>CmakeLists.txt如下：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Tell Tablegen to use MyExtension.td as input.</span></span><br><span class="line"><span class="keyword">set</span>(LLVM_TARGET_DEFINITIONS MyExtension.td)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ask Tablegen to generate op declarations and definitions from ODS.</span></span><br><span class="line">mlir_tablegen(MyExtension.h.inc -gen-op-decls)</span><br><span class="line">mlir_tablegen(MyExtension.cpp.inc -gen-op-defs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add a CMakeTarget we can depend on to ensure the generation happens before the compilation.</span></span><br><span class="line">add_public_tablegen_target(MyExtensionCh2IncGen)</span><br></pre></td></tr></table></figure>

<h4 id="实现operation需要重载的interface操作"><a href="#实现operation需要重载的interface操作" class="headerlink" title="实现operation需要重载的interface操作"></a>实现operation需要重载的interface操作</h4><p><code>/lib/MyExtension.cpp</code>中的代码可以做拆分</p>
<ul>
<li><p>定义transform dialect的extension：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Define a new transform dialect extension. This uses the CRTP idiom to</span></span><br><span class="line"><span class="comment">// identify extensions.</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyExtension</span></span><br><span class="line">    : <span class="keyword">public</span> ::mlir::transform::TransformDialectExtension&lt;MyExtension&gt; &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="comment">// The TypeID of this extension.</span></span><br><span class="line">  <span class="built_in">MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID</span>(MyExtension)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The extension must derive the base constructor.</span></span><br><span class="line">  <span class="keyword">using</span> Base::Base;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// This function initializes the extension, similarly to `initialize` in</span></span><br><span class="line">  <span class="comment">// dialect definitions. List individual operations and dependent dialects</span></span><br><span class="line">  <span class="comment">// here.</span></span><br><span class="line">  <span class="function"><span class="type">void</span> <span class="title">init</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
</li>
<li><p>extension的初始化：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">MyExtension::init</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Similarly to dialects, an extension can declare a dependent dialect. This</span></span><br><span class="line">  <span class="comment">// dialect will be loaded along with the extension and, therefore, along with</span></span><br><span class="line">  <span class="comment">// the Transform dialect. Only declare as dependent the dialects that contain</span></span><br><span class="line">  <span class="comment">// the attributes or types used by transform operations. Do NOT declare as</span></span><br><span class="line">  <span class="comment">// dependent the dialects produced during the transformation.</span></span><br><span class="line">  <span class="comment">// declareDependentDialect&lt;MyDialect&gt;();</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// When transformations are applied, they may produce new operations from</span></span><br><span class="line">  <span class="comment">// previously unloaded dialects. Typically, a pass would need to declare</span></span><br><span class="line">  <span class="comment">// itself dependent on the dialects containing such new operations. To avoid</span></span><br><span class="line">  <span class="comment">// confusion with the dialects the extension itself depends on, the Transform</span></span><br><span class="line">  <span class="comment">// dialects differentiates between:</span></span><br><span class="line">  <span class="comment">//   - dependent dialects, which are used by the transform operations, and</span></span><br><span class="line">  <span class="comment">//   - generated dialects, which contain the entities (attributes, operations,</span></span><br><span class="line">  <span class="comment">//     types) that may be produced by applying the transformation even when</span></span><br><span class="line">  <span class="comment">//     not present in the original payload IR.</span></span><br><span class="line">  <span class="comment">// In the following chapter, we will be add operations that generate function</span></span><br><span class="line">  <span class="comment">// calls and structured control flow operations, so let&#x27;s declare the</span></span><br><span class="line">  <span class="comment">// corresponding dialects as generated.</span></span><br><span class="line">  <span class="built_in">declareGeneratedDialect</span>&lt;::mlir::scf::SCFDialect&gt;();</span><br><span class="line">  <span class="built_in">declareGeneratedDialect</span>&lt;::mlir::func::FuncDialect&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Finally, we register the additional transform operations with the dialect.</span></span><br><span class="line">  <span class="comment">// List all operations generated from ODS. This call will perform additional</span></span><br><span class="line">  <span class="comment">// checks that the operations implement the transform and memory effect</span></span><br><span class="line">  <span class="comment">// interfaces required by the dialect interpreter and assert if they do not.</span></span><br><span class="line">  <span class="built_in">registerTransformOps</span>&lt;</span><br><span class="line"><span class="meta">#<span class="keyword">define</span> GET_OP_LIST</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;MyExtension.cpp.inc&quot;</span></span></span><br><span class="line">      &gt;();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>上述的一个重点是<code>declareDependentDialect</code>和<code>declareGeneratedDialect</code>的区别。<code>dependent dialects</code> 是 transform op 需要的，<code>generated dialects</code> 是 transform 执行后生成的。这里scf和func均是可能生成的dialect。</p>
</li>
<li><p>另一个重点是，<code>registerTransformOps</code>会注册ods定义的operation，同时会有检测机制check是否完成transform和sideeffect的interface。</p>
</li>
</ul>
</li>
<li><p>对特定op做interface重写：</p>
<ul>
<li><p>transform interface的apply方法重写：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">::mlir::DiagnosedSilenceableFailure mlir::transform::ChangeCallTargetOp::<span class="built_in">apply</span>(</span><br><span class="line">    <span class="comment">// The rewriter that should be used when modifying IR.</span></span><br><span class="line">    ::mlir::transform::TransformRewriter &amp;rewriter,</span><br><span class="line">    <span class="comment">// The list of payload IR entities that will be associated with the</span></span><br><span class="line">    <span class="comment">// transform IR values defined by this transform operation. In this case, it</span></span><br><span class="line">    <span class="comment">// can remain empty as there are no results.</span></span><br><span class="line">    ::mlir::transform::TransformResults &amp;results,</span><br><span class="line">    <span class="comment">// The transform application state. This object can be used to query the</span></span><br><span class="line">    <span class="comment">// current associations between transform IR values and payload IR entities.</span></span><br><span class="line">    <span class="comment">// It can also carry additional user-defined state.</span></span><br><span class="line">    ::mlir::transform::TransformState &amp;state) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// First, we need to obtain the list of payload operations that are associated</span></span><br><span class="line">  <span class="comment">// with the operand handle.</span></span><br><span class="line">  <span class="keyword">auto</span> payload = state.<span class="built_in">getPayloadOps</span>(<span class="built_in">getCall</span>());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Then, we iterate over the list of operands and call the actual IR-mutating</span></span><br><span class="line">  <span class="comment">// function. We also check the preconditions here.</span></span><br><span class="line">  <span class="keyword">for</span> (Operation *payloadOp : payload) &#123;</span><br><span class="line">    <span class="keyword">auto</span> call = <span class="built_in">dyn_cast</span>&lt;::mlir::func::CallOp&gt;(payloadOp);</span><br><span class="line">    <span class="keyword">if</span> (!call) &#123;</span><br><span class="line">      DiagnosedSilenceableFailure diag =</span><br><span class="line">          <span class="built_in">emitSilenceableError</span>() &lt;&lt; <span class="string">&quot;only applies to func.call payloads&quot;</span>;</span><br><span class="line">      diag.<span class="built_in">attachNote</span>(payloadOp-&gt;<span class="built_in">getLoc</span>()) &lt;&lt; <span class="string">&quot;offending payload&quot;</span>;</span><br><span class="line">      <span class="keyword">return</span> diag;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">updateCallee</span>(call, <span class="built_in">getNewTarget</span>());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If everything went well, return success.</span></span><br><span class="line">  <span class="keyword">return</span> DiagnosedSilenceableFailure::<span class="built_in">success</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>side effect的interface重写：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">void</span> mlir::transform::ChangeCallTargetOp::<span class="built_in">getEffects</span>(</span><br><span class="line">    ::llvm::SmallVectorImpl&lt;::mlir::MemoryEffects::EffectInstance&gt; &amp;effects) &#123;</span><br><span class="line">  <span class="comment">// Indicate that the `call` handle is only read by this operation because the</span></span><br><span class="line">  <span class="comment">// associated operation is not erased but rather modified in-place, so the</span></span><br><span class="line">  <span class="comment">// reference to it remains valid.</span></span><br><span class="line">  <span class="built_in">onlyReadsHandle</span>(<span class="built_in">getCallMutable</span>(), effects);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Indicate that the payload is modified by this operation.</span></span><br><span class="line">  <span class="built_in">modifiesPayload</span>(effects);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>注册整个myextension：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">registerMyExtension</span><span class="params">(::mlir::DialectRegistry &amp;registry)</span> </span>&#123;</span><br><span class="line">  registry.<span class="built_in">addExtensions</span>&lt;MyExtension&gt;();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>CmakeLists.txt如下：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Outside examples, this should be `add_mlir_library`.</span></span><br><span class="line">add_mlir_dialect_library(</span><br><span class="line">  <span class="comment"># Library called MyExtension.</span></span><br><span class="line">  MyExtensionCh2</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Built from the following source files.</span></span><br><span class="line">  MyExtension.cpp</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Make includes visible without top-level path.</span></span><br><span class="line">  ADDITIONAL_HEADER_DIRS</span><br><span class="line">  <span class="variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>/<span class="keyword">include</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Make sure ODS declaration and definitions are generated before compiling this.</span></span><br><span class="line">  DEPENDS</span><br><span class="line">  MyExtensionCh2IncGen</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Link in the transform dialect, an all generated dialects.</span></span><br><span class="line">  LINK_LIBS PRIVATE</span><br><span class="line">  MLIRTransformDialect</span><br><span class="line">  MLIRFuncDialect</span><br><span class="line">  MLIRSCFDialect</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<h4 id="transform-opt-驱动编写"><a href="#transform-opt-驱动编写" class="headerlink" title="transform-opt 驱动编写"></a>transform-opt 驱动编写</h4><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//===-- transform-opt.cpp - Transform dialect tutorial entry point --------===//</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.</span></span><br><span class="line"><span class="comment">// See https://llvm.org/LICENSE.txt for license information.</span></span><br><span class="line"><span class="comment">// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//===----------------------------------------------------------------------===//</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// This is the top-level file for the Transform dialect tutorial chapter 2.</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//===----------------------------------------------------------------------===//</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;MyExtension.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/Dialect/Transform/Transforms/Passes.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/IR/DialectRegistry.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/IR/MLIRContext.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/InitAllDialects.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/InitAllExtensions.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/Tools/mlir-opt/MlirOptMain.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;mlir/Transforms/Passes.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdlib&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">namespace</span> test &#123;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">registerTestTransformDialectExtension</span><span class="params">(mlir::DialectRegistry &amp;)</span></span>;</span><br><span class="line">&#125; <span class="comment">// namespace test</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> **argv)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// Register all &quot;core&quot; dialects and our transform dialect extension.</span></span><br><span class="line">  mlir::DialectRegistry registry;</span><br><span class="line">  mlir::<span class="built_in">registerAllDialects</span>(registry);</span><br><span class="line">  mlir::<span class="built_in">registerAllExtensions</span>(registry);</span><br><span class="line">  <span class="built_in">registerMyExtension</span>(registry);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Register transform interpreter pass.</span></span><br><span class="line">  mlir::transform::<span class="built_in">registerInterpreterPass</span>();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Register a handful of cleanup passes that we can run to make the output IR</span></span><br><span class="line">  <span class="comment">// look nicer.</span></span><br><span class="line">  mlir::<span class="built_in">registerCanonicalizerPass</span>();</span><br><span class="line">  mlir::<span class="built_in">registerCSEPass</span>();</span><br><span class="line">  mlir::<span class="built_in">registerSymbolDCEPass</span>();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Delegate to the MLIR utility for parsing and pass management.</span></span><br><span class="line">  <span class="keyword">return</span> mlir::<span class="built_in">MlirOptMain</span>(argc, argv, <span class="string">&quot;transform-opt-ch2&quot;</span>, registry)</span><br><span class="line">                 .<span class="built_in">succeeded</span>()</span><br><span class="line">             ? EXIT_SUCCESS</span><br><span class="line">             : EXIT_FAILURE;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>CmakeLists.txt如下：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">get_property</span>(dialect_libs GLOBAL PROPERTY MLIR_DIALECT_LIBS)</span><br><span class="line"><span class="keyword">get_property</span>(conversion_libs GLOBAL PROPERTY MLIR_CONVERSION_LIBS)</span><br><span class="line"><span class="keyword">set</span>(LIBS</span><br><span class="line">        <span class="variable">$&#123;dialect_libs&#125;</span></span><br><span class="line">        <span class="variable">$&#123;conversion_libs&#125;</span></span><br><span class="line">        MLIRIR</span><br><span class="line">        MLIRMlirOptMain</span><br><span class="line">        MLIRSideEffectInterfaces</span><br><span class="line">        MyExtensionCh2</span><br><span class="line">        )</span><br><span class="line">add_llvm_executable(transform-opt transform-opt.cpp)</span><br><span class="line"></span><br><span class="line"><span class="keyword">target_link_libraries</span>(transform-opt PRIVATE <span class="variable">$&#123;LIBS&#125;</span>)</span><br></pre></td></tr></table></figure>

<p>这部分代码和mlir代码没有区别，注册我们的myextension，然后注册各种pass，<strong>重点是注册<code>registerInterpreterPass</code>这个pass</strong>。</p>
<p>顶层CmakeLists.txt如下：</p>
<figure class="highlight cmake"><table><tr><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.20</span>.<span class="number">0</span>)</span><br><span class="line"><span class="keyword">project</span>(standalone-dialect LANGUAGES CXX C)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(CMAKE_BUILD_WITH_INSTALL_NAME_DIR <span class="keyword">ON</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_STANDARD <span class="number">17</span> CACHE <span class="keyword">STRING</span> <span class="string">&quot;C++ standard to conform to&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(CMAKE_SOURCE_DIR <span class="keyword">STREQUAL</span> CMAKE_CURRENT_SOURCE_DIR)</span><br><span class="line">  <span class="keyword">find_package</span>(MLIR REQUIRED CONFIG)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">message</span>(STATUS <span class="string">&quot;Using MLIRConfig.cmake in: $&#123;MLIR_DIR&#125;&quot;</span>)</span><br><span class="line">  <span class="keyword">message</span>(STATUS <span class="string">&quot;Using LLVMConfig.cmake in: $&#123;LLVM_DIR&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">set</span>(LLVM_RUNTIME_OUTPUT_INTDIR <span class="variable">$&#123;CMAKE_BINARY_DIR&#125;</span>/bin)</span><br><span class="line">  <span class="keyword">set</span>(LLVM_LIBRARY_OUTPUT_INTDIR <span class="variable">$&#123;CMAKE_BINARY_DIR&#125;</span>/lib)</span><br><span class="line">  <span class="keyword">set</span>(MLIR_BINARY_DIR <span class="variable">$&#123;CMAKE_BINARY_DIR&#125;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">list</span>(APPEND CMAKE_MODULE_PATH <span class="string">&quot;$&#123;MLIR_CMAKE_DIR&#125;&quot;</span>)</span><br><span class="line">  <span class="keyword">list</span>(APPEND CMAKE_MODULE_PATH <span class="string">&quot;$&#123;LLVM_CMAKE_DIR&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">include</span>(TableGen)</span><br><span class="line">  <span class="keyword">include</span>(AddLLVM)</span><br><span class="line">  <span class="keyword">include</span>(AddMLIR)</span><br><span class="line">  <span class="keyword">include</span>(HandleLLVMOptions)</span><br><span class="line"><span class="keyword">else</span>()</span><br><span class="line">  <span class="comment"># Build via external projects mechanism</span></span><br><span class="line">  <span class="keyword">set</span>(MLIR_MAIN_SRC_DIR <span class="variable">$&#123;LLVM_MAIN_SRC_DIR&#125;</span>/../mlir)</span><br><span class="line">  <span class="keyword">set</span>(MLIR_INCLUDE_DIR <span class="variable">$&#123;MLIR_MAIN_SRC_DIR&#125;</span>/<span class="keyword">include</span>)</span><br><span class="line">  <span class="keyword">set</span>(MLIR_GENERATED_INCLUDE_DIR <span class="variable">$&#123;LLVM_BINARY_DIR&#125;</span>/tools/mlir/<span class="keyword">include</span>)</span><br><span class="line">  <span class="keyword">set</span>(MLIR_INCLUDE_DIRS <span class="string">&quot;$&#123;MLIR_INCLUDE_DIR&#125;;$&#123;MLIR_GENERATED_INCLUDE_DIR&#125;&quot;</span>)</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(MLIR_ENABLE_BINDINGS_PYTHON)</span><br><span class="line">  <span class="keyword">include</span>(MLIRDetectPythonEnv)</span><br><span class="line">  mlir_configure_python_dev_packages()</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(STANDALONE_SOURCE_DIR <span class="variable">$&#123;PROJECT_SOURCE_DIR&#125;</span>)</span><br><span class="line"><span class="keyword">set</span>(STANDALONE_BINARY_DIR <span class="variable">$&#123;PROJECT_BINARY_DIR&#125;</span>)</span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;LLVM_INCLUDE_DIRS&#125;</span>)</span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;MLIR_INCLUDE_DIRS&#125;</span>)</span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;STANDALONE_SOURCE_DIR&#125;</span>/<span class="keyword">include</span>)</span><br><span class="line"><span class="keyword">include_directories</span>(<span class="variable">$&#123;STANDALONE_BINARY_DIR&#125;</span>/<span class="keyword">include</span>)</span><br><span class="line"><span class="keyword">link_directories</span>(<span class="variable">$&#123;LLVM_BUILD_LIBRARY_DIR&#125;</span>)</span><br><span class="line"><span class="keyword">add_definitions</span>(<span class="variable">$&#123;LLVM_DEFINITIONS&#125;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_subdirectory</span>(<span class="keyword">include</span>)</span><br><span class="line"><span class="keyword">add_subdirectory</span>(lib)</span><br><span class="line"><span class="keyword">add_subdirectory</span>(transform-opt)</span><br></pre></td></tr></table></figure>

<h3 id="Transform-extension的核心实现"><a href="#Transform-extension的核心实现" class="headerlink" title="Transform extension的核心实现"></a><font color = green>Transform extension的核心实现</font></h3><p>transform extension的核心操作，就是实现<code>transform.my.change_call_target</code> op的apply方法和getEffect方法，这两个方法决定了tranform之后生成的代码等。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="type">void</span> mlir::transform::ChangeCallTargetOp::<span class="built_in">getEffects</span>(</span><br><span class="line">    ::llvm::SmallVectorImpl&lt;::mlir::MemoryEffects::EffectInstance&gt; &amp;effects) &#123;</span><br><span class="line">  <span class="comment">// Indicate that the `call` handle is only read by this operation because the</span></span><br><span class="line">  <span class="comment">// associated operation is not erased but rather modified in-place, so the</span></span><br><span class="line">  <span class="comment">// reference to it remains valid.</span></span><br><span class="line">  <span class="built_in">onlyReadsHandle</span>(<span class="built_in">getCallMutable</span>(), effects);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Indicate that the payload is modified by this operation.</span></span><br><span class="line">  <span class="built_in">modifiesPayload</span>(effects);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Implementation of our transform dialect operation.</span></span><br><span class="line"><span class="comment">// This operation returns a tri-state result that can be one of:</span></span><br><span class="line"><span class="comment">// - success when the transformation succeeded;</span></span><br><span class="line"><span class="comment">// - definite failure when the transformation failed in such a way that</span></span><br><span class="line"><span class="comment">//   following transformations are impossible or undesirable, typically it could</span></span><br><span class="line"><span class="comment">//   have left payload IR in an invalid state; it is expected that a diagnostic</span></span><br><span class="line"><span class="comment">//   is emitted immediately before returning the definite error;</span></span><br><span class="line"><span class="comment">// - silenceable failure when the transformation failed but following</span></span><br><span class="line"><span class="comment">//   transformations are still applicable, typically this means a precondition</span></span><br><span class="line"><span class="comment">//   for the transformation is not satisfied and the payload IR has not been</span></span><br><span class="line"><span class="comment">//   modified. The silenceable failure additionally carries a Diagnostic that</span></span><br><span class="line"><span class="comment">//   can be emitted to the user.</span></span><br><span class="line">::mlir::DiagnosedSilenceableFailure mlir::transform::ChangeCallTargetOp::<span class="built_in">apply</span>(</span><br><span class="line">    <span class="comment">// The rewriter that should be used when modifying IR.</span></span><br><span class="line">    ::mlir::transform::TransformRewriter &amp;rewriter,</span><br><span class="line">    <span class="comment">// The list of payload IR entities that will be associated with the</span></span><br><span class="line">    <span class="comment">// transform IR values defined by this transform operation. In this case, it</span></span><br><span class="line">    <span class="comment">// can remain empty as there are no results.</span></span><br><span class="line">    ::mlir::transform::TransformResults &amp;results,</span><br><span class="line">    <span class="comment">// The transform application state. This object can be used to query the</span></span><br><span class="line">    <span class="comment">// current associations between transform IR values and payload IR entities.</span></span><br><span class="line">    <span class="comment">// It can also carry additional user-defined state.</span></span><br><span class="line">    ::mlir::transform::TransformState &amp;state) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// First, we need to obtain the list of payload operations that are associated</span></span><br><span class="line">  <span class="comment">// with the operand handle.</span></span><br><span class="line">  <span class="keyword">auto</span> payload = state.<span class="built_in">getPayloadOps</span>(<span class="built_in">getCall</span>());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Then, we iterate over the list of operands and call the actual IR-mutating</span></span><br><span class="line">  <span class="comment">// function. We also check the preconditions here.</span></span><br><span class="line">  <span class="keyword">for</span> (Operation *payloadOp : payload) &#123;</span><br><span class="line">    <span class="keyword">auto</span> call = <span class="built_in">dyn_cast</span>&lt;::mlir::func::CallOp&gt;(payloadOp);</span><br><span class="line">    <span class="keyword">if</span> (!call) &#123;</span><br><span class="line">      DiagnosedSilenceableFailure diag =</span><br><span class="line">          <span class="built_in">emitSilenceableError</span>() &lt;&lt; <span class="string">&quot;only applies to func.call payloads&quot;</span>;</span><br><span class="line">      diag.<span class="built_in">attachNote</span>(payloadOp-&gt;<span class="built_in">getLoc</span>()) &lt;&lt; <span class="string">&quot;offending payload&quot;</span>;</span><br><span class="line">      <span class="keyword">return</span> diag;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">updateCallee</span>(call, <span class="built_in">getNewTarget</span>());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If everything went well, return success.</span></span><br><span class="line">  <span class="keyword">return</span> DiagnosedSilenceableFailure::<span class="built_in">success</span>();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">static</span> <span class="type">void</span> <span class="title">updateCallee</span><span class="params">(mlir::func::CallOp call, llvm::StringRef newTarget)</span> </span>&#123;</span><br><span class="line">  call.<span class="built_in">setCallee</span>(newTarget);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上述两段代码注释已经非常详尽了，再次不多赘述。</p>
<h2 id="Chapter3：实现更加复杂的transform-operation"><a href="#Chapter3：实现更加复杂的transform-operation" class="headerlink" title="Chapter3：实现更加复杂的transform operation"></a><font color = brown>Chapter3：实现更加复杂的transform operation</font></h2><p>这一部分的tutorial完成两件事：</p>
<ul>
<li>给chapter2实现的transform operation针对的payload handle添加constraint trait，通过使用trait的方式简化遍历匹配<code>func::call</code>这一流程。</li>
<li>添加一个新的op，复习整个transform的流程。实现一个callOpInterface，使得handle不局限于func.call，而是只要实现了callOpInterface的op均可。</li>
</ul>
<h2 id="Chapter4：利用transform-op来做payload的匹配"><a href="#Chapter4：利用transform-op来做payload的匹配" class="headerlink" title="Chapter4：利用transform op来做payload的匹配"></a><font color = brown>Chapter4：利用transform op来做payload的匹配</font></h2><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a><font color = brown>参考资料</font></h2><ol>
<li><a href="https://llvm.org/devmtg/2023-05/slides/Tutorial-May11/02-Zinenko-TransformDialectTutorial.pdf">transform dialect tutorial talk - EuroLLVM</a></li>
</ol>
]]></content>
      <categories>
        <category>编译技术</category>
        <category>mlir</category>
        <category>dialect学习</category>
      </categories>
      <tags>
        <tag>mlir</tag>
        <tag>dialect学习</tag>
      </tags>
  </entry>
</search>
