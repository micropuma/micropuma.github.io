<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="Triton DeepDive">
<meta property="og:url" content="http://example.com/2025/09/29/Triton-DeepDive/index.html">
<meta property="og:site_name" content="Leon&#39;s Blog">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/image-20250605183401865.png">
<meta property="og:image" content="http://example.com/images/1634cbce4a1e5664fab16421cca0830c.png">
<meta property="og:image" content="http://example.com/images/ec7951a20c3a28e835b47c7c560dc577.png">
<meta property="og:image" content="http://example.com/images/da17c27e9b73b2a6b00da0bc3cca4a47.png">
<meta property="og:image" content="http://example.com/images/dataflow-1759110868998-1.png">
<meta property="article:published_time" content="2025-09-29T01:48:05.000Z">
<meta property="article:modified_time" content="2025-10-03T06:54:51.217Z">
<meta property="article:author" content="Leon Dou">
<meta property="article:tag" content="GPU">
<meta property="article:tag" content="编译">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/image-20250605183401865.png">

<link rel="canonical" href="http://example.com/2025/09/29/Triton-DeepDive/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Triton DeepDive | Leon's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Leon's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">分享一点有趣的技术</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/micropuma" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/09/29/Triton-DeepDive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Leon Dou">
      <meta itemprop="description" content="关注领域：体系结构，编译技术">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Leon's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Triton DeepDive
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-09-29 09:48:05" itemprop="dateCreated datePublished" datetime="2025-09-29T09:48:05+08:00">2025-09-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-10-03 14:54:51" itemprop="dateModified" datetime="2025-10-03T14:54:51+08:00">2025-10-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GPU/" itemprop="url" rel="index"><span itemprop="name">GPU</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GPU/%E7%BC%96%E8%AF%91/" itemprop="url" rel="index"><span itemprop="name">编译</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GPU/%E7%BC%96%E8%AF%91/triton%E6%8A%80%E6%9C%AF/" itemprop="url" rel="index"><span itemprop="name">triton技术</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>29k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>26 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><img src="/images/image-20250605183401865.png" alt="image-20250605183401865"></p>
<span id="more"></span>

<h1 id="Triton-Deep-Dive"><a href="#Triton-Deep-Dive" class="headerlink" title="Triton Deep Dive"></a>Triton Deep Dive</h1><p>In this doc, we will dive into triton internals, to show how our hand-written triton-lang is lowered into ptx&#x2F;cubin and finally launched by cuda driver. This doc is organized into following parts:  </p>
<ul>
<li>Overall framework </li>
<li>Source code structure</li>
<li>Dive into jit compilation (Python)</li>
<li>Pybind gluer</li>
<li>MLIR Lowering passes (C++)</li>
</ul>
<h2 id="Chapter1-Overall-Framework"><a href="#Chapter1-Overall-Framework" class="headerlink" title="Chapter1: Overall Framework"></a><font color = brown>Chapter1: Overall Framework</font></h2><p>The <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton">Triton</a> codes nowadays is actually quite different from the original <a target="_blank" rel="noopener" href="https://dl.acm.org/doi/10.1145/3315508.3329973">paper</a>. There are two major differences:  </p>
<ol>
<li>Python DSL is introduced to make it easier for model developers. Pythonic is the trend nowadays in ML compiler world :key:.</li>
<li>Passes are all rewritten in MLIR, making it more extendible.</li>
</ol>
<p>So we can clearly see two different parts in this shift, a python wrapper for user end, and the actual code optimization and conversion realized in C++ with MLIR ecosystem, glued by <a href="git@github.com:pybind/pybind11.git">pybind</a> mechanism.  </p>
<p>With this bluescript in mind, we now try to touch three important questions:  </p>
<ol>
<li>What is the input?</li>
<li>What are temporaries and final codes during Triton compilation?</li>
<li>How are those compiled codes loaded by device driver?</li>
</ol>
<p>Let’s explain these questions one by one.</p>
<blockquote>
<p>:question: What is the input?<br>Input of triton source code merely contains three parts: (1) triton kernel (2) kernel wrapper that can easily replace counterpart operator in certain ml models (3) data preparation and kernel launch. Below is a basic code sample:  </p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@triton.jit  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">_kernel</span>(<span class="params">...</span>):</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kernel</span>(<span class="params">...</span>):</span><br><span class="line">    _kernel[grid](...)  <span class="comment"># SPMD programming model</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># data preparation</span></span><br><span class="line">A ...</span><br><span class="line">B ...</span><br><span class="line">C ...</span><br><span class="line"><span class="comment"># kernel call</span></span><br><span class="line">D = kernel(A, B, C)</span><br><span class="line"><span class="comment"># correctness test and perf test</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<blockquote>
<p>:question: What are the temporaries and the final codes?<br>All temporaries are saved on disk, user can set TRITON_CACHE_DIR, or by default the dir path is ~&#x2F;.triton&#x2F;cache. Following figure shows files in the triton cache:<br><img src="/images/1634cbce4a1e5664fab16421cca0830c.png" alt="1634cbce4a1e5664fab16421cca0830c"><br>Here is a simple descriptions for those files:<br><code>cuda_utils.so</code>contains cuda helper functions for correct cuda launching. <code>triton_launcher.so</code>helps launch cuda kernels. <code>.source</code> is user-written triton dsl, <code>.ttir</code> is a one-to-one mapping ir, <code>ttgir</code> is gpu aware ir, <code>llir</code> is llvm ir converted from <code>ttgir</code>. Final result is <code>.ptx</code>, a cuda assembly. <code>.json</code> contains all the metadata of this kernel, which can help jit compile and launch(hash this compiled kernel for future use).</p>
</blockquote>
<blockquote>
<p>:question: How are ptx kernel launched by device driver?<br>Secrete lies in <code>.so</code> file. This file contains wrapper for <code>cuKernelLaunch</code> function、grid configurations and parameters, which enable python to dynamically load this .so with importlib, and finally launch ptx&#x2F;cubin kernels with control of device driver.  </p>
</blockquote>
<p>With these answers in mind, we can get a clearer roadmap of triton compiler. Then let’s touch source code to make it more solid. First let’s have a overall understanding of the codebase structure.</p>
<h2 id="Chapter2-Source-code-structure"><a href="#Chapter2-Source-code-structure" class="headerlink" title="Chapter2: Source code structure "></a><font color = brown>Chapter2: Source code structure </font></h2><p>Triton Compiler project contains following directories:  </p>
<ul>
<li><code>/python/triton/tools/compile.py</code> – The top-level compiler tool.</li>
<li><code>/python/triton/runtime/jit.py</code> – The runtime utility, which includes kernel compilation cache, source data management, driver interaction, and kernel scheduling. :key:</li>
<li><code>/python/triton/compiler/code_generator.py</code> – Mainly handles the AST generated by the DSL, translating it into MLIR IR (i.e., Triton IR).</li>
<li><code>/third_party/nvidia/backend/compiler.py</code> – The compilation pipeline for specific hardware vendors, such as NVIDIA’s CUDA flow. Typically involves the transformation of TritonGPU IR and code generation for PTX and beyond. :key:</li>
<li><code>triton/python/src/passes.cc</code> – Glue layer(pybind) that organizes the various passes. :key:</li>
<li><code>*/lib</code> - cpp codebase, contains mlir source code for ir optimization, analysis and conversion. :key:</li>
<li><code>*/include</code> - header file for lib. :key:</li>
</ul>
<p>Let’s first dive into python layer, to grasp how JIT compilation works in triton.</p>
<h2 id="Chapter3-JIT-compile"><a href="#Chapter3-JIT-compile" class="headerlink" title="Chapter3: JIT compile"></a><font color = brown>Chapter3: JIT compile</font></h2><p>This part, we focus on <code>python/triton/runtime/</code> repository, mainly <code>jit.py</code> , <code>compile.py</code> and <code>build.py</code>. First let’s see a flowgraph to have a general understanding of triton jit compilation:<br><img src="/images/ec7951a20c3a28e835b47c7c560dc577.png" alt="ec7951a20c3a28e835b47c7c560dc577"></p>
<p>We can divide the whole jit compile workflow into three parts, and readers can refer to source code each part for further understanding: </p>
<ul>
<li>Jit Interface: this part mainly deal with user-written triton-lang kernel and kernel launch. Jit main function is <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/python/triton/runtime/jit.py#L885-L937">here</a>, and there are two main classes that we should pay attention with, JitFunction and its parent class KernelInterface. In KernelInterface, we should focus on <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/python/triton/runtime/jit.py#L411-L421"><code>__getitem__()</code></a>function, which is the entry of <code>kernel[grid]()</code>. And in JitFunction, <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/python/triton/runtime/jit.py#L693-L744"><code>run()</code></a> function is quite essential.</li>
<li>Compilation: this part works when our kernel is firstly compiled, no previous cached kernel matched. This part is mainly written in mlir c++, turn to <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/compiler.py#L229-L243"><code>make_ttir()</code></a>,<a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/compiler.py#L245-L323"><code>make_ttgir()</code></a>,<a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/compiler.py#L341-L411"><code>make_llir()</code></a>,<a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/compiler.py#L413-L443"><code>make_ptx()</code></a> and <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/compiler.py#L435-L494"><code>make_cubin()</code></a> for further dive. This pipeline is the keypoint in next chapter.   <blockquote>
<p>This pipeline is vendor specific. For Nvidia GPU, refer to <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/compiler.py">cuda backend compiler</a>.</p>
</blockquote>
</li>
<li>Runtime &amp; Driver: this part first turn compiled cubin into <code>.so</code> lib, refer <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/python/triton/compiler/compiler.py#L412-L441">here</a>. Then use <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/python/triton/compiler/compiler.py#L443-L472"><code>init_handle()</code></a> to init a cuda stream and all metadata. Finally use <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/driver.py#L723-L726"><code>launcher_cls</code></a> to evoke a <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/driver.py#L677-L718"><code>CudaLuancher</code></a> class and finally launch a cuda kernel.</li>
</ul>
<p>Hoping by following the flow graph and descriptions of each key part of JIT compilation, readers can get a clear picture of the whole python codebase and jit flow. Then comes the most exciting part to MLIR compilers, the actual code transformation and optimizations, proposing many interesting and advanced gpu compilation techniques such as: <a target="_blank" rel="noopener" href="https://www.lei.chat/posts/triton-linear-layout-concept/">triton layout</a>, coalesce opt, tensor core opt, etc. </p>
<h2 id="Chapter4-MLIR-Lowering-Passes"><a href="#Chapter4-MLIR-Lowering-Passes" class="headerlink" title="Chapter4: MLIR Lowering Passes"></a><font color = brown>Chapter4: MLIR Lowering Passes</font></h2><p>This part, we enter the core optimization pipeline of Triton. The whole lowering pipeline is: triton-lang -&gt; ttir -&gt; ttgir -&gt; llir -&gt; ptx -&gt; cubin. Refer to <a href="./pipeline">triton pipeline</a> for detailed code transformation of a basic vector add triton-lang operator.  </p>
<h3 id="TTIR"><a href="#TTIR" class="headerlink" title="TTIR"></a>TTIR</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_ttir</span>(<span class="params">mod, metadata, opt, capability</span>):     <span class="comment"># triton ir，主要描述上层计算行为</span></span><br><span class="line">    <span class="keyword">import</span> pdb</span><br><span class="line">    pdb.set_trace()</span><br><span class="line">    pm = ir.pass_manager(mod.context)</span><br><span class="line">    pm.enable_debug()</span><br><span class="line">    passes.common.add_inliner(pm)</span><br><span class="line">    passes.ttir.add_rewrite_tensor_pointer(pm)</span><br><span class="line">    <span class="keyword">if</span> capability // <span class="number">10</span> &lt; <span class="number">9</span>:</span><br><span class="line">        passes.ttir.add_rewrite_tensor_descriptor_to_pointer(pm)</span><br><span class="line">    passes.common.add_canonicalizer(pm)</span><br><span class="line">    passes.ttir.add_combine(pm)</span><br><span class="line">    passes.ttir.add_reorder_broadcast(pm)</span><br><span class="line">    passes.common.add_cse(pm)</span><br><span class="line">    passes.common.add_symbol_dce(pm)</span><br><span class="line">    passes.ttir.add_loop_unroll(pm)</span><br><span class="line">    pm.run(mod)</span><br><span class="line">    <span class="keyword">return</span> mod</span><br></pre></td></tr></table></figure>

<p>In this doc, we only focus on Triton-specific optimizations, that are <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/lib/Dialect/Triton/Transforms/RewriteTensorPointer.cpp"><code>RewriteTensorPointer</code></a>, <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/lib/Dialect/Triton/Transforms/Combine.cpp"><code>Combine</code></a> and <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/lib/Dialect/Triton/Transforms/ReorderBroadcast.cpp"><code>ReorderBroadcast</code></a> three passes. This part highly refer to <a target="_blank" rel="noopener" href="https://tfruan2000.github.io/posts/triton-source-code-1/">OpenAI Triton 源码走读[transforms in ttir]</a>.</p>
<h4 id="RewriteTensorPointer"><a href="#RewriteTensorPointer" class="headerlink" title="RewriteTensorPointer"></a>RewriteTensorPointer</h4><p>The figure below shows all details on how this pass do to <code>tl.make_block_ptr</code> and <code>tl.advance</code> operations.<br><img src="/images/da17c27e9b73b2a6b00da0bc3cca4a47.png" alt="da17c27e9b73b2a6b00da0bc3cca4a47" style="zoom:67%;" /></p>
<p>Refer to <a href="../Triton-101/DeepDive/ttir/test/test_rewrite.mlir">Before-pass-example</a> and <a href="../Triton-101/DeepDive/ttir/test/result1.mlir">After-pass-example</a> to see detailed effects of this pass.</p>
<blockquote>
<p>A tip to help debug pattern rewrite in mlir: use <code>-debug-only=greedy-rewriter</code>. Refer to <a target="_blank" rel="noopener" href="https://mlir.llvm.org/docs/PatternRewriter/">Pattern Rewrite</a> for further details.</p>
</blockquote>
<h4 id="Combine"><a href="#Combine" class="headerlink" title="Combine"></a>Combine</h4><p>The table below shows all combination rules that triton applied:  </p>
<table>
<thead>
<tr>
<th>Pattern Name</th>
<th>Match Rule</th>
<th>Rewrite Result</th>
<th>Optimization Purpose</th>
</tr>
</thead>
<tbody><tr>
<td>CombineDotAddIPattern</td>
<td><code>AddIOp(d, DotOp(a, b, c=0))</code></td>
<td><code>DotOp(a, b, d)</code></td>
<td>Merge dot with zero-init + add to eliminate redundant <code>add</code>.</td>
</tr>
<tr>
<td>CombineDotAddFPattern</td>
<td><code>AddFOp(d, DotOp(a, b, c=0, maxNumImpreciseAcc=0))</code></td>
<td><code>DotOp(a, b, d)</code></td>
<td>Same as above, but for floating-point add, restricted to <code>maxNumImpreciseAcc == 0</code>.</td>
</tr>
<tr>
<td>CombineDotAddIRevPattern</td>
<td><code>AddIOp(DotOp(a, b, c=0), d)</code></td>
<td><code>DotOp(a, b, d)</code></td>
<td>Same as <code>CombineDotAddIPattern</code>, but with <code>dot</code> on the left-hand side.</td>
</tr>
<tr>
<td>CombineDotAddFRevPattern</td>
<td><code>AddFOp(DotOp(a, b, c=0, maxNumImpreciseAcc=0), d)</code></td>
<td><code>DotOp(a, b, d)</code></td>
<td>Same as <code>CombineDotAddFPattern</code>, but with <code>dot</code> on the left-hand side.</td>
</tr>
<tr>
<td>CombineAddPtrPattern</td>
<td><code>addptr(addptr(ptr, idx0), idx1)</code></td>
<td><code>addptr(ptr, AddIOp(idx0, idx1))</code></td>
<td>Merge multi-level pointer offsets to avoid nested <code>addptr</code>; preserve optional attributes (div&#x2F;cont&#x2F;const).</td>
</tr>
<tr>
<td>CombineSelectMaskedLoadPattern</td>
<td><code>select(cond, load(ptrs, splat(cond), ?), other)</code></td>
<td><code>load(ptrs, splat(cond), other)</code></td>
<td>Merge <code>select</code>-wrapped masked load into a more concise <code>load</code>.</td>
</tr>
<tr>
<td>CombineBroadcastMulReducePattern</td>
<td><code>reduce(sum, broadcast(x[:, :, None]) * broadcast(y[None, :, :]))</code></td>
<td><code>dot(x, y)</code></td>
<td>Recognize matrix multiplication pattern (broadcast-mul-reduce) and replace with efficient <code>dot</code>.</td>
</tr>
<tr>
<td>CombineReshapeReducePatterns</td>
<td><code>reshape(tensor)</code> (1D, <code>allowReorder=false</code>, user is reduce&#x2F;histogram)</td>
<td>set <code>allowReorder=true</code></td>
<td>Enable element reordering for 1D tensor reshape in reduction&#x2F;histogram cases, improving optimization.</td>
</tr>
<tr>
<td>RankedReduceDescriptorLoads</td>
<td><code>reshape(descriptor_load(...))</code> with rank-reducing reshape</td>
<td>absorb reshape into <code>descriptor_load</code> and modify result type</td>
<td>Eliminate meaningless reshape by folding it into <code>descriptor_load</code>.</td>
</tr>
</tbody></table>
<h4 id="ReorderBroadcast"><a href="#ReorderBroadcast" class="headerlink" title="ReorderBroadcast"></a>ReorderBroadcast</h4><p>The table below lists all broadcast + elementwise reorder rules that triton applied:</p>
<table>
<thead>
<tr>
<th>Pattern Name</th>
<th>Original Form</th>
<th>Reordered Form</th>
<th>Conditions</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td>MoveSplatAfterElementwisePattern</td>
<td><code>elementwise(splat(a), splat(b), ...)</code></td>
<td><code>splat(elementwise(a, b, ...))</code></td>
<td>- All operands are <code>SplatOp</code> or constant splats.<br>- Operation is <strong>elementwise</strong> and has no side effects.</td>
<td>Compute on scalars first, then splat once → avoids redundant tensor elementwise ops.</td>
</tr>
<tr>
<td>MoveBroadcastAfterElementwisePattern</td>
<td><code>elementwise(broadcast(a), splat(b), ...)</code></td>
<td><code>broadcast(elementwise(a, b, ...))</code></td>
<td>- At most one broadcast operand.<br>- All broadcasts must expand to the <strong>same shape</strong>.<br>- Operation is <strong>elementwise</strong> and has no side effects.</td>
<td>Compute on the smaller source tensor, then broadcast once → reduces duplicated computation.</td>
</tr>
<tr>
<td>Canonicalization (built-in)</td>
<td>e.g., <code>broadcast(broadcast(a))</code>, <code>expand_dims(expand_dims(a))</code></td>
<td>Simplified canonical form</td>
<td>Provided by <code>BroadcastOp</code> and <code>ExpandDimsOp</code>.</td>
<td>Normalizes IR to expose more rewrite opportunities and remove redundant ops.</td>
</tr>
</tbody></table>
<p>These three passes are all hardware-agnostic optimizations, to make further analysis and transformation more efficient. </p>
<h3 id="TTIR-TTGIR"><a href="#TTIR-TTGIR" class="headerlink" title="TTIR -&gt; TTGIR"></a>TTIR -&gt; TTGIR</h3><p>This part highly refer to <a target="_blank" rel="noopener" href="https://tfruan2000.github.io/posts/triton-source-code-2/">OpenAI Triton 源码走读[ttir-2-ttgir]</a>, <a target="_blank" rel="noopener" href="https://www.lei.chat/posts/triton-linear-layout-concept/">Triton Linear Layout: Concept</a> and <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/687394750">Triton Axis Analysis</a>. Let’s first see what this pass make changes to IR.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">IR before conversion</span><br><span class="line">module &#123;</span><br><span class="line">  tt.func public @add_kernel(%arg0: !tt.ptr&lt;f32&gt; &#123;tt.divisibility = 16 : i32&#125; loc(&quot;/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py&quot;:32:0), %arg1: !tt.ptr&lt;f32&gt; &#123;tt.divisibility = 16 : i32&#125; loc(&quot;/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py&quot;:32:0), %arg2: !tt.ptr&lt;f32&gt; &#123;tt.divisibility = 16 : i32&#125; loc(&quot;/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py&quot;:32:0), %arg3: i32 &#123;tt.divisibility = 16 : i32&#125; loc(&quot;/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py&quot;:32:0)) attributes &#123;noinline = false&#125; &#123; ... &#125;</span><br><span class="line">&#125;</span><br><span class="line">IR after conversion</span><br><span class="line">#blocked = #ttg.blocked&lt;&#123;sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]&#125;&gt;</span><br><span class="line">#loc = loc(&quot;/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py&quot;:32:0)</span><br><span class="line">module attributes &#123;&quot;ttg.num-ctas&quot; = 1 : i32, &quot;ttg.num-warps&quot; = 4 : i32, ttg.target = &quot;cuda:86&quot;, &quot;ttg.threads-per-warp&quot; = 32 : i32&#125; &#123;</span><br><span class="line">  tt.func public @add_kernel(%arg0: !tt.ptr&lt;f32&gt; &#123;tt.divisibility = 16 : i32&#125; loc(&quot;/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py&quot;:32:0), %arg1: !tt.ptr&lt;f32&gt; &#123;tt.divisibility = 16 : i32&#125; loc(&quot;/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py&quot;:32:0), %arg2: !tt.ptr&lt;f32&gt; &#123;tt.divisibility = 16 : i32&#125; loc(&quot;/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py&quot;:32:0), %arg3: i32 &#123;tt.divisibility = 16 : i32&#125; loc(&quot;/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py&quot;:32:0)) attributes &#123;noinline = false&#125; &#123; ... &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>The main difference is:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#blocked = #ttg.blocked<span class="string">&lt;&#123;sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]&#125;&gt;</span></span></span><br><span class="line"><span class="meta">#loc = loc(<span class="string">&quot;/mnt/home/douliyang/triton-workspace/triton-tutorial/tutorial/Triton-101/Debug/vector_add.py&quot;</span>:32:0)</span></span><br><span class="line"><span class="keyword">module</span> attributes &#123;<span class="string">&quot;ttg.num-ctas&quot;</span> = <span class="number">1</span> : i32, <span class="string">&quot;ttg.num-warps&quot;</span> = <span class="number">4</span> : i32, ttg.target = <span class="string">&quot;cuda:86&quot;</span>, <span class="string">&quot;ttg.threads-per-warp&quot;</span> = <span class="number">32</span> : i32&#125;</span><br></pre></td></tr></table></figure>

<p>In conclusion, the main difference after ttir-&gt;ttgir conversion is the layout attribute attached to IR tensors, which defines how the data is parallelized and processed by threads. This layout attribute is propagated during the lowering process. In this doc, we are more interested in how TTIR is converted to TTGIR, and how Layout attrs are attatched and transformed. Readers should refer to <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/include/triton/Dialect/TritonGPU/IR/TritonGPUAttrDefs.td#L62-L704">TritonGPUAttr</a> and <a href="./Triton-Layout.md">Triton Layout doc</a>for prior knowledge on triton layout.</p>
<h3 id="TTGIR"><a href="#TTGIR" class="headerlink" title="TTGIR"></a>TTGIR</h3><p>Readers can refer to <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/third_party/nvidia/backend/compiler.py#L245-L323">make_ttgir</a> to view pass pipelines in TTGIR dialect.   </p>
<p>In this part, triton do all critical GPU-specific optimizations, with the help fo Axis analysis. Let’s first dive into the design of Axis analysis, combined with previous layout information, which clearly shows how tensors are stored and distribute within threads.   </p>
<h4 id="Axis-Analysis"><a href="#Axis-Analysis" class="headerlink" title="Axis Analysis"></a>Axis Analysis</h4><p>Axis Analysis is like other standard analysis pass, gather detailed information to help guide further code transformation. This part highly refer to <a target="_blank" rel="noopener" href="https://www.zhihu.com/search?type=content&q=triton%20axis%20analysis">OpenAI Triton: Dive into Axis and Coalesce</a> blog. To fully understand the detailed implementation behind, we need to understand an important concept: <strong>Dataflow Analysis</strong> and how it is implemented in <code>Triton</code> with the help of mlir infrastructure.  </p>
<p>Below is a Flow graph shows the whole dataflow framework:  </p>
<p><img src="/images/dataflow-1759110868998-1.png" alt="dataflow"></p>
<blockquote>
<p>Refer to <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/include/triton/Analysis/AxisInfo.h#L191-L270">ModuleAxisInfoAnalysis</a> for detailed implementation of intraprocedual info propagation, pay attention to <a href=""><strong>its class construction</strong></a> and <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/lib/Analysis/AxisInfo.cpp#L1311-L1348"><strong>initialize()</strong></a>. Refer to <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/lib/Analysis/AxisInfo.cpp#L137-L181">AxisInfoAnalysis</a> for AxisInfoAnalysis, where <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/lib/Analysis/AxisInfo.cpp#L1040-L1075"><strong>visitOperation()</strong></a> is essential. Also refer to <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/include/triton/Analysis/AxisInfo.h#L19-L150">AxisInfo.h</a> for AxisInfo to see lattice define in dataflow analysis. To have a glance of Dataflow framework in MLIR, refer to <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1895569118196904165">Dataflow In MLIR</a> and <a target="_blank" rel="noopener" href="https://lowlevelbits.com/p/the-missing-guide-to-dataflow-analysis">The Misssing Guide of Dataflow Analysis in MLIR</a>.   </p>
</blockquote>
<p>After clarifying the whole AxisInfo analysis process, let’s focus on a certain op (pick <code>triton::AddPtrOp</code> here) to show how a certain op’s AxisInfo is updated. Below is the source code:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> OpTy&gt;</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AddSubOpAxisInfoVisitor</span> <span class="keyword">final</span> : <span class="keyword">public</span> BinaryOpVisitorImpl&lt;OpTy&gt; &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">  <span class="keyword">using</span> BinaryOpVisitorImpl&lt;OpTy&gt;::BinaryOpVisitorImpl;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">  <span class="function"><span class="type">int64_t</span> <span class="title">getContiguity</span><span class="params">(OpTy op, <span class="type">const</span> AxisInfo &amp;lhs, <span class="type">const</span> AxisInfo &amp;rhs,</span></span></span><br><span class="line"><span class="params"><span class="function">                        <span class="type">int</span> dim)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">    <span class="comment">// Contiguity assumes an increasing sequence. So for SubIOp contiguous</span></span><br><span class="line">    <span class="comment">// RHS doesn&#x27;t produce a contiguous result.</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">isa</span>&lt;arith::SubIOp&gt;(op))</span><br><span class="line">      <span class="keyword">return</span> <span class="built_in">gcd</span>(lhs.<span class="built_in">getContiguity</span>(dim), rhs.<span class="built_in">getConstancy</span>(dim));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> std::<span class="built_in">max</span>(<span class="built_in">gcd</span>(lhs.<span class="built_in">getConstancy</span>(dim), rhs.<span class="built_in">getContiguity</span>(dim)),</span><br><span class="line">                    <span class="built_in">gcd</span>(lhs.<span class="built_in">getContiguity</span>(dim), rhs.<span class="built_in">getConstancy</span>(dim)));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">int64_t</span> <span class="title">getDivisibility</span><span class="params">(OpTy op, <span class="type">const</span> AxisInfo &amp;lhs, <span class="type">const</span> AxisInfo &amp;rhs,</span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">int</span> dim)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">    <span class="comment">// lhs = k * d_lhs = k * k&#x27; * gcd(d_lhs, d_rhs)</span></span><br><span class="line">    <span class="comment">// rhs = p * d_rhs = p * p&#x27; * gcd(d_lhs, d_rhs)</span></span><br><span class="line">    <span class="comment">// lhs + rhs = k * d_lhs + p * d_rhs = (k * k&#x27; + p * p&#x27;) * gcd(d_lhs, d_rhs)</span></span><br><span class="line">    <span class="keyword">auto</span> rhsDivisibility = rhs.<span class="built_in">getDivisibility</span>(dim);</span><br><span class="line">    <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(std::is_same_v&lt;OpTy, triton::AddPtrOp&gt;)</span> </span>&#123;</span><br><span class="line">      <span class="comment">//  %ptr = addptr %lhs, %rhs</span></span><br><span class="line">      <span class="comment">// is equivalent to</span></span><br><span class="line">      <span class="comment">//  %0 = mul %rhs, %elemSize</span></span><br><span class="line">      <span class="comment">//  %ptr = add %lhs, %0</span></span><br><span class="line">      <span class="comment">// The result will still be contiguous in terms of elements but not bytes</span></span><br><span class="line">      <span class="comment">// For example:</span></span><br><span class="line">      <span class="comment">// addptr [16] : !ptr&lt;i32&gt;, [0, 1, 2, 3] : i32 -&gt; !ptr&lt;i32&gt;</span></span><br><span class="line">      <span class="comment">// returns:</span></span><br><span class="line">      <span class="comment">// [16, 20, 24, 28] : !ptr&lt;i32&gt;</span></span><br><span class="line">      <span class="comment">// with element locations:</span></span><br><span class="line">      <span class="comment">// [4, 5, 6, 7]</span></span><br><span class="line">      <span class="comment">// It is &quot;strided contiguous&quot; with a divisibility of 16 bytes</span></span><br><span class="line">      <span class="keyword">auto</span> rank = lhs.<span class="built_in">getRank</span>();</span><br><span class="line">      <span class="keyword">auto</span> elemSize = std::<span class="built_in">max</span>&lt;<span class="type">int64_t</span>&gt;(</span><br><span class="line">          <span class="number">1</span>, triton::<span class="built_in">getPointeeBitWidth</span>(op.<span class="built_in">getPtr</span>().<span class="built_in">getType</span>()) / <span class="number">8</span>);</span><br><span class="line">      rhsDivisibility = <span class="built_in">multiplyDivisor</span>(rhs.<span class="built_in">getDivisibility</span>(dim), elemSize);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">gcd</span>(lhs.<span class="built_in">getDivisibility</span>(dim), rhsDivisibility);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="type">int64_t</span> <span class="title">getConstancy</span><span class="params">(OpTy op, <span class="type">const</span> AxisInfo &amp;lhs, <span class="type">const</span> AxisInfo &amp;rhs,</span></span></span><br><span class="line"><span class="params"><span class="function">                       <span class="type">int</span> dim)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">gcd</span>(lhs.<span class="built_in">getConstancy</span>(dim), rhs.<span class="built_in">getConstancy</span>(dim));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">std::optional&lt;<span class="type">int64_t</span>&gt; <span class="title">getConstantValue</span><span class="params">(OpTy op, <span class="type">const</span> AxisInfo &amp;lhs,</span></span></span><br><span class="line"><span class="params"><span class="function">                                          <span class="type">const</span> AxisInfo &amp;rhs)</span> <span class="keyword">override</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (lhs.<span class="built_in">getConstantValue</span>().<span class="built_in">has_value</span>() &amp;&amp;</span><br><span class="line">        rhs.<span class="built_in">getConstantValue</span>().<span class="built_in">has_value</span>()) &#123;</span><br><span class="line">      <span class="function"><span class="keyword">if</span> <span class="title">constexpr</span> <span class="params">(std::is_same_v&lt;OpTy, arith::AddIOp&gt;)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> &#123;lhs.<span class="built_in">getConstantValue</span>().<span class="built_in">value</span>() +</span><br><span class="line">                rhs.<span class="built_in">getConstantValue</span>().<span class="built_in">value</span>()&#125;;</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> <span class="built_in">constexpr</span> (std::is_same_v&lt;OpTy, arith::SubIOp&gt;) &#123;</span><br><span class="line">        <span class="keyword">return</span> &#123;lhs.<span class="built_in">getConstantValue</span>().<span class="built_in">value</span>() -</span><br><span class="line">                rhs.<span class="built_in">getConstantValue</span>().<span class="built_in">value</span>()&#125;;</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> <span class="built_in">constexpr</span> (std::is_same_v&lt;OpTy, triton::AddPtrOp&gt;) &#123;</span><br><span class="line">        <span class="keyword">auto</span> rank = lhs.<span class="built_in">getRank</span>();</span><br><span class="line">        <span class="keyword">auto</span> elemSize = std::<span class="built_in">max</span>&lt;<span class="type">int64_t</span>&gt;(</span><br><span class="line">            <span class="number">1</span>, triton::<span class="built_in">getPointeeBitWidth</span>(op.<span class="built_in">getPtr</span>().<span class="built_in">getType</span>()) / <span class="number">8</span>);</span><br><span class="line">        <span class="keyword">auto</span> rhsValue = rhs.<span class="built_in">getConstantValue</span>().<span class="built_in">value</span>() * elemSize;</span><br><span class="line">        <span class="keyword">return</span> &#123;lhs.<span class="built_in">getConstantValue</span>().<span class="built_in">value</span>() + rhsValue&#125;;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> &#123;&#125;;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<p>AxisInfo mainly consists of three new concepts:  </p>
<ul>
<li>Divisibility</li>
<li>Contiguity</li>
<li>Constancy</li>
</ul>
<p>Below are comments in source codes, which are clear to follow.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">Divisibility</span><br><span class="line">// For example, the 2D array</span><br><span class="line">//</span><br><span class="line">//   [[10, 11, 12, 13, 18, 19, 20, 21],</span><br><span class="line">//    [20, 21, 22, 23, 28, 29, 30, 31]]</span><br><span class="line">//</span><br><span class="line">// has contiguity [1, 4], and</span><br><span class="line">//</span><br><span class="line">//   [[12, 16, 20, 24],</span><br><span class="line">//    [13, 17, 21, 25],</span><br><span class="line">//    [14, 18, 22, 26],</span><br><span class="line">//    [15, 19, 23, 27],</span><br><span class="line">//    [18, 22, 26, 30],</span><br><span class="line">//    [19, 23, 27, 31]]</span><br><span class="line">//</span><br><span class="line">// has contiguity [2, 1].</span><br><span class="line">Contiguity</span><br><span class="line">// For example,</span><br><span class="line">//</span><br><span class="line">//   [[10, 11, 12, 13, 18, 19, 20, 21],</span><br><span class="line">//    [20, 21, 22, 23, 28, 29, 30, 31]]</span><br><span class="line">//</span><br><span class="line">//  has divisibility [1, 2], and</span><br><span class="line">//</span><br><span class="line">//    [[12, 16, 20, 24],</span><br><span class="line">//     [13, 17, 21, 25],</span><br><span class="line">//     [14, 18, 22, 26],</span><br><span class="line">//     [15, 19, 23, 27]]</span><br><span class="line">//</span><br><span class="line">// has divisibility [4, 1].</span><br><span class="line">//</span><br><span class="line">// On the other hand,</span><br><span class="line">//</span><br><span class="line">//   [0, 1, 2, 0, 4, 5, 6, 7]</span><br><span class="line">//</span><br><span class="line">// has divisibility 1 because its contiguity is 1.</span><br><span class="line">Constancy</span><br><span class="line">// For example</span><br><span class="line">//</span><br><span class="line">//   [[8, 8, 8, 8, 12, 12, 12, 12],</span><br><span class="line">//    [16, 16, 16, 16, 20, 20, 20, 20]]</span><br><span class="line">//</span><br><span class="line">// has constancy [1, 4].</span><br></pre></td></tr></table></figure>

<h4 id="Coalesce-Pass"><a href="#Coalesce-Pass" class="headerlink" title="Coalesce Pass"></a>Coalesce Pass</h4><p>After having a general galance of AxisInfo analysis, we first try to understand <code>Coalesce Optimization</code>, which enables consecutive memory access in <code>triton.load</code> or <code>triton.store</code> process. Refer to <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/670141785">OpenAI Triton: Memory coalesce</a> doc for further understanding.  </p>
<p><code>Transpose case-study</code> :key:</p>
<p>First let’s see a transpose test case to see how coalesce helps optimize:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">#blocked0 = <span class="meta">#ttg.blocked<span class="string">&lt;&#123;sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [4], order = [0]&#125;&gt;</span></span></span><br><span class="line">#blocked1 = <span class="meta">#ttg.blocked<span class="string">&lt;&#123;sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]&#125;&gt;</span></span></span><br><span class="line">#blocked2 = <span class="meta">#ttg.blocked<span class="string">&lt;&#123;sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]&#125;&gt;</span></span></span><br><span class="line">#slice1dim1 = <span class="meta">#ttg.slice<span class="string">&lt;&#123;dim = 1, parent = #blocked1&#125;&gt;</span></span></span><br><span class="line">#slice2dim0 = <span class="meta">#ttg.slice<span class="string">&lt;&#123;dim = 0, parent = #blocked2&#125;&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">module</span> attributes &#123;<span class="string">&quot;ttg.num-ctas&quot;</span> = <span class="number">1</span> : i32, <span class="string">&quot;ttg.num-warps&quot;</span> = <span class="number">4</span> : i32&#125; &#123;</span><br><span class="line"></span><br><span class="line">tt.func @<span class="built_in">transpose</span>(%arg0: !tt.ptr&lt;f32&gt; &#123;tt.divisibility = <span class="number">16</span> : i32&#125;,</span><br><span class="line">                %arg1: i32 &#123;tt.divisibility = <span class="number">16</span> : i32&#125;,</span><br><span class="line">                %arg2: !tt.ptr&lt;f32&gt; &#123;tt.divisibility = <span class="number">16</span> : i32&#125;,</span><br><span class="line">                %arg3: i32 &#123;tt.divisibility = <span class="number">16</span> : i32&#125;) &#123;</span><br><span class="line">  <span class="comment">// 定义boolean类型的掩码张量，初始值为true</span></span><br><span class="line">  <span class="comment">// 定义浮点类型的张量，初始值为0.0</span></span><br><span class="line">  %cst = arith.constant dense&lt;<span class="literal">true</span>&gt; : tensor&lt;<span class="number">64</span>x64xi1, #blocked1&gt;</span><br><span class="line">  %cst_0 = arith.constant dense&lt;<span class="number">0.000000e+00</span>&gt; : tensor&lt;<span class="number">64</span>x64xf32, #blocked1&gt;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 计算每行的起始地址</span></span><br><span class="line">  <span class="comment">// 计算base_ptr + row_index * row_stride</span></span><br><span class="line">  %<span class="number">00</span> = tt.make_range &#123;end = <span class="number">64</span> : i32, start = <span class="number">0</span> : i32&#125; : tensor&lt;<span class="number">64</span>xi32, #slice1dim1&gt;</span><br><span class="line">  %<span class="number">01</span> = tt.make_range &#123;end = <span class="number">64</span> : i32, start = <span class="number">0</span> : i32&#125; : tensor&lt;<span class="number">64</span>xi32, #slice2dim0&gt;</span><br><span class="line">  %<span class="number">1</span> = tt.expand_dims %<span class="number">00</span> &#123;axis = <span class="number">1</span> : i32&#125; : tensor&lt;<span class="number">64</span>xi32, #slice1dim1&gt; -&gt; tensor&lt;<span class="number">64</span>x1xi32, #blocked1&gt;</span><br><span class="line">  %<span class="number">2</span> = tt.splat %arg1 : i32 -&gt; tensor&lt;<span class="number">64</span>x1xi32, #blocked1&gt;</span><br><span class="line">  %<span class="number">3</span> = arith.muli %<span class="number">1</span>, %<span class="number">2</span> : tensor&lt;<span class="number">64</span>x1xi32, #blocked1&gt;</span><br><span class="line">  %<span class="number">4</span> = tt.splat %arg0 : !tt.ptr&lt;f32&gt; -&gt; tensor&lt;<span class="number">64</span>x1x!tt.ptr&lt;f32&gt;, #blocked1&gt;</span><br><span class="line">  %<span class="number">5</span> = tt.addptr %<span class="number">4</span>, %<span class="number">3</span> : tensor&lt;<span class="number">64</span>x1x!tt.ptr&lt;f32&gt;, #blocked1&gt;, tensor&lt;<span class="number">64</span>x1xi32, #blocked1&gt;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 计算列的位置</span></span><br><span class="line">  <span class="comment">// 公式为input_address = base_ptr + row_index * row_stride + col_index</span></span><br><span class="line">  %<span class="number">6</span> = tt.expand_dims %<span class="number">01</span> &#123;axis = <span class="number">0</span> : i32&#125; : tensor&lt;<span class="number">64</span>xi32, #slice2dim0&gt; -&gt; tensor&lt;<span class="number">1</span>x64xi32, #blocked2&gt;</span><br><span class="line">  %<span class="number">7</span> = tt.broadcast %<span class="number">5</span> : tensor&lt;<span class="number">64</span>x1x!tt.ptr&lt;f32&gt;, #blocked1&gt; -&gt; tensor&lt;<span class="number">64</span>x64x!tt.ptr&lt;f32&gt;, #blocked1&gt;</span><br><span class="line">  %<span class="number">8</span> = tt.broadcast %<span class="number">6</span> : tensor&lt;<span class="number">1</span>x64xi32, #blocked2&gt; -&gt; tensor&lt;<span class="number">64</span>x64xi32, #blocked2&gt;</span><br><span class="line">  %<span class="number">9</span> = ttg.convert_layout %<span class="number">8</span> : tensor&lt;<span class="number">64</span>x64xi32, #blocked2&gt; -&gt; tensor&lt;<span class="number">64</span>x64xi32, #blocked1&gt;</span><br><span class="line">  <span class="comment">// 最终的load 地址</span></span><br><span class="line">  %<span class="number">10</span> = tt.addptr %<span class="number">7</span>, %<span class="number">9</span> : tensor&lt;<span class="number">64</span>x64x!tt.ptr&lt;f32&gt;, #blocked1&gt;, tensor&lt;<span class="number">64</span>x64xi32, #blocked1&gt;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 计算最终store的地址</span></span><br><span class="line">  <span class="comment">// 公式为：output_address = base_ptr_out + col_index * output_row_stride + row_index</span></span><br><span class="line">  %<span class="number">11</span> = tt.splat %arg2 : !tt.ptr&lt;f32&gt; -&gt; tensor&lt;<span class="number">64</span>x1x!tt.ptr&lt;f32&gt;, #blocked1&gt;    </span><br><span class="line">  %<span class="number">12</span> = tt.addptr %<span class="number">11</span>, %<span class="number">1</span> : tensor&lt;<span class="number">64</span>x1x!tt.ptr&lt;f32&gt;, #blocked1&gt;, tensor&lt;<span class="number">64</span>x1xi32, #blocked1&gt;  <span class="comment">// 行基地址</span></span><br><span class="line">  %<span class="number">13</span> = tt.splat %arg3 : i32 -&gt; tensor&lt;<span class="number">1</span>x64xi32, #blocked2&gt;  <span class="comment">// 输出行步长</span></span><br><span class="line">  <span class="comment">// 计算行偏移</span></span><br><span class="line">  %<span class="number">14</span> = arith.muli %<span class="number">6</span>, %<span class="number">13</span> : tensor&lt;<span class="number">1</span>x64xi32, #blocked2&gt;     <span class="comment">// 列索引 * 输出行步长</span></span><br><span class="line">  %<span class="number">15</span> = tt.broadcast %<span class="number">12</span> : tensor&lt;<span class="number">64</span>x1x!tt.ptr&lt;f32&gt;, #blocked1&gt; -&gt; tensor&lt;<span class="number">64</span>x64x!tt.ptr&lt;f32&gt;, #blocked1&gt;</span><br><span class="line">  %<span class="number">16</span> = tt.broadcast %<span class="number">14</span> : tensor&lt;<span class="number">1</span>x64xi32, #blocked2&gt; -&gt; tensor&lt;<span class="number">64</span>x64xi32, #blocked2&gt;</span><br><span class="line">  %<span class="number">17</span> = ttg.convert_layout %<span class="number">16</span> : tensor&lt;<span class="number">64</span>x64xi32, #blocked2&gt; -&gt; tensor&lt;<span class="number">64</span>x64xi32, #blocked1&gt;</span><br><span class="line">  %<span class="number">18</span> = tt.addptr %<span class="number">15</span>, %<span class="number">17</span> : tensor&lt;<span class="number">64</span>x64x!tt.ptr&lt;f32&gt;, #blocked1&gt;, tensor&lt;<span class="number">64</span>x64xi32, #blocked1&gt;  <span class="comment">// 输出矩阵元素地址</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// load操作</span></span><br><span class="line">  %<span class="number">19</span> = tt.load %<span class="number">10</span>, %cst, %cst_0 : tensor&lt;<span class="number">64</span>x64x!tt.ptr&lt;f32&gt;, #blocked1&gt;</span><br><span class="line">  <span class="comment">// store操作</span></span><br><span class="line">  tt.store %<span class="number">18</span>, %<span class="number">19</span>, %cst : tensor&lt;<span class="number">64</span>x64x!tt.ptr&lt;f32&gt;, #blocked1&gt;</span><br><span class="line">  tt.<span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>To write a correct transpose triton-lang is quite easy, the key part lies in pointer calculation. That is, <code>A[i, j]</code> goes to <code>A-prime[j][i]</code>, where the math should be converted from <code>base_ptr + row_index * row_stride + col_index</code> to <code>base_ptr_out + col_index * output_row_stride + row_index</code>.   </p>
<p>The main tricks are <code>layout config</code> and <code>layout </code>. Let’s first analyze the origin layouts and potential optimizations that coalesce pass can make, see below:</p>
<blockquote>
<p><code>#blocked1 = #ttg.blocked&lt;&#123;sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]&#125;&gt;</code> means<br>each thread only process one item, each warp has 32 threads and distributed among rows. That is a thread is responsible for a row, this layout is quite bad for <code>tt.load</code>.<br>So there are two potential optimizations: (1) each thread process continguous 4 items, that is 4 x 32 &#x3D; 128bits, which fits a<br>vector operation length. (2) change layout for <code>tt.load</code>to row manner, that is distribute threads in warp in a row &#x2F; few rows.  </p>
</blockquote>
<p>Here is the optimized code:   </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#blocked = #ttg.blocked<span class="string">&lt;&#123;sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]&#125;&gt;</span></span></span><br><span class="line">#blocked1 = <span class="meta">#ttg.blocked<span class="string">&lt;&#123;sizePerThread = [1, 1], threadsPerWarp = [1, 32], warpsPerCTA = [1, 4], order = [0, 1]&#125;&gt;</span></span></span><br><span class="line">#blocked2 = <span class="meta">#ttg.blocked<span class="string">&lt;&#123;sizePerThread = [1, 4], threadsPerWarp = [2, 16], warpsPerCTA = [4, 1], order = [1, 0]&#125;&gt;</span></span></span><br><span class="line">#blocked3 = <span class="meta">#ttg.blocked<span class="string">&lt;&#123;sizePerThread = [4, 1], threadsPerWarp = [16, 2], warpsPerCTA = [1, 4], order = [0, 1]&#125;&gt;</span></span></span><br><span class="line"><span class="keyword">module</span> attributes &#123;<span class="string">&quot;ttg.num-ctas&quot;</span> = <span class="number">1</span> : i32, <span class="string">&quot;ttg.num-warps&quot;</span> = <span class="number">4</span> : i32&#125; &#123;</span><br><span class="line">  tt.func @<span class="built_in">transpose</span>(%arg0: !tt.ptr&lt;f32&gt; &#123;tt.divisibility = <span class="number">16</span> : i32&#125;, %arg1: i32 &#123;tt.divisibility = <span class="number">16</span> : i32&#125;, %arg2: !tt.ptr&lt;f32&gt; &#123;tt.divisibility = <span class="number">16</span> : i32&#125;, %arg3: i32 &#123;tt.divisibility = <span class="number">16</span> : i32&#125;) &#123;</span><br><span class="line">    %cst = arith.constant dense&lt;<span class="literal">true</span>&gt; : tensor&lt;<span class="number">64</span>x64xi1, <span class="meta">#blocked&gt;</span></span><br><span class="line">    %cst_0 = arith.constant dense&lt;<span class="number">0.000000e+00</span>&gt; : tensor&lt;<span class="number">64</span>x64xf32, <span class="meta">#blocked&gt;</span></span><br><span class="line">    %<span class="number">0</span> = tt.make_range &#123;end = <span class="number">64</span> : i32, start = <span class="number">0</span> : i32&#125; : tensor&lt;<span class="number">64</span>xi32, <span class="meta">#ttg.slice<span class="string">&lt;&#123;dim = 1, parent = #blocked&#125;&gt;</span>&gt;</span></span><br><span class="line">    %<span class="number">1</span> = tt.make_range &#123;end = <span class="number">64</span> : i32, start = <span class="number">0</span> : i32&#125; : tensor&lt;<span class="number">64</span>xi32, <span class="meta">#ttg.slice<span class="string">&lt;&#123;dim = 0, parent = #blocked1&#125;&gt;</span>&gt;</span></span><br><span class="line">    %<span class="number">2</span> = tt.expand_dims %<span class="number">0</span> &#123;axis = <span class="number">1</span> : i32&#125; : tensor&lt;<span class="number">64</span>xi32, <span class="meta">#ttg.slice<span class="string">&lt;&#123;dim = 1, parent = #blocked&#125;&gt;</span>&gt; -&gt; tensor<span class="string">&lt;64x1xi32, #blocked&gt;</span></span></span><br><span class="line">    %<span class="number">3</span> = tt.splat %arg1 : i32 -&gt; tensor&lt;<span class="number">64</span>x1xi32, <span class="meta">#blocked&gt;</span></span><br><span class="line">    %<span class="number">4</span> = arith.muli %<span class="number">2</span>, %<span class="number">3</span> : tensor&lt;<span class="number">64</span>x1xi32, <span class="meta">#blocked&gt;</span></span><br><span class="line">    %<span class="number">5</span> = tt.splat %arg0 : !tt.ptr&lt;f32&gt; -&gt; tensor&lt;<span class="number">64</span>x1x!tt.ptr&lt;f32&gt;, <span class="meta">#blocked&gt;</span></span><br><span class="line">    %<span class="number">6</span> = tt.addptr %<span class="number">5</span>, %<span class="number">4</span> : tensor&lt;<span class="number">64</span>x1x!tt.ptr&lt;f32&gt;, <span class="meta">#blocked&gt;, tensor<span class="string">&lt;64x1xi32, #blocked&gt;</span></span></span><br><span class="line">    %<span class="number">7</span> = tt.expand_dims %<span class="number">1</span> &#123;axis = <span class="number">0</span> : i32&#125; : tensor&lt;<span class="number">64</span>xi32, <span class="meta">#ttg.slice<span class="string">&lt;&#123;dim = 0, parent = #blocked1&#125;&gt;</span>&gt; -&gt; tensor<span class="string">&lt;1x64xi32, #blocked1&gt;</span></span></span><br><span class="line">    %<span class="number">8</span> = tt.broadcast %<span class="number">6</span> : tensor&lt;<span class="number">64</span>x1x!tt.ptr&lt;f32&gt;, <span class="meta">#blocked&gt; -&gt; tensor<span class="string">&lt;64x64x!tt.ptr&lt;f32&gt;</span>, #blocked&gt;</span></span><br><span class="line">    %<span class="number">9</span> = tt.broadcast %<span class="number">7</span> : tensor&lt;<span class="number">1</span>x64xi32, #blocked1&gt; -&gt; tensor&lt;<span class="number">64</span>x64xi32, #blocked1&gt;</span><br><span class="line">    %<span class="number">10</span> = ttg.convert_layout %<span class="number">9</span> : tensor&lt;<span class="number">64</span>x64xi32, #blocked1&gt; -&gt; tensor&lt;<span class="number">64</span>x64xi32, <span class="meta">#blocked&gt;</span></span><br><span class="line">    %<span class="number">11</span> = tt.addptr %<span class="number">8</span>, %<span class="number">10</span> : tensor&lt;<span class="number">64</span>x64x!tt.ptr&lt;f32&gt;, <span class="meta">#blocked&gt;, tensor<span class="string">&lt;64x64xi32, #blocked&gt;</span></span></span><br><span class="line">    %<span class="number">12</span> = tt.splat %arg2 : !tt.ptr&lt;f32&gt; -&gt; tensor&lt;<span class="number">64</span>x1x!tt.ptr&lt;f32&gt;, <span class="meta">#blocked&gt;</span></span><br><span class="line">    %<span class="number">13</span> = tt.addptr %<span class="number">12</span>, %<span class="number">2</span> : tensor&lt;<span class="number">64</span>x1x!tt.ptr&lt;f32&gt;, <span class="meta">#blocked&gt;, tensor<span class="string">&lt;64x1xi32, #blocked&gt;</span></span></span><br><span class="line">    %<span class="number">14</span> = tt.splat %arg3 : i32 -&gt; tensor&lt;<span class="number">1</span>x64xi32, #blocked1&gt;</span><br><span class="line">    %<span class="number">15</span> = arith.muli %<span class="number">7</span>, %<span class="number">14</span> : tensor&lt;<span class="number">1</span>x64xi32, #blocked1&gt;</span><br><span class="line">    %<span class="number">16</span> = tt.broadcast %<span class="number">13</span> : tensor&lt;<span class="number">64</span>x1x!tt.ptr&lt;f32&gt;, <span class="meta">#blocked&gt; -&gt; tensor<span class="string">&lt;64x64x!tt.ptr&lt;f32&gt;</span>, #blocked&gt;</span></span><br><span class="line">    %<span class="number">17</span> = tt.broadcast %<span class="number">15</span> : tensor&lt;<span class="number">1</span>x64xi32, #blocked1&gt; -&gt; tensor&lt;<span class="number">64</span>x64xi32, #blocked1&gt;</span><br><span class="line">    %<span class="number">18</span> = ttg.convert_layout %<span class="number">17</span> : tensor&lt;<span class="number">64</span>x64xi32, #blocked1&gt; -&gt; tensor&lt;<span class="number">64</span>x64xi32, <span class="meta">#blocked&gt;</span></span><br><span class="line">    %<span class="number">19</span> = tt.addptr %<span class="number">16</span>, %<span class="number">18</span> : tensor&lt;<span class="number">64</span>x64x!tt.ptr&lt;f32&gt;, <span class="meta">#blocked&gt;, tensor<span class="string">&lt;64x64xi32, #blocked&gt;</span></span></span><br><span class="line">    %<span class="number">20</span> = ttg.convert_layout %<span class="number">11</span> : tensor&lt;<span class="number">64</span>x64x!tt.ptr&lt;f32&gt;, <span class="meta">#blocked&gt; -&gt; tensor<span class="string">&lt;64x64x!tt.ptr&lt;f32&gt;</span>, #blocked2&gt;</span></span><br><span class="line">    %<span class="number">21</span> = ttg.convert_layout %cst : tensor&lt;<span class="number">64</span>x64xi1, <span class="meta">#blocked&gt; -&gt; tensor<span class="string">&lt;64x64xi1, #blocked2&gt;</span></span></span><br><span class="line">    %<span class="number">22</span> = ttg.convert_layout %cst_0 : tensor&lt;<span class="number">64</span>x64xf32, <span class="meta">#blocked&gt; -&gt; tensor<span class="string">&lt;64x64xf32, #blocked2&gt;</span></span></span><br><span class="line">    %<span class="number">23</span> = tt.load %<span class="number">20</span>, %<span class="number">21</span>, %<span class="number">22</span> : tensor&lt;<span class="number">64</span>x64x!tt.ptr&lt;f32&gt;, #blocked2&gt;</span><br><span class="line">    %<span class="number">24</span> = ttg.convert_layout %<span class="number">23</span> : tensor&lt;<span class="number">64</span>x64xf32, #blocked2&gt; -&gt; tensor&lt;<span class="number">64</span>x64xf32, <span class="meta">#blocked&gt;</span></span><br><span class="line">    %<span class="number">25</span> = ttg.convert_layout %<span class="number">19</span> : tensor&lt;<span class="number">64</span>x64x!tt.ptr&lt;f32&gt;, <span class="meta">#blocked&gt; -&gt; tensor<span class="string">&lt;64x64x!tt.ptr&lt;f32&gt;</span>, #blocked3&gt;</span></span><br><span class="line">    %<span class="number">26</span> = ttg.convert_layout %<span class="number">24</span> : tensor&lt;<span class="number">64</span>x64xf32, <span class="meta">#blocked&gt; -&gt; tensor<span class="string">&lt;64x64xf32, #blocked3&gt;</span></span></span><br><span class="line">    %<span class="number">27</span> = ttg.convert_layout %cst : tensor&lt;<span class="number">64</span>x64xi1, <span class="meta">#blocked&gt; -&gt; tensor<span class="string">&lt;64x64xi1, #blocked3&gt;</span></span></span><br><span class="line">    tt.store %<span class="number">25</span>, %<span class="number">26</span>, %<span class="number">27</span> : tensor&lt;<span class="number">64</span>x64x!tt.ptr&lt;f32&gt;, #blocked3&gt;</span><br><span class="line">    tt.<span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Then let’s dive into how triton makes coalesce pass come true:  </p>
<p><code>Coalesce Implementation</code><br>What coalesce does is quite simple, it only focus on following five operations that are global memory-related:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">Value <span class="title">getMemAccessPtr</span><span class="params">(Operation *op)</span> </span>&#123;        <span class="comment">// coalesce优化只处理load/store/atomic/copy等涉及指针的操作</span></span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">auto</span> ld = <span class="built_in">dyn_cast</span>&lt;triton::LoadOp&gt;(op))</span><br><span class="line">    <span class="keyword">return</span> ld.<span class="built_in">getPtr</span>();    <span class="comment">// 一般返回的类型是tensor&lt;1024x!tt.ptr&lt;f32&gt;&gt;，用getMask()可以获得load的掩码，用于处理访问越界情况</span></span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">auto</span> atomic = <span class="built_in">dyn_cast</span>&lt;triton::AtomicRMWOp&gt;(op))</span><br><span class="line">    <span class="keyword">return</span> atomic.<span class="built_in">getPtr</span>();</span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">auto</span> atomic = <span class="built_in">dyn_cast</span>&lt;triton::AtomicCASOp&gt;(op))</span><br><span class="line">    <span class="keyword">return</span> atomic.<span class="built_in">getPtr</span>();</span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">auto</span> copy = <span class="built_in">dyn_cast</span>&lt;triton::gpu::AsyncCopyGlobalToLocalOp&gt;(op))</span><br><span class="line">    <span class="keyword">return</span> copy.<span class="built_in">getSrc</span>();</span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">auto</span> store = <span class="built_in">dyn_cast</span>&lt;triton::StoreOp&gt;(op))</span><br><span class="line">    <span class="keyword">return</span> store.<span class="built_in">getPtr</span>();</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">nullptr</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>What it does to these five operations are: </p>
<ol>
<li>Create a coalesced memory layout L2 of the pointer operands (determined by Axisinfo analysis:key:)</li>
<li>Convert all operands from layout L1 to layout L2 (<code>ttg.convert_layout</code> inserted:key:)</li>
<li>Create a new memory op that consumes these operands and produces a tensor with layout L2</li>
<li>Convert the output of this new memory op back to L1</li>
<li>Replace all the uses of the original memory op by the new one</li>
</ol>
<p>From the five steps above, most important parts are:    </p>
<ul>
<li>Layout Determination by AxisInfo Analysis</li>
<li>Insertion of Layout conversion operation</li>
</ul>
<p>First let’s have a view on Layout determination. Below is the AxisInfo dumped:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[tritongpu-coalesce]: Considering op: %<span class="number">20</span> = tt.load %<span class="number">11</span>, %cst, %cst_0 : tensor&lt;<span class="number">64</span>x64x!tt.ptr&lt;f32&gt;, <span class="meta">#ttg.blocked<span class="string">&lt;&#123;sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]&#125;&gt;</span>&gt;</span></span><br><span class="line">[tritongpu-coalesce]: axis info of pointer: contiguity = [<span class="number">1</span>, <span class="number">64</span>], divisibility = [<span class="number">4</span>, <span class="number">16</span>], constancy = [<span class="number">1</span>, <span class="number">1</span>], constant_value = &lt;none&gt;</span><br><span class="line">[tritongpu-coalesce]: order=[<span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">[tritongpu-coalesce]: shapePerCTA=[<span class="number">64</span>, <span class="number">64</span>]</span><br><span class="line">[ttg-utility]: elemNumBytes: <span class="number">4</span>, divisibility: <span class="number">16</span>, contig: <span class="number">64</span>, alignment: <span class="number">4</span></span><br><span class="line">[tritongpu-coalesce]: perThread <span class="keyword">for</span> op: <span class="number">4</span></span><br><span class="line">[tritongpu-coalesce]: perThread: <span class="number">4</span></span><br><span class="line">[tritongpu-coalesce]: Considering op: tt.store %<span class="number">19</span>, %<span class="number">20</span>, %cst : tensor&lt;<span class="number">64</span>x64x!tt.ptr&lt;f32&gt;, <span class="meta">#ttg.blocked<span class="string">&lt;&#123;sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [4, 1], order = [0, 1]&#125;&gt;</span>&gt;</span></span><br><span class="line">[tritongpu-coalesce]: axis info of pointer: contiguity = [<span class="number">64</span>, <span class="number">1</span>], divisibility = [<span class="number">16</span>, <span class="number">4</span>], constancy = [<span class="number">1</span>, <span class="number">1</span>], constant_value = &lt;none&gt;</span><br><span class="line">[tritongpu-coalesce]: order=[<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">[tritongpu-coalesce]: shapePerCTA=[<span class="number">64</span>, <span class="number">64</span>]</span><br><span class="line">[ttg-utility]: elemNumBytes: <span class="number">4</span>, divisibility: <span class="number">16</span>, contig: <span class="number">64</span>, alignment: <span class="number">4</span></span><br><span class="line">[tritongpu-coalesce]: perThread <span class="keyword">for</span> op: <span class="number">4</span></span><br><span class="line">[tritongpu-coalesce]: perThread: <span class="number">4</span></span><br><span class="line">[ttg-utility]: elemNumBytes: <span class="number">4</span>, divisibility: <span class="number">16</span>, contig: <span class="number">64</span>, alignment: <span class="number">4</span></span><br></pre></td></tr></table></figure>

<p>Two key functions, one for order determination and another for elements&#x2F;thread determination:  </p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">SmallVector&lt;<span class="type">unsigned</span>, 4&gt;</span></span><br><span class="line"><span class="function"><span class="title">getOrderFromContiguity</span><span class="params">(<span class="type">const</span> SmallVector&lt;<span class="type">int64_t</span>&gt; &amp;arr)</span> </span>&#123;</span><br><span class="line">  <span class="function">SmallVector&lt;<span class="type">unsigned</span>, 4&gt; <span class="title">ret</span><span class="params">(arr.size())</span></span>;</span><br><span class="line">  std::<span class="built_in">iota</span>(ret.<span class="built_in">begin</span>(), ret.<span class="built_in">end</span>(), <span class="number">0</span>);    <span class="comment">// [0,1,2,3,4] 上升序列</span></span><br><span class="line">  std::<span class="built_in">reverse</span>(ret.<span class="built_in">begin</span>(), ret.<span class="built_in">end</span>());</span><br><span class="line">  std::<span class="built_in">stable_sort</span>(ret.<span class="built_in">begin</span>(), ret.<span class="built_in">end</span>(),</span><br><span class="line">                   [&amp;](<span class="type">unsigned</span> x, <span class="type">unsigned</span> y) &#123; <span class="keyword">return</span> arr[x] &gt; arr[y]; &#125;);   <span class="comment">// 即dim上连续性大的order排在前面</span></span><br><span class="line">  <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">unsigned</span> <span class="title">getNumElementsPerThread</span><span class="params">(Operation *op, SmallVector&lt;<span class="type">unsigned</span>&gt; order,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 ModuleAxisInfoAnalysis &amp;axisInfoAnalysis)</span> </span>&#123;     <span class="comment">// 计算一个thread应该处理多少element</span></span><br><span class="line">  Value val = <span class="built_in">getMemAccessPtr</span>(op);                                               <span class="comment">// 只有后几代架构引入了CTG概念</span></span><br><span class="line">  <span class="keyword">auto</span> ty = <span class="built_in">cast</span>&lt;RankedTensorType&gt;(val.<span class="built_in">getType</span>());</span><br><span class="line">  <span class="keyword">auto</span> shapePerCTA = triton::gpu::<span class="built_in">getShapePerCTA</span>(ty);</span><br><span class="line">  AxisInfo &amp;valInfo = *axisInfoAnalysis.<span class="built_in">getAxisInfo</span>(val);</span><br><span class="line">  <span class="type">unsigned</span> elemNumBits = <span class="built_in">getElementBitWidth</span>(ty);</span><br><span class="line">  <span class="type">unsigned</span> elemNumBytes = std::<span class="built_in">max</span>(elemNumBits / <span class="number">8</span>, <span class="number">1u</span>);</span><br><span class="line">  <span class="type">unsigned</span> maxMultipleBytes = valInfo.<span class="built_in">getDivisibility</span>(order[<span class="number">0</span>]);</span><br><span class="line">  <span class="type">unsigned</span> maxMultiple = std::<span class="built_in">max</span>(maxMultipleBytes / elemNumBytes, <span class="number">1u</span>);</span><br><span class="line">  <span class="type">unsigned</span> maxContig =</span><br><span class="line">      std::<span class="built_in">min</span>(valInfo.<span class="built_in">getContiguity</span>(order[<span class="number">0</span>]), shapePerCTA[order[<span class="number">0</span>]]);</span><br><span class="line">  <span class="type">unsigned</span> alignment = std::<span class="built_in">min</span>(maxMultiple, maxContig);                   <span class="comment">// 平衡对齐要求和连续性要求</span></span><br><span class="line">  <span class="type">unsigned</span> currPerThread = std::<span class="built_in">min</span>(alignment, <span class="number">128</span> / elemNumBits);         <span class="comment">// 一个transaction最多一次处理（load/store）128bits</span></span><br><span class="line">  <span class="built_in">LDBG</span>(<span class="string">&quot;elemNumBytes: &quot;</span> &lt;&lt; elemNumBytes</span><br><span class="line">                        &lt;&lt; <span class="string">&quot;, divisibility: &quot;</span> &lt;&lt; maxMultipleBytes</span><br><span class="line">                        &lt;&lt; <span class="string">&quot;, contig: &quot;</span> &lt;&lt; valInfo.<span class="built_in">getContiguity</span>(order[<span class="number">0</span>])</span><br><span class="line">                        &lt;&lt; <span class="string">&quot;, alignment: &quot;</span> &lt;&lt; alignment);</span><br><span class="line">  <span class="keyword">return</span> currPerThread;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Chapter5-Python-Binding-Layer"><a href="#Chapter5-Python-Binding-Layer" class="headerlink" title="Chapter5: Python Binding Layer"></a><font color = brown>Chapter5: Python Binding Layer</font></h2><p>refer to <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/python/src/passes.cc#L20-L128">python&#x2F;src&#x2F;passes.cpp</a> and <a target="_blank" rel="noopener" href="https://github.com/triton-lang/triton/blob/main/python/src/passes.h#L1-L43">python&#x2F;src&#x2F;passes.h</a></p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a><font color = brown>References</font></h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/gpu-mode/lectures/blob/main/lecture_029/presentation.pdf/">Triton Internal Talk</a> </li>
<li><a target="_blank" rel="noopener" href="https://github.com/gpu-mode/lectures/tree/main/lecture_018">Triton Fusing Talk</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/gpu-mode/lectures/blob/main/lecture_014/A_Practitioners_Guide_to_Triton.ipynb">A Practioner’s Guide to Triton</a> </li>
<li><a target="_blank" rel="noopener" href="https://www.kapilsharma.dev/posts/deep-dive-into-triton-internals/">Deep Dive into Triton 1&amp;2&amp;3</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/kapilsh/triton/tree/14025786d108596cfd99700caa4f438938c2ceba?tab=readme-ov-file#tips-for-hacking">Official Hacking for Triton</a> </li>
<li><a target="_blank" rel="noopener" href="https://www.lei.chat/posts/triton-compiler-development-tips/">Triton Developer Guide</a></li>
<li><a target="_blank" rel="noopener" href="https://www.lei.chat/posts/triton-linear-layout-concept/">Triton Linear Layout: Concept</a></li>
<li><a target="_blank" rel="noopener" href="http://zhuanlan.zhihu.com/p/672720213">OpenAI Triton: Why layout is important</a> </li>
<li><a target="_blank" rel="noopener" href="https://tfruan2000.github.io/posts/triton-source-code-1/">OpenAI Triton 源码走读[transforms in ttir]</a></li>
<li><a target="_blank" rel="noopener" href="https://tfruan2000.github.io/posts/triton-source-code-2/">OpenAI Triton 源码走读[ttir-2-ttgir]</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/687394750">Triton Axis Analysis</a></li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/GPU/" rel="tag"># GPU</a>
              <a href="/tags/%E7%BC%96%E8%AF%91/" rel="tag"># 编译</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/09/13/LLM-Inference/" rel="prev" title="LLM Inference">
      <i class="fa fa-chevron-left"></i> LLM Inference
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/10/03/Paper-Reading-All-about-CGRA/" rel="next" title="Paper Reading: All about CGRA">
      Paper Reading: All about CGRA <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Triton-Deep-Dive"><span class="nav-number">1.</span> <span class="nav-text">Triton Deep Dive</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter1-Overall-Framework"><span class="nav-number">1.1.</span> <span class="nav-text">Chapter1: Overall Framework</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter2-Source-code-structure"><span class="nav-number">1.2.</span> <span class="nav-text">Chapter2: Source code structure </span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter3-JIT-compile"><span class="nav-number">1.3.</span> <span class="nav-text">Chapter3: JIT compile</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter4-MLIR-Lowering-Passes"><span class="nav-number">1.4.</span> <span class="nav-text">Chapter4: MLIR Lowering Passes</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TTIR"><span class="nav-number">1.4.1.</span> <span class="nav-text">TTIR</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#RewriteTensorPointer"><span class="nav-number">1.4.1.1.</span> <span class="nav-text">RewriteTensorPointer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Combine"><span class="nav-number">1.4.1.2.</span> <span class="nav-text">Combine</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ReorderBroadcast"><span class="nav-number">1.4.1.3.</span> <span class="nav-text">ReorderBroadcast</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TTIR-TTGIR"><span class="nav-number">1.4.2.</span> <span class="nav-text">TTIR -&gt; TTGIR</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TTGIR"><span class="nav-number">1.4.3.</span> <span class="nav-text">TTGIR</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Axis-Analysis"><span class="nav-number">1.4.3.1.</span> <span class="nav-text">Axis Analysis</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Coalesce-Pass"><span class="nav-number">1.4.3.2.</span> <span class="nav-text">Coalesce Pass</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Chapter5-Python-Binding-Layer"><span class="nav-number">1.5.</span> <span class="nav-text">Chapter5: Python Binding Layer</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">1.6.</span> <span class="nav-text">References</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Leon Dou</p>
  <div class="site-description" itemprop="description">关注领域：体系结构，编译技术</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">32</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Leon Dou</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">338k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">5:07</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        
<div class="busuanzi-count">
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/pjax/pjax.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[data-pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.dataset.pjax !== undefined) {
      script.dataset.pjax = '';
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  




  
<script src="/js/local-search.js"></script>













    <div id="pjax">
  

  

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'Ov23liFsw1lTgh0R8s8H',
      clientSecret: '1f947cb0d107ba1ffbcad8c25688c075224fff36',
      repo        : 'micropuma.github.io',
      owner       : 'micropuma',
      admin       : ['micropuma'],
      id          : '002cc9372482984558f3fb78a0256f39',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

    </div>
</body>
</html>
